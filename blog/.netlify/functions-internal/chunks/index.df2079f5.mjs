import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead, d as renderComponent } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           *//* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "basic-overview", "text": "Basic Overview" }, { "depth": 3, "slug": "search", "text": "Search" }, { "depth": 3, "slug": "keywords", "text": "Keywords" }, { "depth": 2, "slug": "real-world-scenarios", "text": "Real-World Scenarios" }, { "depth": 3, "slug": "scenario-1-compliance", "text": "Scenario 1: Compliance" }, { "depth": 4, "slug": "your-companys-employees-speak-on-the-phone-in-a-customer-service-role-and-they-are-required-to-say-at-the-start-of-every-phone-call-this-call-is-being-recorded-for-the-purpose-of-compliance-you-want-to-check-that-they-are-actually-saying-this-phrase", "text": "Your company\u2019s employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, \u201CThis call is being recorded.\u201D For the purpose of compliance, you want to check that they are actually saying this phrase." }, { "depth": 3, "slug": "scenario-2-discovery", "text": "Scenario 2: Discovery" }, { "depth": 4, "slug": "your-company-is-participating-in-litigation-and-as-part-of-the-ediscovery-phase-you-are-tasked-with-producing-electronic-documents-including-audio-and-video-which-are-relevant-to-the-case", "text": "Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case" }, { "depth": 3, "slug": "scenario-3-demos", "text": "Scenario 3: Demos" }, { "depth": 4, "slug": "you-are-planning-an-important-demo-for-the-upcoming-week-and-you-want-to-use-deepgrams-speech-to-text-api-but-you-havent-trained-a-language-model-yet-the-demo-is-crucial-to-your-business-so-you-want-to-give-an-added-boost-to-deepgrams-transcription-technology-to-improve-accuracy-for-certain-words", "text": "You are planning an important demo for the upcoming week, and you want to use Deepgram\u2019s speech-to-text API, but you haven\u2019t trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram\u2019s transcription technology to improve accuracy for certain words." }, { "depth": 3, "slug": "other-common-scenarios", "text": "Other Common Scenarios" }, { "depth": 2, "slug": "final-thoughts", "text": "Final Thoughts" }], "source": `
**Search** and **keywords** are two different features of Deepgram's API that may sound similar but actually work best in different scenarios. In this blog post, I am going to help you understand these two features and give you a better idea of the very different situations in which you might use them.

To get the most benefit out of this post, I recommend you have at least skimmed through our documentation on [search](https://developers.deepgram.com/documentation/features/search/) and [keywords](https://developers.deepgram.com/documentation/features/keywords/).

## Basic Overview

### Search

Deepgram\u2019s **search** feature is a query that can be made to **find out if a word or phrase has been said** in the audio that is being transcribed. During the transcribing of the audio of a pre-recorded sound file or a stream, Deepgram can analyze the audio to find the queried word or phrase. In the JSON response, it will return information about the **start time** and **end time** of when that word was possibly uttered. It will also give a **confidence** rating to each match. So you will see the **query** (the word or phrase you searched for), and then you will see **hits** (an array of objects that give you the **confidence**, **start**, **end**, and **snippet** of each possible match to your search).

To search for matches to the word 'epistemology,' I would add this query parameter to the API call URL or provide it as an option when using a [Deepgram SDK](https://developers.deepgram.com/sdks-tools/):

**search=epistemology**

\`\`\`js
"search":[\r
  {\r
    "query":"epistemology",\r
    "hits":[\r
      {"confidence":0.9348958,"start":10.725,"end":11.485,"snippet":"is a"},\r
      {"confidence":0.9348958,"start":13.965,"end":14.764999,"snippet":"epi"},\r
      {"confidence":0.9204282,"start":4.0074897,"end":4.805,"snippet":"social epi"},\r
      {"confidence":0.27662036,"start":8.792552,"end":10.365,"snippet":"us today is"},\r
      {"confidence":0.1319444,"start":17.205,"end":18.115,"snippet":"nature of knowledge"},\r
      {"confidence":0.0885417,"start":15.285,"end":16.085,"snippet":"branch philosophy"},\r
      {"confidence":0.045138836,"start":5.1240044,"end":5.722137,"snippet":"university of"},\r
      {"confidence":0.045138836,"start":5.6025105,"end":7.4367843,"snippet":"warwick and"},\r
      {"confidence":0.0,"start":1.0168257,"end":1.9339627,"snippet":"hello this is steve"},\r
      {"confidence":0.0,"start":7.277282,"end":8.27417,"snippet":"and the question"}\r
    ]\r
  }\r
]
\`\`\`

### Keywords

The **keywords** feature is also a query, but it is used to give the Deepgram AI more information so that it can **better transcribe the audio in the request**. Keywords are words that can be sent along with the audio, and then Deepgram will train itself to watch more closely for those words and transcribe them more accurately.

To improve transcription of the words 'epistemology' and 'ontology', I would add this query parameter to the API call URL or provide it as an option when using a [Deepgram SDK](https://developers.deepgram.com/sdks-tools/):

**keywords=epistemology\\&keywords=ontology**

It can be challenging for ASR technology to transcribe certain words, particularly jargon, names, or very uncommon words. But the beauty of deep learning is that the technology can *learn*. If you know about these context-unique words ahead of time, you can send them with the request as keywords, and Deepgram will do a better job of actually transcribing those words.

<Alert type="warning">

Keywords are a much less powerful substitute than training a custom model. The best solution is to work with Deepgram to train a model for your specific language context. If you are using the keywords boosting feature for many words, and you are using this feature often, you should look into having a custom model trained for your situation because it will perform better.

</Alert>

## Real-World Scenarios

The fundamental difference between keywords and search is that search will *return a data object that gives you possible matches to your search terms*. If you just need to find a word or to know if that word was said (or not said), use **search**.

Keywords do not return an added data object; they just *improve the transcription itself* so that you see the boosted keywords transcribed correctly in the transcription. If you want to see a better transcription - improved specifically for words important to you - use **keywords** (however, there are caveats to using **keywords**, which we will examine in more detail later).

While this distinction might be becoming more clear to you, the best way to really understand when to use **search** versus when to use **keywords** is to look at some possible scenarios.

### Scenario 1: Compliance

#### Your company's employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, "This call is being recorded." For the purpose of compliance, you want to check that they are actually saying this phrase.

In this scenario, you can use **search** to find matches to that phrase. You will not receive just a yes/no response (as in a Boolean telling you true/false) in the response object, but you will receive the list of matches with a confidence rating. Deepgram's AI will do its best to find phrases that it thinks might match the phrase "This call is being recorded." **However, this doesn't mean that all the matches are correct.**

This is where the nuance comes in. You have a powerful tool that has helped you search through audio files to find possible matches, and then as a user or a customer, you can decide for yourself what confidence rating is more likely to return a match. You can analyze the data you get back and start to see a pattern for what confidence number is bringing back accurate matches.

If it were me, I would pay attention to the confidence level that seems to be bringing back accurate matches, and then I would make an executive decision to say that this number gives us reasonable confidence that this phrase was uttered. Because every language context is different, Deepgram's general language model is going to perform differently when transcribing audio in those different contexts, so this feature can only be used as a tool to guide you to knowing whether the search term/phrase was uttered or not.

### Scenario 2: Discovery

#### Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case

Having a list of possible word or phrase matches can be extremely useful, but **search** is also a powerful tool for pointing you in the direction of valuable *sections* of your audio. A perfect example of this is eDiscovery, when attorneys are required to provide documents which are likely to have a certain relevance to the case. In this situation, they wouldn't be searching for just one word or phrase, but they would be looking for language that suggests that the conversation in the audio or video file is relevant.

The goal of ASR is to recognize human speech, but human speech isn't always direct, straightforward, or even completely honest. In order to comprehend what is said, humans often read between the lines. Since Deepgram's search feature isn't just a yes/no check to find a match, but actually gives you a confidence level in the response, this search tool can be combined with the human brain to come up with a combination of words and phrases that might help to find more nuanced relevant language.

To me, this is one of the most exciting possibilities of Deepgram's **search** feature because it demonstrates how AI isn't just about building machines to think for us, it's about building machines to aid humans in thinking better.

### Scenario 3: Demos

#### You are planning an important demo for the upcoming week, and you want to use Deepgram's speech-to-text API, but you haven't trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram's transcription technology to improve accuracy for certain words.

The purpose of **keywords** is to improve the transcription output, so if you want to get a more accurate transcription (and you have not been able to train a custom model), a less powerful option would be to use keywords. You can improve the transcription by putting a list of keywords that are more difficult to recognize by ASR into the query parameter. Jargon, names, and scientific words are good examples of words that can be tricky for ASR.

The **keywords** feature is one that requires thoughtful implementation. Keywords do on a much smaller scale what model-training does, but **there are tradeoffs.** It might seem like a great way to improve a transcription, but because so many factors go into transcribing audio, and because training a model is something that is done by deep learning experts, substituting keywords for model training doesn't always work very well straight out of the box. However, it can be used if you are willing to test out various keywords and adjust based on the results you see.

The best approach would be to benchmark the transcription results without any keywords, then add words one by one and notice the effect. You want to aim to find that sweet spot where the transcription is coming out as accurate as possible. But due to the improvements Deepgram has made in its various models, adding keywords does not always improve the transcription. Depending on the model you are using and your unique language context, the keywords might even lower the quality of the transcription.

The keywords feature works differently depending on if the words are 'in-vocabulary' or 'out-of-vocabulary' (see the [documentation](https://developers.deepgram.com/documentation/features/keywords/#in-vocabulary--out-of-vocabulary-keywords)). If the words already exist in the model (whether you are using a general model or your own trained model), Deepgram will just put more emphasis on those keywords (which the model already knows exist), telling the AI to boost the confidence that those words were spoken. If you are using keywords to improve the transcription of words that don't exist in the general language model, keyword boosting can be slower. Sending a huge amount of keywords may affect the speed of the transcription creation. **It is recommended to use 10-100 keywords, with 100 really pushing the limits.** This is important for real-time transcription, but it may not be a concern to you if you are doing batch-processing of audio files.

The better Deepgram's general model gets at transcribing on its own, the less powerful the keywords tool is, since more confidence will be given to the transcription itself. Using keywords might not have an effect. If you add really high intensifiers to your keywords (see the documentation about [intensifiers](https://developers.deepgram.com/documentation/features/keywords/#intensifiers)), it might cause the transcription to overuse those keywords. You have to find the right balance, which comes with trial and error.

Finally, keywords **cannot boost phrases**, so if you are searching for a *first name* + *last name* combination, the boosting will happen individually for those two words, and you might see that one shows up correctly while the other does not.

The keywords feature is still in beta at this time, so please factor this into your usage. We are actively working to improve this feature.

### Other Common Scenarios

Here are some other possible scenarios for when **search** would be a great tool:

*   Your product uses Deepgram to provide captions to real-time streams or pre-recorded videos, and you want to be able to search the captions to find a topic or point that was discussed.
*   Searching for competitor or brand names from various types of media recordings, so you can gather information to help build strategies.
*   Searching for specific language phrases that imply dissatisfaction (such as profanity) so you can analyze situations when customers were not happy.

Here are some other possible scenarios for when **keywords** would be a great tool:

*   You haven't had an opportunity to train a custom model, but you have an idea of context-unique words that can improve the transcription.
*   You want to improve real-time transcription of a meeting by including keywords that are names of participants or company terms that will be used during the meeting.

## Final Thoughts

After chatting with Deepgrammers who build our Speech Recognition API, the incredible power of deep learning has become even more evident to me. Deepgram's **search** feature really exemplifies this. **Search** is a tool that can be used in so many different situations and gives us valuable information about a transcription. **Keywords** is more of an added layer to the already impressive Deepgram AI, used to improve the transcription.

ASR is not perfect and humans are not perfect, but we can use a tool like the Deepgram **search** feature to get data about a transcription and make really informed decisions about it. We can use the **keywords** feature to prompt the Deepgram technology to learn so that it performs better for our unique situation.

The AI of Deepgram isn't about to take over the world all by itself, but humans with the AI of Deepgram can build great things.

        `, "html": '<p><strong>Search</strong> and <strong>keywords</strong> are two different features of Deepgram\u2019s API that may sound similar but actually work best in different scenarios. In this blog post, I am going to help you understand these two features and give you a better idea of the very different situations in which you might use them.</p>\n<p>To get the most benefit out of this post, I recommend you have at least skimmed through our documentation on <a href="https://developers.deepgram.com/documentation/features/search/">search</a> and <a href="https://developers.deepgram.com/documentation/features/keywords/">keywords</a>.</p>\n<h2 id="basic-overview">Basic Overview</h2>\n<h3 id="search">Search</h3>\n<p>Deepgram\u2019s <strong>search</strong> feature is a query that can be made to <strong>find out if a word or phrase has been said</strong> in the audio that is being transcribed. During the transcribing of the audio of a pre-recorded sound file or a stream, Deepgram can analyze the audio to find the queried word or phrase. In the JSON response, it will return information about the <strong>start time</strong> and <strong>end time</strong> of when that word was possibly uttered. It will also give a <strong>confidence</strong> rating to each match. So you will see the <strong>query</strong> (the word or phrase you searched for), and then you will see <strong>hits</strong> (an array of objects that give you the <strong>confidence</strong>, <strong>start</strong>, <strong>end</strong>, and <strong>snippet</strong> of each possible match to your search).</p>\n<p>To search for matches to the word \u2018epistemology,\u2019 I would add this query parameter to the API call URL or provide it as an option when using a <a href="https://developers.deepgram.com/sdks-tools/">Deepgram SDK</a>:</p>\n<p><strong>search=epistemology</strong></p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;search&quot;</span><span style="color: #C9D1D9">:[</span></span>\n<span class="line"><span style="color: #C9D1D9">  {</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #A5D6FF">&quot;query&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;epistemology&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #A5D6FF">&quot;hits&quot;</span><span style="color: #C9D1D9">:[</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9348958</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">10.725</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">11.485</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;is a&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9348958</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">13.965</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">14.764999</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;epi&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9204282</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">4.0074897</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">4.805</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;social epi&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.27662036</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">8.792552</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">10.365</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;us today is&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.1319444</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">17.205</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">18.115</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;nature of knowledge&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0885417</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">15.285</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">16.085</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;branch philosophy&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.045138836</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.1240044</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.722137</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;university of&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.045138836</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.6025105</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">7.4367843</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;warwick and&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">1.0168257</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">1.9339627</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;hello this is steve&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">7.277282</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">8.27417</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;and the question&quot;</span><span style="color: #C9D1D9">}</span></span>\n<span class="line"><span style="color: #C9D1D9">    ]</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">]</span></span></code></pre>\n<h3 id="keywords">Keywords</h3>\n<p>The <strong>keywords</strong> feature is also a query, but it is used to give the Deepgram AI more information so that it can <strong>better transcribe the audio in the request</strong>. Keywords are words that can be sent along with the audio, and then Deepgram will train itself to watch more closely for those words and transcribe them more accurately.</p>\n<p>To improve transcription of the words \u2018epistemology\u2019 and \u2018ontology\u2019, I would add this query parameter to the API call URL or provide it as an option when using a <a href="https://developers.deepgram.com/sdks-tools/">Deepgram SDK</a>:</p>\n<p><strong>keywords=epistemology&keywords=ontology</strong></p>\n<p>It can be challenging for ASR technology to transcribe certain words, particularly jargon, names, or very uncommon words. But the beauty of deep learning is that the technology can <em>learn</em>. If you know about these context-unique words ahead of time, you can send them with the request as keywords, and Deepgram will do a better job of actually transcribing those words.</p>\n<Alert type="warning"><p>Keywords are a much less powerful substitute than training a custom model. The best solution is to work with Deepgram to train a model for your specific language context. If you are using the keywords boosting feature for many words, and you are using this feature often, you should look into having a custom model trained for your situation because it will perform better.</p></Alert>\n<h2 id="real-world-scenarios">Real-World Scenarios</h2>\n<p>The fundamental difference between keywords and search is that search will <em>return a data object that gives you possible matches to your search terms</em>. If you just need to find a word or to know if that word was said (or not said), use <strong>search</strong>.</p>\n<p>Keywords do not return an added data object; they just <em>improve the transcription itself</em> so that you see the boosted keywords transcribed correctly in the transcription. If you want to see a better transcription - improved specifically for words important to you - use <strong>keywords</strong> (however, there are caveats to using <strong>keywords</strong>, which we will examine in more detail later).</p>\n<p>While this distinction might be becoming more clear to you, the best way to really understand when to use <strong>search</strong> versus when to use <strong>keywords</strong> is to look at some possible scenarios.</p>\n<h3 id="scenario-1-compliance">Scenario 1: Compliance</h3>\n<h4 id="your-companys-employees-speak-on-the-phone-in-a-customer-service-role-and-they-are-required-to-say-at-the-start-of-every-phone-call-this-call-is-being-recorded-for-the-purpose-of-compliance-you-want-to-check-that-they-are-actually-saying-this-phrase">Your company\u2019s employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, \u201CThis call is being recorded.\u201D For the purpose of compliance, you want to check that they are actually saying this phrase.</h4>\n<p>In this scenario, you can use <strong>search</strong> to find matches to that phrase. You will not receive just a yes/no response (as in a Boolean telling you true/false) in the response object, but you will receive the list of matches with a confidence rating. Deepgram\u2019s AI will do its best to find phrases that it thinks might match the phrase \u201CThis call is being recorded.\u201D <strong>However, this doesn\u2019t mean that all the matches are correct.</strong></p>\n<p>This is where the nuance comes in. You have a powerful tool that has helped you search through audio files to find possible matches, and then as a user or a customer, you can decide for yourself what confidence rating is more likely to return a match. You can analyze the data you get back and start to see a pattern for what confidence number is bringing back accurate matches.</p>\n<p>If it were me, I would pay attention to the confidence level that seems to be bringing back accurate matches, and then I would make an executive decision to say that this number gives us reasonable confidence that this phrase was uttered. Because every language context is different, Deepgram\u2019s general language model is going to perform differently when transcribing audio in those different contexts, so this feature can only be used as a tool to guide you to knowing whether the search term/phrase was uttered or not.</p>\n<h3 id="scenario-2-discovery">Scenario 2: Discovery</h3>\n<h4 id="your-company-is-participating-in-litigation-and-as-part-of-the-ediscovery-phase-you-are-tasked-with-producing-electronic-documents-including-audio-and-video-which-are-relevant-to-the-case">Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case</h4>\n<p>Having a list of possible word or phrase matches can be extremely useful, but <strong>search</strong> is also a powerful tool for pointing you in the direction of valuable <em>sections</em> of your audio. A perfect example of this is eDiscovery, when attorneys are required to provide documents which are likely to have a certain relevance to the case. In this situation, they wouldn\u2019t be searching for just one word or phrase, but they would be looking for language that suggests that the conversation in the audio or video file is relevant.</p>\n<p>The goal of ASR is to recognize human speech, but human speech isn\u2019t always direct, straightforward, or even completely honest. In order to comprehend what is said, humans often read between the lines. Since Deepgram\u2019s search feature isn\u2019t just a yes/no check to find a match, but actually gives you a confidence level in the response, this search tool can be combined with the human brain to come up with a combination of words and phrases that might help to find more nuanced relevant language.</p>\n<p>To me, this is one of the most exciting possibilities of Deepgram\u2019s <strong>search</strong> feature because it demonstrates how AI isn\u2019t just about building machines to think for us, it\u2019s about building machines to aid humans in thinking better.</p>\n<h3 id="scenario-3-demos">Scenario 3: Demos</h3>\n<h4 id="you-are-planning-an-important-demo-for-the-upcoming-week-and-you-want-to-use-deepgrams-speech-to-text-api-but-you-havent-trained-a-language-model-yet-the-demo-is-crucial-to-your-business-so-you-want-to-give-an-added-boost-to-deepgrams-transcription-technology-to-improve-accuracy-for-certain-words">You are planning an important demo for the upcoming week, and you want to use Deepgram\u2019s speech-to-text API, but you haven\u2019t trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram\u2019s transcription technology to improve accuracy for certain words.</h4>\n<p>The purpose of <strong>keywords</strong> is to improve the transcription output, so if you want to get a more accurate transcription (and you have not been able to train a custom model), a less powerful option would be to use keywords. You can improve the transcription by putting a list of keywords that are more difficult to recognize by ASR into the query parameter. Jargon, names, and scientific words are good examples of words that can be tricky for ASR.</p>\n<p>The <strong>keywords</strong> feature is one that requires thoughtful implementation. Keywords do on a much smaller scale what model-training does, but <strong>there are tradeoffs.</strong> It might seem like a great way to improve a transcription, but because so many factors go into transcribing audio, and because training a model is something that is done by deep learning experts, substituting keywords for model training doesn\u2019t always work very well straight out of the box. However, it can be used if you are willing to test out various keywords and adjust based on the results you see.</p>\n<p>The best approach would be to benchmark the transcription results without any keywords, then add words one by one and notice the effect. You want to aim to find that sweet spot where the transcription is coming out as accurate as possible. But due to the improvements Deepgram has made in its various models, adding keywords does not always improve the transcription. Depending on the model you are using and your unique language context, the keywords might even lower the quality of the transcription.</p>\n<p>The keywords feature works differently depending on if the words are \u2018in-vocabulary\u2019 or \u2018out-of-vocabulary\u2019 (see the <a href="https://developers.deepgram.com/documentation/features/keywords/#in-vocabulary--out-of-vocabulary-keywords">documentation</a>). If the words already exist in the model (whether you are using a general model or your own trained model), Deepgram will just put more emphasis on those keywords (which the model already knows exist), telling the AI to boost the confidence that those words were spoken. If you are using keywords to improve the transcription of words that don\u2019t exist in the general language model, keyword boosting can be slower. Sending a huge amount of keywords may affect the speed of the transcription creation. <strong>It is recommended to use 10-100 keywords, with 100 really pushing the limits.</strong> This is important for real-time transcription, but it may not be a concern to you if you are doing batch-processing of audio files.</p>\n<p>The better Deepgram\u2019s general model gets at transcribing on its own, the less powerful the keywords tool is, since more confidence will be given to the transcription itself. Using keywords might not have an effect. If you add really high intensifiers to your keywords (see the documentation about <a href="https://developers.deepgram.com/documentation/features/keywords/#intensifiers">intensifiers</a>), it might cause the transcription to overuse those keywords. You have to find the right balance, which comes with trial and error.</p>\n<p>Finally, keywords <strong>cannot boost phrases</strong>, so if you are searching for a <em>first name</em> + <em>last name</em> combination, the boosting will happen individually for those two words, and you might see that one shows up correctly while the other does not.</p>\n<p>The keywords feature is still in beta at this time, so please factor this into your usage. We are actively working to improve this feature.</p>\n<h3 id="other-common-scenarios">Other Common Scenarios</h3>\n<p>Here are some other possible scenarios for when <strong>search</strong> would be a great tool:</p>\n<ul>\n<li>Your product uses Deepgram to provide captions to real-time streams or pre-recorded videos, and you want to be able to search the captions to find a topic or point that was discussed.</li>\n<li>Searching for competitor or brand names from various types of media recordings, so you can gather information to help build strategies.</li>\n<li>Searching for specific language phrases that imply dissatisfaction (such as profanity) so you can analyze situations when customers were not happy.</li>\n</ul>\n<p>Here are some other possible scenarios for when <strong>keywords</strong> would be a great tool:</p>\n<ul>\n<li>You haven\u2019t had an opportunity to train a custom model, but you have an idea of context-unique words that can improve the transcription.</li>\n<li>You want to improve real-time transcription of a meeting by including keywords that are names of participants or company terms that will be used during the meeting.</li>\n</ul>\n<h2 id="final-thoughts">Final Thoughts</h2>\n<p>After chatting with Deepgrammers who build our Speech Recognition API, the incredible power of deep learning has become even more evident to me. Deepgram\u2019s <strong>search</strong> feature really exemplifies this. <strong>Search</strong> is a tool that can be used in so many different situations and gives us valuable information about a transcription. <strong>Keywords</strong> is more of an added layer to the already impressive Deepgram AI, used to improve the transcription.</p>\n<p>ASR is not perfect and humans are not perfect, but we can use a tool like the Deepgram <strong>search</strong> feature to get data about a transcription and make really informed decisions about it. We can use the <strong>keywords</strong> feature to prompt the Deepgram technology to learn so that it performs better for our unique situation.</p>\n<p>The AI of Deepgram isn\u2019t about to take over the world all by itself, but humans with the AI of Deepgram can build great things.</p>' };
const frontmatter = { "title": "Keywords vs Search", "description": "Compare Deepgram's search and keywords to learn which scenarios each feature works best for you", "date": "2021-12-09T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1638909280/blog/2021/12/keywords-vs-search/keywords-v-search-blog%402x.jpg", "authors": ["sandra-rodgers"], "category": "best-practice", "tags": ["search", "keywords"], "seo": { "title": "Keywords vs Search", "description": "Compare Deepgram's search and keywords to learn which scenarios each feature works best for you" }, "shorturls": { "share": "https://dpgr.am/3c18657", "twitter": "https://dpgr.am/a03e2a5", "linkedin": "https://dpgr.am/babbe84", "reddit": "https://dpgr.am/35057c5", "facebook": "https://dpgr.am/971ad63" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661453831/blog/keywords-vs-search/ograph.png" }, "astro": { "headings": [{ "depth": 2, "slug": "basic-overview", "text": "Basic Overview" }, { "depth": 3, "slug": "search", "text": "Search" }, { "depth": 3, "slug": "keywords", "text": "Keywords" }, { "depth": 2, "slug": "real-world-scenarios", "text": "Real-World Scenarios" }, { "depth": 3, "slug": "scenario-1-compliance", "text": "Scenario 1: Compliance" }, { "depth": 4, "slug": "your-companys-employees-speak-on-the-phone-in-a-customer-service-role-and-they-are-required-to-say-at-the-start-of-every-phone-call-this-call-is-being-recorded-for-the-purpose-of-compliance-you-want-to-check-that-they-are-actually-saying-this-phrase", "text": "Your company\u2019s employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, \u201CThis call is being recorded.\u201D For the purpose of compliance, you want to check that they are actually saying this phrase." }, { "depth": 3, "slug": "scenario-2-discovery", "text": "Scenario 2: Discovery" }, { "depth": 4, "slug": "your-company-is-participating-in-litigation-and-as-part-of-the-ediscovery-phase-you-are-tasked-with-producing-electronic-documents-including-audio-and-video-which-are-relevant-to-the-case", "text": "Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case" }, { "depth": 3, "slug": "scenario-3-demos", "text": "Scenario 3: Demos" }, { "depth": 4, "slug": "you-are-planning-an-important-demo-for-the-upcoming-week-and-you-want-to-use-deepgrams-speech-to-text-api-but-you-havent-trained-a-language-model-yet-the-demo-is-crucial-to-your-business-so-you-want-to-give-an-added-boost-to-deepgrams-transcription-technology-to-improve-accuracy-for-certain-words", "text": "You are planning an important demo for the upcoming week, and you want to use Deepgram\u2019s speech-to-text API, but you haven\u2019t trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram\u2019s transcription technology to improve accuracy for certain words." }, { "depth": 3, "slug": "other-common-scenarios", "text": "Other Common Scenarios" }, { "depth": 2, "slug": "final-thoughts", "text": "Final Thoughts" }], "source": `
**Search** and **keywords** are two different features of Deepgram's API that may sound similar but actually work best in different scenarios. In this blog post, I am going to help you understand these two features and give you a better idea of the very different situations in which you might use them.

To get the most benefit out of this post, I recommend you have at least skimmed through our documentation on [search](https://developers.deepgram.com/documentation/features/search/) and [keywords](https://developers.deepgram.com/documentation/features/keywords/).

## Basic Overview

### Search

Deepgram\u2019s **search** feature is a query that can be made to **find out if a word or phrase has been said** in the audio that is being transcribed. During the transcribing of the audio of a pre-recorded sound file or a stream, Deepgram can analyze the audio to find the queried word or phrase. In the JSON response, it will return information about the **start time** and **end time** of when that word was possibly uttered. It will also give a **confidence** rating to each match. So you will see the **query** (the word or phrase you searched for), and then you will see **hits** (an array of objects that give you the **confidence**, **start**, **end**, and **snippet** of each possible match to your search).

To search for matches to the word 'epistemology,' I would add this query parameter to the API call URL or provide it as an option when using a [Deepgram SDK](https://developers.deepgram.com/sdks-tools/):

**search=epistemology**

\`\`\`js
"search":[\r
  {\r
    "query":"epistemology",\r
    "hits":[\r
      {"confidence":0.9348958,"start":10.725,"end":11.485,"snippet":"is a"},\r
      {"confidence":0.9348958,"start":13.965,"end":14.764999,"snippet":"epi"},\r
      {"confidence":0.9204282,"start":4.0074897,"end":4.805,"snippet":"social epi"},\r
      {"confidence":0.27662036,"start":8.792552,"end":10.365,"snippet":"us today is"},\r
      {"confidence":0.1319444,"start":17.205,"end":18.115,"snippet":"nature of knowledge"},\r
      {"confidence":0.0885417,"start":15.285,"end":16.085,"snippet":"branch philosophy"},\r
      {"confidence":0.045138836,"start":5.1240044,"end":5.722137,"snippet":"university of"},\r
      {"confidence":0.045138836,"start":5.6025105,"end":7.4367843,"snippet":"warwick and"},\r
      {"confidence":0.0,"start":1.0168257,"end":1.9339627,"snippet":"hello this is steve"},\r
      {"confidence":0.0,"start":7.277282,"end":8.27417,"snippet":"and the question"}\r
    ]\r
  }\r
]
\`\`\`

### Keywords

The **keywords** feature is also a query, but it is used to give the Deepgram AI more information so that it can **better transcribe the audio in the request**. Keywords are words that can be sent along with the audio, and then Deepgram will train itself to watch more closely for those words and transcribe them more accurately.

To improve transcription of the words 'epistemology' and 'ontology', I would add this query parameter to the API call URL or provide it as an option when using a [Deepgram SDK](https://developers.deepgram.com/sdks-tools/):

**keywords=epistemology\\&keywords=ontology**

It can be challenging for ASR technology to transcribe certain words, particularly jargon, names, or very uncommon words. But the beauty of deep learning is that the technology can *learn*. If you know about these context-unique words ahead of time, you can send them with the request as keywords, and Deepgram will do a better job of actually transcribing those words.

<Alert type="warning">

Keywords are a much less powerful substitute than training a custom model. The best solution is to work with Deepgram to train a model for your specific language context. If you are using the keywords boosting feature for many words, and you are using this feature often, you should look into having a custom model trained for your situation because it will perform better.

</Alert>

## Real-World Scenarios

The fundamental difference between keywords and search is that search will *return a data object that gives you possible matches to your search terms*. If you just need to find a word or to know if that word was said (or not said), use **search**.

Keywords do not return an added data object; they just *improve the transcription itself* so that you see the boosted keywords transcribed correctly in the transcription. If you want to see a better transcription - improved specifically for words important to you - use **keywords** (however, there are caveats to using **keywords**, which we will examine in more detail later).

While this distinction might be becoming more clear to you, the best way to really understand when to use **search** versus when to use **keywords** is to look at some possible scenarios.

### Scenario 1: Compliance

#### Your company's employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, "This call is being recorded." For the purpose of compliance, you want to check that they are actually saying this phrase.

In this scenario, you can use **search** to find matches to that phrase. You will not receive just a yes/no response (as in a Boolean telling you true/false) in the response object, but you will receive the list of matches with a confidence rating. Deepgram's AI will do its best to find phrases that it thinks might match the phrase "This call is being recorded." **However, this doesn't mean that all the matches are correct.**

This is where the nuance comes in. You have a powerful tool that has helped you search through audio files to find possible matches, and then as a user or a customer, you can decide for yourself what confidence rating is more likely to return a match. You can analyze the data you get back and start to see a pattern for what confidence number is bringing back accurate matches.

If it were me, I would pay attention to the confidence level that seems to be bringing back accurate matches, and then I would make an executive decision to say that this number gives us reasonable confidence that this phrase was uttered. Because every language context is different, Deepgram's general language model is going to perform differently when transcribing audio in those different contexts, so this feature can only be used as a tool to guide you to knowing whether the search term/phrase was uttered or not.

### Scenario 2: Discovery

#### Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case

Having a list of possible word or phrase matches can be extremely useful, but **search** is also a powerful tool for pointing you in the direction of valuable *sections* of your audio. A perfect example of this is eDiscovery, when attorneys are required to provide documents which are likely to have a certain relevance to the case. In this situation, they wouldn't be searching for just one word or phrase, but they would be looking for language that suggests that the conversation in the audio or video file is relevant.

The goal of ASR is to recognize human speech, but human speech isn't always direct, straightforward, or even completely honest. In order to comprehend what is said, humans often read between the lines. Since Deepgram's search feature isn't just a yes/no check to find a match, but actually gives you a confidence level in the response, this search tool can be combined with the human brain to come up with a combination of words and phrases that might help to find more nuanced relevant language.

To me, this is one of the most exciting possibilities of Deepgram's **search** feature because it demonstrates how AI isn't just about building machines to think for us, it's about building machines to aid humans in thinking better.

### Scenario 3: Demos

#### You are planning an important demo for the upcoming week, and you want to use Deepgram's speech-to-text API, but you haven't trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram's transcription technology to improve accuracy for certain words.

The purpose of **keywords** is to improve the transcription output, so if you want to get a more accurate transcription (and you have not been able to train a custom model), a less powerful option would be to use keywords. You can improve the transcription by putting a list of keywords that are more difficult to recognize by ASR into the query parameter. Jargon, names, and scientific words are good examples of words that can be tricky for ASR.

The **keywords** feature is one that requires thoughtful implementation. Keywords do on a much smaller scale what model-training does, but **there are tradeoffs.** It might seem like a great way to improve a transcription, but because so many factors go into transcribing audio, and because training a model is something that is done by deep learning experts, substituting keywords for model training doesn't always work very well straight out of the box. However, it can be used if you are willing to test out various keywords and adjust based on the results you see.

The best approach would be to benchmark the transcription results without any keywords, then add words one by one and notice the effect. You want to aim to find that sweet spot where the transcription is coming out as accurate as possible. But due to the improvements Deepgram has made in its various models, adding keywords does not always improve the transcription. Depending on the model you are using and your unique language context, the keywords might even lower the quality of the transcription.

The keywords feature works differently depending on if the words are 'in-vocabulary' or 'out-of-vocabulary' (see the [documentation](https://developers.deepgram.com/documentation/features/keywords/#in-vocabulary--out-of-vocabulary-keywords)). If the words already exist in the model (whether you are using a general model or your own trained model), Deepgram will just put more emphasis on those keywords (which the model already knows exist), telling the AI to boost the confidence that those words were spoken. If you are using keywords to improve the transcription of words that don't exist in the general language model, keyword boosting can be slower. Sending a huge amount of keywords may affect the speed of the transcription creation. **It is recommended to use 10-100 keywords, with 100 really pushing the limits.** This is important for real-time transcription, but it may not be a concern to you if you are doing batch-processing of audio files.

The better Deepgram's general model gets at transcribing on its own, the less powerful the keywords tool is, since more confidence will be given to the transcription itself. Using keywords might not have an effect. If you add really high intensifiers to your keywords (see the documentation about [intensifiers](https://developers.deepgram.com/documentation/features/keywords/#intensifiers)), it might cause the transcription to overuse those keywords. You have to find the right balance, which comes with trial and error.

Finally, keywords **cannot boost phrases**, so if you are searching for a *first name* + *last name* combination, the boosting will happen individually for those two words, and you might see that one shows up correctly while the other does not.

The keywords feature is still in beta at this time, so please factor this into your usage. We are actively working to improve this feature.

### Other Common Scenarios

Here are some other possible scenarios for when **search** would be a great tool:

*   Your product uses Deepgram to provide captions to real-time streams or pre-recorded videos, and you want to be able to search the captions to find a topic or point that was discussed.
*   Searching for competitor or brand names from various types of media recordings, so you can gather information to help build strategies.
*   Searching for specific language phrases that imply dissatisfaction (such as profanity) so you can analyze situations when customers were not happy.

Here are some other possible scenarios for when **keywords** would be a great tool:

*   You haven't had an opportunity to train a custom model, but you have an idea of context-unique words that can improve the transcription.
*   You want to improve real-time transcription of a meeting by including keywords that are names of participants or company terms that will be used during the meeting.

## Final Thoughts

After chatting with Deepgrammers who build our Speech Recognition API, the incredible power of deep learning has become even more evident to me. Deepgram's **search** feature really exemplifies this. **Search** is a tool that can be used in so many different situations and gives us valuable information about a transcription. **Keywords** is more of an added layer to the already impressive Deepgram AI, used to improve the transcription.

ASR is not perfect and humans are not perfect, but we can use a tool like the Deepgram **search** feature to get data about a transcription and make really informed decisions about it. We can use the **keywords** feature to prompt the Deepgram technology to learn so that it performs better for our unique situation.

The AI of Deepgram isn't about to take over the world all by itself, but humans with the AI of Deepgram can build great things.

        `, "html": '<p><strong>Search</strong> and <strong>keywords</strong> are two different features of Deepgram\u2019s API that may sound similar but actually work best in different scenarios. In this blog post, I am going to help you understand these two features and give you a better idea of the very different situations in which you might use them.</p>\n<p>To get the most benefit out of this post, I recommend you have at least skimmed through our documentation on <a href="https://developers.deepgram.com/documentation/features/search/">search</a> and <a href="https://developers.deepgram.com/documentation/features/keywords/">keywords</a>.</p>\n<h2 id="basic-overview">Basic Overview</h2>\n<h3 id="search">Search</h3>\n<p>Deepgram\u2019s <strong>search</strong> feature is a query that can be made to <strong>find out if a word or phrase has been said</strong> in the audio that is being transcribed. During the transcribing of the audio of a pre-recorded sound file or a stream, Deepgram can analyze the audio to find the queried word or phrase. In the JSON response, it will return information about the <strong>start time</strong> and <strong>end time</strong> of when that word was possibly uttered. It will also give a <strong>confidence</strong> rating to each match. So you will see the <strong>query</strong> (the word or phrase you searched for), and then you will see <strong>hits</strong> (an array of objects that give you the <strong>confidence</strong>, <strong>start</strong>, <strong>end</strong>, and <strong>snippet</strong> of each possible match to your search).</p>\n<p>To search for matches to the word \u2018epistemology,\u2019 I would add this query parameter to the API call URL or provide it as an option when using a <a href="https://developers.deepgram.com/sdks-tools/">Deepgram SDK</a>:</p>\n<p><strong>search=epistemology</strong></p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;search&quot;</span><span style="color: #C9D1D9">:[</span></span>\n<span class="line"><span style="color: #C9D1D9">  {</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #A5D6FF">&quot;query&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;epistemology&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #A5D6FF">&quot;hits&quot;</span><span style="color: #C9D1D9">:[</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9348958</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">10.725</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">11.485</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;is a&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9348958</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">13.965</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">14.764999</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;epi&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9204282</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">4.0074897</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">4.805</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;social epi&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.27662036</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">8.792552</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">10.365</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;us today is&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.1319444</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">17.205</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">18.115</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;nature of knowledge&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0885417</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">15.285</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">16.085</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;branch philosophy&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.045138836</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.1240044</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.722137</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;university of&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.045138836</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.6025105</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">7.4367843</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;warwick and&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">1.0168257</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">1.9339627</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;hello this is steve&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">7.277282</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">8.27417</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;and the question&quot;</span><span style="color: #C9D1D9">}</span></span>\n<span class="line"><span style="color: #C9D1D9">    ]</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">]</span></span></code></pre>\n<h3 id="keywords">Keywords</h3>\n<p>The <strong>keywords</strong> feature is also a query, but it is used to give the Deepgram AI more information so that it can <strong>better transcribe the audio in the request</strong>. Keywords are words that can be sent along with the audio, and then Deepgram will train itself to watch more closely for those words and transcribe them more accurately.</p>\n<p>To improve transcription of the words \u2018epistemology\u2019 and \u2018ontology\u2019, I would add this query parameter to the API call URL or provide it as an option when using a <a href="https://developers.deepgram.com/sdks-tools/">Deepgram SDK</a>:</p>\n<p><strong>keywords=epistemology&keywords=ontology</strong></p>\n<p>It can be challenging for ASR technology to transcribe certain words, particularly jargon, names, or very uncommon words. But the beauty of deep learning is that the technology can <em>learn</em>. If you know about these context-unique words ahead of time, you can send them with the request as keywords, and Deepgram will do a better job of actually transcribing those words.</p>\n<Alert type="warning"><p>Keywords are a much less powerful substitute than training a custom model. The best solution is to work with Deepgram to train a model for your specific language context. If you are using the keywords boosting feature for many words, and you are using this feature often, you should look into having a custom model trained for your situation because it will perform better.</p></Alert>\n<h2 id="real-world-scenarios">Real-World Scenarios</h2>\n<p>The fundamental difference between keywords and search is that search will <em>return a data object that gives you possible matches to your search terms</em>. If you just need to find a word or to know if that word was said (or not said), use <strong>search</strong>.</p>\n<p>Keywords do not return an added data object; they just <em>improve the transcription itself</em> so that you see the boosted keywords transcribed correctly in the transcription. If you want to see a better transcription - improved specifically for words important to you - use <strong>keywords</strong> (however, there are caveats to using <strong>keywords</strong>, which we will examine in more detail later).</p>\n<p>While this distinction might be becoming more clear to you, the best way to really understand when to use <strong>search</strong> versus when to use <strong>keywords</strong> is to look at some possible scenarios.</p>\n<h3 id="scenario-1-compliance">Scenario 1: Compliance</h3>\n<h4 id="your-companys-employees-speak-on-the-phone-in-a-customer-service-role-and-they-are-required-to-say-at-the-start-of-every-phone-call-this-call-is-being-recorded-for-the-purpose-of-compliance-you-want-to-check-that-they-are-actually-saying-this-phrase">Your company\u2019s employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, \u201CThis call is being recorded.\u201D For the purpose of compliance, you want to check that they are actually saying this phrase.</h4>\n<p>In this scenario, you can use <strong>search</strong> to find matches to that phrase. You will not receive just a yes/no response (as in a Boolean telling you true/false) in the response object, but you will receive the list of matches with a confidence rating. Deepgram\u2019s AI will do its best to find phrases that it thinks might match the phrase \u201CThis call is being recorded.\u201D <strong>However, this doesn\u2019t mean that all the matches are correct.</strong></p>\n<p>This is where the nuance comes in. You have a powerful tool that has helped you search through audio files to find possible matches, and then as a user or a customer, you can decide for yourself what confidence rating is more likely to return a match. You can analyze the data you get back and start to see a pattern for what confidence number is bringing back accurate matches.</p>\n<p>If it were me, I would pay attention to the confidence level that seems to be bringing back accurate matches, and then I would make an executive decision to say that this number gives us reasonable confidence that this phrase was uttered. Because every language context is different, Deepgram\u2019s general language model is going to perform differently when transcribing audio in those different contexts, so this feature can only be used as a tool to guide you to knowing whether the search term/phrase was uttered or not.</p>\n<h3 id="scenario-2-discovery">Scenario 2: Discovery</h3>\n<h4 id="your-company-is-participating-in-litigation-and-as-part-of-the-ediscovery-phase-you-are-tasked-with-producing-electronic-documents-including-audio-and-video-which-are-relevant-to-the-case">Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case</h4>\n<p>Having a list of possible word or phrase matches can be extremely useful, but <strong>search</strong> is also a powerful tool for pointing you in the direction of valuable <em>sections</em> of your audio. A perfect example of this is eDiscovery, when attorneys are required to provide documents which are likely to have a certain relevance to the case. In this situation, they wouldn\u2019t be searching for just one word or phrase, but they would be looking for language that suggests that the conversation in the audio or video file is relevant.</p>\n<p>The goal of ASR is to recognize human speech, but human speech isn\u2019t always direct, straightforward, or even completely honest. In order to comprehend what is said, humans often read between the lines. Since Deepgram\u2019s search feature isn\u2019t just a yes/no check to find a match, but actually gives you a confidence level in the response, this search tool can be combined with the human brain to come up with a combination of words and phrases that might help to find more nuanced relevant language.</p>\n<p>To me, this is one of the most exciting possibilities of Deepgram\u2019s <strong>search</strong> feature because it demonstrates how AI isn\u2019t just about building machines to think for us, it\u2019s about building machines to aid humans in thinking better.</p>\n<h3 id="scenario-3-demos">Scenario 3: Demos</h3>\n<h4 id="you-are-planning-an-important-demo-for-the-upcoming-week-and-you-want-to-use-deepgrams-speech-to-text-api-but-you-havent-trained-a-language-model-yet-the-demo-is-crucial-to-your-business-so-you-want-to-give-an-added-boost-to-deepgrams-transcription-technology-to-improve-accuracy-for-certain-words">You are planning an important demo for the upcoming week, and you want to use Deepgram\u2019s speech-to-text API, but you haven\u2019t trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram\u2019s transcription technology to improve accuracy for certain words.</h4>\n<p>The purpose of <strong>keywords</strong> is to improve the transcription output, so if you want to get a more accurate transcription (and you have not been able to train a custom model), a less powerful option would be to use keywords. You can improve the transcription by putting a list of keywords that are more difficult to recognize by ASR into the query parameter. Jargon, names, and scientific words are good examples of words that can be tricky for ASR.</p>\n<p>The <strong>keywords</strong> feature is one that requires thoughtful implementation. Keywords do on a much smaller scale what model-training does, but <strong>there are tradeoffs.</strong> It might seem like a great way to improve a transcription, but because so many factors go into transcribing audio, and because training a model is something that is done by deep learning experts, substituting keywords for model training doesn\u2019t always work very well straight out of the box. However, it can be used if you are willing to test out various keywords and adjust based on the results you see.</p>\n<p>The best approach would be to benchmark the transcription results without any keywords, then add words one by one and notice the effect. You want to aim to find that sweet spot where the transcription is coming out as accurate as possible. But due to the improvements Deepgram has made in its various models, adding keywords does not always improve the transcription. Depending on the model you are using and your unique language context, the keywords might even lower the quality of the transcription.</p>\n<p>The keywords feature works differently depending on if the words are \u2018in-vocabulary\u2019 or \u2018out-of-vocabulary\u2019 (see the <a href="https://developers.deepgram.com/documentation/features/keywords/#in-vocabulary--out-of-vocabulary-keywords">documentation</a>). If the words already exist in the model (whether you are using a general model or your own trained model), Deepgram will just put more emphasis on those keywords (which the model already knows exist), telling the AI to boost the confidence that those words were spoken. If you are using keywords to improve the transcription of words that don\u2019t exist in the general language model, keyword boosting can be slower. Sending a huge amount of keywords may affect the speed of the transcription creation. <strong>It is recommended to use 10-100 keywords, with 100 really pushing the limits.</strong> This is important for real-time transcription, but it may not be a concern to you if you are doing batch-processing of audio files.</p>\n<p>The better Deepgram\u2019s general model gets at transcribing on its own, the less powerful the keywords tool is, since more confidence will be given to the transcription itself. Using keywords might not have an effect. If you add really high intensifiers to your keywords (see the documentation about <a href="https://developers.deepgram.com/documentation/features/keywords/#intensifiers">intensifiers</a>), it might cause the transcription to overuse those keywords. You have to find the right balance, which comes with trial and error.</p>\n<p>Finally, keywords <strong>cannot boost phrases</strong>, so if you are searching for a <em>first name</em> + <em>last name</em> combination, the boosting will happen individually for those two words, and you might see that one shows up correctly while the other does not.</p>\n<p>The keywords feature is still in beta at this time, so please factor this into your usage. We are actively working to improve this feature.</p>\n<h3 id="other-common-scenarios">Other Common Scenarios</h3>\n<p>Here are some other possible scenarios for when <strong>search</strong> would be a great tool:</p>\n<ul>\n<li>Your product uses Deepgram to provide captions to real-time streams or pre-recorded videos, and you want to be able to search the captions to find a topic or point that was discussed.</li>\n<li>Searching for competitor or brand names from various types of media recordings, so you can gather information to help build strategies.</li>\n<li>Searching for specific language phrases that imply dissatisfaction (such as profanity) so you can analyze situations when customers were not happy.</li>\n</ul>\n<p>Here are some other possible scenarios for when <strong>keywords</strong> would be a great tool:</p>\n<ul>\n<li>You haven\u2019t had an opportunity to train a custom model, but you have an idea of context-unique words that can improve the transcription.</li>\n<li>You want to improve real-time transcription of a meeting by including keywords that are names of participants or company terms that will be used during the meeting.</li>\n</ul>\n<h2 id="final-thoughts">Final Thoughts</h2>\n<p>After chatting with Deepgrammers who build our Speech Recognition API, the incredible power of deep learning has become even more evident to me. Deepgram\u2019s <strong>search</strong> feature really exemplifies this. <strong>Search</strong> is a tool that can be used in so many different situations and gives us valuable information about a transcription. <strong>Keywords</strong> is more of an added layer to the already impressive Deepgram AI, used to improve the transcription.</p>\n<p>ASR is not perfect and humans are not perfect, but we can use a tool like the Deepgram <strong>search</strong> feature to get data about a transcription and make really informed decisions about it. We can use the <strong>keywords</strong> feature to prompt the Deepgram technology to learn so that it performs better for our unique situation.</p>\n<p>The AI of Deepgram isn\u2019t about to take over the world all by itself, but humans with the AI of Deepgram can build great things.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/keywords-vs-search/index.md" };
function rawContent() {
  return `
**Search** and **keywords** are two different features of Deepgram's API that may sound similar but actually work best in different scenarios. In this blog post, I am going to help you understand these two features and give you a better idea of the very different situations in which you might use them.

To get the most benefit out of this post, I recommend you have at least skimmed through our documentation on [search](https://developers.deepgram.com/documentation/features/search/) and [keywords](https://developers.deepgram.com/documentation/features/keywords/).

## Basic Overview

### Search

Deepgram\u2019s **search** feature is a query that can be made to **find out if a word or phrase has been said** in the audio that is being transcribed. During the transcribing of the audio of a pre-recorded sound file or a stream, Deepgram can analyze the audio to find the queried word or phrase. In the JSON response, it will return information about the **start time** and **end time** of when that word was possibly uttered. It will also give a **confidence** rating to each match. So you will see the **query** (the word or phrase you searched for), and then you will see **hits** (an array of objects that give you the **confidence**, **start**, **end**, and **snippet** of each possible match to your search).

To search for matches to the word 'epistemology,' I would add this query parameter to the API call URL or provide it as an option when using a [Deepgram SDK](https://developers.deepgram.com/sdks-tools/):

**search=epistemology**

\`\`\`js
"search":[\r
  {\r
    "query":"epistemology",\r
    "hits":[\r
      {"confidence":0.9348958,"start":10.725,"end":11.485,"snippet":"is a"},\r
      {"confidence":0.9348958,"start":13.965,"end":14.764999,"snippet":"epi"},\r
      {"confidence":0.9204282,"start":4.0074897,"end":4.805,"snippet":"social epi"},\r
      {"confidence":0.27662036,"start":8.792552,"end":10.365,"snippet":"us today is"},\r
      {"confidence":0.1319444,"start":17.205,"end":18.115,"snippet":"nature of knowledge"},\r
      {"confidence":0.0885417,"start":15.285,"end":16.085,"snippet":"branch philosophy"},\r
      {"confidence":0.045138836,"start":5.1240044,"end":5.722137,"snippet":"university of"},\r
      {"confidence":0.045138836,"start":5.6025105,"end":7.4367843,"snippet":"warwick and"},\r
      {"confidence":0.0,"start":1.0168257,"end":1.9339627,"snippet":"hello this is steve"},\r
      {"confidence":0.0,"start":7.277282,"end":8.27417,"snippet":"and the question"}\r
    ]\r
  }\r
]
\`\`\`

### Keywords

The **keywords** feature is also a query, but it is used to give the Deepgram AI more information so that it can **better transcribe the audio in the request**. Keywords are words that can be sent along with the audio, and then Deepgram will train itself to watch more closely for those words and transcribe them more accurately.

To improve transcription of the words 'epistemology' and 'ontology', I would add this query parameter to the API call URL or provide it as an option when using a [Deepgram SDK](https://developers.deepgram.com/sdks-tools/):

**keywords=epistemology\\&keywords=ontology**

It can be challenging for ASR technology to transcribe certain words, particularly jargon, names, or very uncommon words. But the beauty of deep learning is that the technology can *learn*. If you know about these context-unique words ahead of time, you can send them with the request as keywords, and Deepgram will do a better job of actually transcribing those words.

<Alert type="warning">

Keywords are a much less powerful substitute than training a custom model. The best solution is to work with Deepgram to train a model for your specific language context. If you are using the keywords boosting feature for many words, and you are using this feature often, you should look into having a custom model trained for your situation because it will perform better.

</Alert>

## Real-World Scenarios

The fundamental difference between keywords and search is that search will *return a data object that gives you possible matches to your search terms*. If you just need to find a word or to know if that word was said (or not said), use **search**.

Keywords do not return an added data object; they just *improve the transcription itself* so that you see the boosted keywords transcribed correctly in the transcription. If you want to see a better transcription - improved specifically for words important to you - use **keywords** (however, there are caveats to using **keywords**, which we will examine in more detail later).

While this distinction might be becoming more clear to you, the best way to really understand when to use **search** versus when to use **keywords** is to look at some possible scenarios.

### Scenario 1: Compliance

#### Your company's employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, "This call is being recorded." For the purpose of compliance, you want to check that they are actually saying this phrase.

In this scenario, you can use **search** to find matches to that phrase. You will not receive just a yes/no response (as in a Boolean telling you true/false) in the response object, but you will receive the list of matches with a confidence rating. Deepgram's AI will do its best to find phrases that it thinks might match the phrase "This call is being recorded." **However, this doesn't mean that all the matches are correct.**

This is where the nuance comes in. You have a powerful tool that has helped you search through audio files to find possible matches, and then as a user or a customer, you can decide for yourself what confidence rating is more likely to return a match. You can analyze the data you get back and start to see a pattern for what confidence number is bringing back accurate matches.

If it were me, I would pay attention to the confidence level that seems to be bringing back accurate matches, and then I would make an executive decision to say that this number gives us reasonable confidence that this phrase was uttered. Because every language context is different, Deepgram's general language model is going to perform differently when transcribing audio in those different contexts, so this feature can only be used as a tool to guide you to knowing whether the search term/phrase was uttered or not.

### Scenario 2: Discovery

#### Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case

Having a list of possible word or phrase matches can be extremely useful, but **search** is also a powerful tool for pointing you in the direction of valuable *sections* of your audio. A perfect example of this is eDiscovery, when attorneys are required to provide documents which are likely to have a certain relevance to the case. In this situation, they wouldn't be searching for just one word or phrase, but they would be looking for language that suggests that the conversation in the audio or video file is relevant.

The goal of ASR is to recognize human speech, but human speech isn't always direct, straightforward, or even completely honest. In order to comprehend what is said, humans often read between the lines. Since Deepgram's search feature isn't just a yes/no check to find a match, but actually gives you a confidence level in the response, this search tool can be combined with the human brain to come up with a combination of words and phrases that might help to find more nuanced relevant language.

To me, this is one of the most exciting possibilities of Deepgram's **search** feature because it demonstrates how AI isn't just about building machines to think for us, it's about building machines to aid humans in thinking better.

### Scenario 3: Demos

#### You are planning an important demo for the upcoming week, and you want to use Deepgram's speech-to-text API, but you haven't trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram's transcription technology to improve accuracy for certain words.

The purpose of **keywords** is to improve the transcription output, so if you want to get a more accurate transcription (and you have not been able to train a custom model), a less powerful option would be to use keywords. You can improve the transcription by putting a list of keywords that are more difficult to recognize by ASR into the query parameter. Jargon, names, and scientific words are good examples of words that can be tricky for ASR.

The **keywords** feature is one that requires thoughtful implementation. Keywords do on a much smaller scale what model-training does, but **there are tradeoffs.** It might seem like a great way to improve a transcription, but because so many factors go into transcribing audio, and because training a model is something that is done by deep learning experts, substituting keywords for model training doesn't always work very well straight out of the box. However, it can be used if you are willing to test out various keywords and adjust based on the results you see.

The best approach would be to benchmark the transcription results without any keywords, then add words one by one and notice the effect. You want to aim to find that sweet spot where the transcription is coming out as accurate as possible. But due to the improvements Deepgram has made in its various models, adding keywords does not always improve the transcription. Depending on the model you are using and your unique language context, the keywords might even lower the quality of the transcription.

The keywords feature works differently depending on if the words are 'in-vocabulary' or 'out-of-vocabulary' (see the [documentation](https://developers.deepgram.com/documentation/features/keywords/#in-vocabulary--out-of-vocabulary-keywords)). If the words already exist in the model (whether you are using a general model or your own trained model), Deepgram will just put more emphasis on those keywords (which the model already knows exist), telling the AI to boost the confidence that those words were spoken. If you are using keywords to improve the transcription of words that don't exist in the general language model, keyword boosting can be slower. Sending a huge amount of keywords may affect the speed of the transcription creation. **It is recommended to use 10-100 keywords, with 100 really pushing the limits.** This is important for real-time transcription, but it may not be a concern to you if you are doing batch-processing of audio files.

The better Deepgram's general model gets at transcribing on its own, the less powerful the keywords tool is, since more confidence will be given to the transcription itself. Using keywords might not have an effect. If you add really high intensifiers to your keywords (see the documentation about [intensifiers](https://developers.deepgram.com/documentation/features/keywords/#intensifiers)), it might cause the transcription to overuse those keywords. You have to find the right balance, which comes with trial and error.

Finally, keywords **cannot boost phrases**, so if you are searching for a *first name* + *last name* combination, the boosting will happen individually for those two words, and you might see that one shows up correctly while the other does not.

The keywords feature is still in beta at this time, so please factor this into your usage. We are actively working to improve this feature.

### Other Common Scenarios

Here are some other possible scenarios for when **search** would be a great tool:

*   Your product uses Deepgram to provide captions to real-time streams or pre-recorded videos, and you want to be able to search the captions to find a topic or point that was discussed.
*   Searching for competitor or brand names from various types of media recordings, so you can gather information to help build strategies.
*   Searching for specific language phrases that imply dissatisfaction (such as profanity) so you can analyze situations when customers were not happy.

Here are some other possible scenarios for when **keywords** would be a great tool:

*   You haven't had an opportunity to train a custom model, but you have an idea of context-unique words that can improve the transcription.
*   You want to improve real-time transcription of a meeting by including keywords that are names of participants or company terms that will be used during the meeting.

## Final Thoughts

After chatting with Deepgrammers who build our Speech Recognition API, the incredible power of deep learning has become even more evident to me. Deepgram's **search** feature really exemplifies this. **Search** is a tool that can be used in so many different situations and gives us valuable information about a transcription. **Keywords** is more of an added layer to the already impressive Deepgram AI, used to improve the transcription.

ASR is not perfect and humans are not perfect, but we can use a tool like the Deepgram **search** feature to get data about a transcription and make really informed decisions about it. We can use the **keywords** feature to prompt the Deepgram technology to learn so that it performs better for our unique situation.

The AI of Deepgram isn't about to take over the world all by itself, but humans with the AI of Deepgram can build great things.

        `;
}
function compiledContent() {
  return '<p><strong>Search</strong> and <strong>keywords</strong> are two different features of Deepgram\u2019s API that may sound similar but actually work best in different scenarios. In this blog post, I am going to help you understand these two features and give you a better idea of the very different situations in which you might use them.</p>\n<p>To get the most benefit out of this post, I recommend you have at least skimmed through our documentation on <a href="https://developers.deepgram.com/documentation/features/search/">search</a> and <a href="https://developers.deepgram.com/documentation/features/keywords/">keywords</a>.</p>\n<h2 id="basic-overview">Basic Overview</h2>\n<h3 id="search">Search</h3>\n<p>Deepgram\u2019s <strong>search</strong> feature is a query that can be made to <strong>find out if a word or phrase has been said</strong> in the audio that is being transcribed. During the transcribing of the audio of a pre-recorded sound file or a stream, Deepgram can analyze the audio to find the queried word or phrase. In the JSON response, it will return information about the <strong>start time</strong> and <strong>end time</strong> of when that word was possibly uttered. It will also give a <strong>confidence</strong> rating to each match. So you will see the <strong>query</strong> (the word or phrase you searched for), and then you will see <strong>hits</strong> (an array of objects that give you the <strong>confidence</strong>, <strong>start</strong>, <strong>end</strong>, and <strong>snippet</strong> of each possible match to your search).</p>\n<p>To search for matches to the word \u2018epistemology,\u2019 I would add this query parameter to the API call URL or provide it as an option when using a <a href="https://developers.deepgram.com/sdks-tools/">Deepgram SDK</a>:</p>\n<p><strong>search=epistemology</strong></p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;search&quot;</span><span style="color: #C9D1D9">:[</span></span>\n<span class="line"><span style="color: #C9D1D9">  {</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #A5D6FF">&quot;query&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;epistemology&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #A5D6FF">&quot;hits&quot;</span><span style="color: #C9D1D9">:[</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9348958</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">10.725</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">11.485</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;is a&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9348958</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">13.965</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">14.764999</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;epi&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9204282</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">4.0074897</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">4.805</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;social epi&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.27662036</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">8.792552</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">10.365</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;us today is&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.1319444</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">17.205</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">18.115</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;nature of knowledge&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0885417</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">15.285</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">16.085</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;branch philosophy&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.045138836</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.1240044</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.722137</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;university of&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.045138836</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.6025105</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">7.4367843</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;warwick and&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">1.0168257</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">1.9339627</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;hello this is steve&quot;</span><span style="color: #C9D1D9">},</span></span>\n<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">7.277282</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">8.27417</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;and the question&quot;</span><span style="color: #C9D1D9">}</span></span>\n<span class="line"><span style="color: #C9D1D9">    ]</span></span>\n<span class="line"><span style="color: #C9D1D9">  }</span></span>\n<span class="line"><span style="color: #C9D1D9">]</span></span></code></pre>\n<h3 id="keywords">Keywords</h3>\n<p>The <strong>keywords</strong> feature is also a query, but it is used to give the Deepgram AI more information so that it can <strong>better transcribe the audio in the request</strong>. Keywords are words that can be sent along with the audio, and then Deepgram will train itself to watch more closely for those words and transcribe them more accurately.</p>\n<p>To improve transcription of the words \u2018epistemology\u2019 and \u2018ontology\u2019, I would add this query parameter to the API call URL or provide it as an option when using a <a href="https://developers.deepgram.com/sdks-tools/">Deepgram SDK</a>:</p>\n<p><strong>keywords=epistemology&keywords=ontology</strong></p>\n<p>It can be challenging for ASR technology to transcribe certain words, particularly jargon, names, or very uncommon words. But the beauty of deep learning is that the technology can <em>learn</em>. If you know about these context-unique words ahead of time, you can send them with the request as keywords, and Deepgram will do a better job of actually transcribing those words.</p>\n<Alert type="warning"><p>Keywords are a much less powerful substitute than training a custom model. The best solution is to work with Deepgram to train a model for your specific language context. If you are using the keywords boosting feature for many words, and you are using this feature often, you should look into having a custom model trained for your situation because it will perform better.</p></Alert>\n<h2 id="real-world-scenarios">Real-World Scenarios</h2>\n<p>The fundamental difference between keywords and search is that search will <em>return a data object that gives you possible matches to your search terms</em>. If you just need to find a word or to know if that word was said (or not said), use <strong>search</strong>.</p>\n<p>Keywords do not return an added data object; they just <em>improve the transcription itself</em> so that you see the boosted keywords transcribed correctly in the transcription. If you want to see a better transcription - improved specifically for words important to you - use <strong>keywords</strong> (however, there are caveats to using <strong>keywords</strong>, which we will examine in more detail later).</p>\n<p>While this distinction might be becoming more clear to you, the best way to really understand when to use <strong>search</strong> versus when to use <strong>keywords</strong> is to look at some possible scenarios.</p>\n<h3 id="scenario-1-compliance">Scenario 1: Compliance</h3>\n<h4 id="your-companys-employees-speak-on-the-phone-in-a-customer-service-role-and-they-are-required-to-say-at-the-start-of-every-phone-call-this-call-is-being-recorded-for-the-purpose-of-compliance-you-want-to-check-that-they-are-actually-saying-this-phrase">Your company\u2019s employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, \u201CThis call is being recorded.\u201D For the purpose of compliance, you want to check that they are actually saying this phrase.</h4>\n<p>In this scenario, you can use <strong>search</strong> to find matches to that phrase. You will not receive just a yes/no response (as in a Boolean telling you true/false) in the response object, but you will receive the list of matches with a confidence rating. Deepgram\u2019s AI will do its best to find phrases that it thinks might match the phrase \u201CThis call is being recorded.\u201D <strong>However, this doesn\u2019t mean that all the matches are correct.</strong></p>\n<p>This is where the nuance comes in. You have a powerful tool that has helped you search through audio files to find possible matches, and then as a user or a customer, you can decide for yourself what confidence rating is more likely to return a match. You can analyze the data you get back and start to see a pattern for what confidence number is bringing back accurate matches.</p>\n<p>If it were me, I would pay attention to the confidence level that seems to be bringing back accurate matches, and then I would make an executive decision to say that this number gives us reasonable confidence that this phrase was uttered. Because every language context is different, Deepgram\u2019s general language model is going to perform differently when transcribing audio in those different contexts, so this feature can only be used as a tool to guide you to knowing whether the search term/phrase was uttered or not.</p>\n<h3 id="scenario-2-discovery">Scenario 2: Discovery</h3>\n<h4 id="your-company-is-participating-in-litigation-and-as-part-of-the-ediscovery-phase-you-are-tasked-with-producing-electronic-documents-including-audio-and-video-which-are-relevant-to-the-case">Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case</h4>\n<p>Having a list of possible word or phrase matches can be extremely useful, but <strong>search</strong> is also a powerful tool for pointing you in the direction of valuable <em>sections</em> of your audio. A perfect example of this is eDiscovery, when attorneys are required to provide documents which are likely to have a certain relevance to the case. In this situation, they wouldn\u2019t be searching for just one word or phrase, but they would be looking for language that suggests that the conversation in the audio or video file is relevant.</p>\n<p>The goal of ASR is to recognize human speech, but human speech isn\u2019t always direct, straightforward, or even completely honest. In order to comprehend what is said, humans often read between the lines. Since Deepgram\u2019s search feature isn\u2019t just a yes/no check to find a match, but actually gives you a confidence level in the response, this search tool can be combined with the human brain to come up with a combination of words and phrases that might help to find more nuanced relevant language.</p>\n<p>To me, this is one of the most exciting possibilities of Deepgram\u2019s <strong>search</strong> feature because it demonstrates how AI isn\u2019t just about building machines to think for us, it\u2019s about building machines to aid humans in thinking better.</p>\n<h3 id="scenario-3-demos">Scenario 3: Demos</h3>\n<h4 id="you-are-planning-an-important-demo-for-the-upcoming-week-and-you-want-to-use-deepgrams-speech-to-text-api-but-you-havent-trained-a-language-model-yet-the-demo-is-crucial-to-your-business-so-you-want-to-give-an-added-boost-to-deepgrams-transcription-technology-to-improve-accuracy-for-certain-words">You are planning an important demo for the upcoming week, and you want to use Deepgram\u2019s speech-to-text API, but you haven\u2019t trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgram\u2019s transcription technology to improve accuracy for certain words.</h4>\n<p>The purpose of <strong>keywords</strong> is to improve the transcription output, so if you want to get a more accurate transcription (and you have not been able to train a custom model), a less powerful option would be to use keywords. You can improve the transcription by putting a list of keywords that are more difficult to recognize by ASR into the query parameter. Jargon, names, and scientific words are good examples of words that can be tricky for ASR.</p>\n<p>The <strong>keywords</strong> feature is one that requires thoughtful implementation. Keywords do on a much smaller scale what model-training does, but <strong>there are tradeoffs.</strong> It might seem like a great way to improve a transcription, but because so many factors go into transcribing audio, and because training a model is something that is done by deep learning experts, substituting keywords for model training doesn\u2019t always work very well straight out of the box. However, it can be used if you are willing to test out various keywords and adjust based on the results you see.</p>\n<p>The best approach would be to benchmark the transcription results without any keywords, then add words one by one and notice the effect. You want to aim to find that sweet spot where the transcription is coming out as accurate as possible. But due to the improvements Deepgram has made in its various models, adding keywords does not always improve the transcription. Depending on the model you are using and your unique language context, the keywords might even lower the quality of the transcription.</p>\n<p>The keywords feature works differently depending on if the words are \u2018in-vocabulary\u2019 or \u2018out-of-vocabulary\u2019 (see the <a href="https://developers.deepgram.com/documentation/features/keywords/#in-vocabulary--out-of-vocabulary-keywords">documentation</a>). If the words already exist in the model (whether you are using a general model or your own trained model), Deepgram will just put more emphasis on those keywords (which the model already knows exist), telling the AI to boost the confidence that those words were spoken. If you are using keywords to improve the transcription of words that don\u2019t exist in the general language model, keyword boosting can be slower. Sending a huge amount of keywords may affect the speed of the transcription creation. <strong>It is recommended to use 10-100 keywords, with 100 really pushing the limits.</strong> This is important for real-time transcription, but it may not be a concern to you if you are doing batch-processing of audio files.</p>\n<p>The better Deepgram\u2019s general model gets at transcribing on its own, the less powerful the keywords tool is, since more confidence will be given to the transcription itself. Using keywords might not have an effect. If you add really high intensifiers to your keywords (see the documentation about <a href="https://developers.deepgram.com/documentation/features/keywords/#intensifiers">intensifiers</a>), it might cause the transcription to overuse those keywords. You have to find the right balance, which comes with trial and error.</p>\n<p>Finally, keywords <strong>cannot boost phrases</strong>, so if you are searching for a <em>first name</em> + <em>last name</em> combination, the boosting will happen individually for those two words, and you might see that one shows up correctly while the other does not.</p>\n<p>The keywords feature is still in beta at this time, so please factor this into your usage. We are actively working to improve this feature.</p>\n<h3 id="other-common-scenarios">Other Common Scenarios</h3>\n<p>Here are some other possible scenarios for when <strong>search</strong> would be a great tool:</p>\n<ul>\n<li>Your product uses Deepgram to provide captions to real-time streams or pre-recorded videos, and you want to be able to search the captions to find a topic or point that was discussed.</li>\n<li>Searching for competitor or brand names from various types of media recordings, so you can gather information to help build strategies.</li>\n<li>Searching for specific language phrases that imply dissatisfaction (such as profanity) so you can analyze situations when customers were not happy.</li>\n</ul>\n<p>Here are some other possible scenarios for when <strong>keywords</strong> would be a great tool:</p>\n<ul>\n<li>You haven\u2019t had an opportunity to train a custom model, but you have an idea of context-unique words that can improve the transcription.</li>\n<li>You want to improve real-time transcription of a meeting by including keywords that are names of participants or company terms that will be used during the meeting.</li>\n</ul>\n<h2 id="final-thoughts">Final Thoughts</h2>\n<p>After chatting with Deepgrammers who build our Speech Recognition API, the incredible power of deep learning has become even more evident to me. Deepgram\u2019s <strong>search</strong> feature really exemplifies this. <strong>Search</strong> is a tool that can be used in so many different situations and gives us valuable information about a transcription. <strong>Keywords</strong> is more of an added layer to the already impressive Deepgram AI, used to improve the transcription.</p>\n<p>ASR is not perfect and humans are not perfect, but we can use a tool like the Deepgram <strong>search</strong> feature to get data about a transcription and make really informed decisions about it. We can use the <strong>keywords</strong> feature to prompt the Deepgram technology to learn so that it performs better for our unique situation.</p>\n<p>The AI of Deepgram isn\u2019t about to take over the world all by itself, but humans with the AI of Deepgram can build great things.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/keywords-vs-search/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p><strong>Search</strong> and <strong>keywords</strong> are two different features of Deepgrams API that may sound similar but actually work best in different scenarios. In this blog post, I am going to help you understand these two features and give you a better idea of the very different situations in which you might use them.</p>
<p>To get the most benefit out of this post, I recommend you have at least skimmed through our documentation on <a href="https://developers.deepgram.com/documentation/features/search/">search</a> and <a href="https://developers.deepgram.com/documentation/features/keywords/">keywords</a>.</p>
<h2 id="basic-overview">Basic Overview</h2>
<h3 id="search">Search</h3>
<p>Deepgrams <strong>search</strong> feature is a query that can be made to <strong>find out if a word or phrase has been said</strong> in the audio that is being transcribed. During the transcribing of the audio of a pre-recorded sound file or a stream, Deepgram can analyze the audio to find the queried word or phrase. In the JSON response, it will return information about the <strong>start time</strong> and <strong>end time</strong> of when that word was possibly uttered. It will also give a <strong>confidence</strong> rating to each match. So you will see the <strong>query</strong> (the word or phrase you searched for), and then you will see <strong>hits</strong> (an array of objects that give you the <strong>confidence</strong>, <strong>start</strong>, <strong>end</strong>, and <strong>snippet</strong> of each possible match to your search).</p>
<p>To search for matches to the word epistemology, I would add this query parameter to the API call URL or provide it as an option when using a <a href="https://developers.deepgram.com/sdks-tools/">Deepgram SDK</a>:</p>
<p><strong>search=epistemology</strong></p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #A5D6FF">&quot;search&quot;</span><span style="color: #C9D1D9">:[</span></span>
<span class="line"><span style="color: #C9D1D9">  {</span></span>
<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #A5D6FF">&quot;query&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;epistemology&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #C9D1D9">    </span><span style="color: #A5D6FF">&quot;hits&quot;</span><span style="color: #C9D1D9">:[</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9348958</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">10.725</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">11.485</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;is a&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9348958</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">13.965</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">14.764999</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;epi&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.9204282</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">4.0074897</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">4.805</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;social epi&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.27662036</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">8.792552</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">10.365</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;us today is&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.1319444</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">17.205</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">18.115</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;nature of knowledge&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0885417</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">15.285</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">16.085</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;branch philosophy&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.045138836</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.1240044</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.722137</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;university of&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.045138836</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">5.6025105</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">7.4367843</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;warwick and&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">1.0168257</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">1.9339627</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;hello this is steve&quot;</span><span style="color: #C9D1D9">},</span></span>
<span class="line"><span style="color: #C9D1D9">      {</span><span style="color: #A5D6FF">&quot;confidence&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">0.0</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;start&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">7.277282</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;end&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #79C0FF">8.27417</span><span style="color: #C9D1D9">,</span><span style="color: #A5D6FF">&quot;snippet&quot;</span><span style="color: #C9D1D9">:</span><span style="color: #A5D6FF">&quot;and the question&quot;</span><span style="color: #C9D1D9">}</span></span>
<span class="line"><span style="color: #C9D1D9">    ]</span></span>
<span class="line"><span style="color: #C9D1D9">  }</span></span>
<span class="line"><span style="color: #C9D1D9">]</span></span></code></pre>
<h3 id="keywords">Keywords</h3>
<p>The <strong>keywords</strong> feature is also a query, but it is used to give the Deepgram AI more information so that it can <strong>better transcribe the audio in the request</strong>. Keywords are words that can be sent along with the audio, and then Deepgram will train itself to watch more closely for those words and transcribe them more accurately.</p>
<p>To improve transcription of the words epistemology and ontology, I would add this query parameter to the API call URL or provide it as an option when using a <a href="https://developers.deepgram.com/sdks-tools/">Deepgram SDK</a>:</p>
<p><strong>keywords=epistemology&keywords=ontology</strong></p>
<p>It can be challenging for ASR technology to transcribe certain words, particularly jargon, names, or very uncommon words. But the beauty of deep learning is that the technology can <em>learn</em>. If you know about these context-unique words ahead of time, you can send them with the request as keywords, and Deepgram will do a better job of actually transcribing those words.</p>
${renderComponent($$result, "Alert", Alert, { "type": "warning" }, { "default": () => renderTemplate`<p>Keywords are a much less powerful substitute than training a custom model. The best solution is to work with Deepgram to train a model for your specific language context. If you are using the keywords boosting feature for many words, and you are using this feature often, you should look into having a custom model trained for your situation because it will perform better.</p>` })}
<h2 id="real-world-scenarios">Real-World Scenarios</h2>
<p>The fundamental difference between keywords and search is that search will <em>return a data object that gives you possible matches to your search terms</em>. If you just need to find a word or to know if that word was said (or not said), use <strong>search</strong>.</p>
<p>Keywords do not return an added data object; they just <em>improve the transcription itself</em> so that you see the boosted keywords transcribed correctly in the transcription. If you want to see a better transcription - improved specifically for words important to you - use <strong>keywords</strong> (however, there are caveats to using <strong>keywords</strong>, which we will examine in more detail later).</p>
<p>While this distinction might be becoming more clear to you, the best way to really understand when to use <strong>search</strong> versus when to use <strong>keywords</strong> is to look at some possible scenarios.</p>
<h3 id="scenario-1-compliance">Scenario 1: Compliance</h3>
<h4 id="your-companys-employees-speak-on-the-phone-in-a-customer-service-role-and-they-are-required-to-say-at-the-start-of-every-phone-call-this-call-is-being-recorded-for-the-purpose-of-compliance-you-want-to-check-that-they-are-actually-saying-this-phrase">Your companys employees speak on the phone in a customer service role, and they are required to say at the start of every phone call, This call is being recorded. For the purpose of compliance, you want to check that they are actually saying this phrase.</h4>
<p>In this scenario, you can use <strong>search</strong> to find matches to that phrase. You will not receive just a yes/no response (as in a Boolean telling you true/false) in the response object, but you will receive the list of matches with a confidence rating. Deepgrams AI will do its best to find phrases that it thinks might match the phrase This call is being recorded. <strong>However, this doesnt mean that all the matches are correct.</strong></p>
<p>This is where the nuance comes in. You have a powerful tool that has helped you search through audio files to find possible matches, and then as a user or a customer, you can decide for yourself what confidence rating is more likely to return a match. You can analyze the data you get back and start to see a pattern for what confidence number is bringing back accurate matches.</p>
<p>If it were me, I would pay attention to the confidence level that seems to be bringing back accurate matches, and then I would make an executive decision to say that this number gives us reasonable confidence that this phrase was uttered. Because every language context is different, Deepgrams general language model is going to perform differently when transcribing audio in those different contexts, so this feature can only be used as a tool to guide you to knowing whether the search term/phrase was uttered or not.</p>
<h3 id="scenario-2-discovery">Scenario 2: Discovery</h3>
<h4 id="your-company-is-participating-in-litigation-and-as-part-of-the-ediscovery-phase-you-are-tasked-with-producing-electronic-documents-including-audio-and-video-which-are-relevant-to-the-case">Your company is participating in litigation, and as part of the eDiscovery phase, you are tasked with producing electronic documents (including audio and video) which are relevant to the case</h4>
<p>Having a list of possible word or phrase matches can be extremely useful, but <strong>search</strong> is also a powerful tool for pointing you in the direction of valuable <em>sections</em> of your audio. A perfect example of this is eDiscovery, when attorneys are required to provide documents which are likely to have a certain relevance to the case. In this situation, they wouldnt be searching for just one word or phrase, but they would be looking for language that suggests that the conversation in the audio or video file is relevant.</p>
<p>The goal of ASR is to recognize human speech, but human speech isnt always direct, straightforward, or even completely honest. In order to comprehend what is said, humans often read between the lines. Since Deepgrams search feature isnt just a yes/no check to find a match, but actually gives you a confidence level in the response, this search tool can be combined with the human brain to come up with a combination of words and phrases that might help to find more nuanced relevant language.</p>
<p>To me, this is one of the most exciting possibilities of Deepgrams <strong>search</strong> feature because it demonstrates how AI isnt just about building machines to think for us, its about building machines to aid humans in thinking better.</p>
<h3 id="scenario-3-demos">Scenario 3: Demos</h3>
<h4 id="you-are-planning-an-important-demo-for-the-upcoming-week-and-you-want-to-use-deepgrams-speech-to-text-api-but-you-havent-trained-a-language-model-yet-the-demo-is-crucial-to-your-business-so-you-want-to-give-an-added-boost-to-deepgrams-transcription-technology-to-improve-accuracy-for-certain-words">You are planning an important demo for the upcoming week, and you want to use Deepgrams speech-to-text API, but you havent trained a language model yet. The demo is crucial to your business, so you want to give an added boost to Deepgrams transcription technology to improve accuracy for certain words.</h4>
<p>The purpose of <strong>keywords</strong> is to improve the transcription output, so if you want to get a more accurate transcription (and you have not been able to train a custom model), a less powerful option would be to use keywords. You can improve the transcription by putting a list of keywords that are more difficult to recognize by ASR into the query parameter. Jargon, names, and scientific words are good examples of words that can be tricky for ASR.</p>
<p>The <strong>keywords</strong> feature is one that requires thoughtful implementation. Keywords do on a much smaller scale what model-training does, but <strong>there are tradeoffs.</strong> It might seem like a great way to improve a transcription, but because so many factors go into transcribing audio, and because training a model is something that is done by deep learning experts, substituting keywords for model training doesnt always work very well straight out of the box. However, it can be used if you are willing to test out various keywords and adjust based on the results you see.</p>
<p>The best approach would be to benchmark the transcription results without any keywords, then add words one by one and notice the effect. You want to aim to find that sweet spot where the transcription is coming out as accurate as possible. But due to the improvements Deepgram has made in its various models, adding keywords does not always improve the transcription. Depending on the model you are using and your unique language context, the keywords might even lower the quality of the transcription.</p>
<p>The keywords feature works differently depending on if the words are in-vocabulary or out-of-vocabulary (see the <a href="https://developers.deepgram.com/documentation/features/keywords/#in-vocabulary--out-of-vocabulary-keywords">documentation</a>). If the words already exist in the model (whether you are using a general model or your own trained model), Deepgram will just put more emphasis on those keywords (which the model already knows exist), telling the AI to boost the confidence that those words were spoken. If you are using keywords to improve the transcription of words that dont exist in the general language model, keyword boosting can be slower. Sending a huge amount of keywords may affect the speed of the transcription creation. <strong>It is recommended to use 10-100 keywords, with 100 really pushing the limits.</strong> This is important for real-time transcription, but it may not be a concern to you if you are doing batch-processing of audio files.</p>
<p>The better Deepgrams general model gets at transcribing on its own, the less powerful the keywords tool is, since more confidence will be given to the transcription itself. Using keywords might not have an effect. If you add really high intensifiers to your keywords (see the documentation about <a href="https://developers.deepgram.com/documentation/features/keywords/#intensifiers">intensifiers</a>), it might cause the transcription to overuse those keywords. You have to find the right balance, which comes with trial and error.</p>
<p>Finally, keywords <strong>cannot boost phrases</strong>, so if you are searching for a <em>first name</em> + <em>last name</em> combination, the boosting will happen individually for those two words, and you might see that one shows up correctly while the other does not.</p>
<p>The keywords feature is still in beta at this time, so please factor this into your usage. We are actively working to improve this feature.</p>
<h3 id="other-common-scenarios">Other Common Scenarios</h3>
<p>Here are some other possible scenarios for when <strong>search</strong> would be a great tool:</p>
<ul>
<li>Your product uses Deepgram to provide captions to real-time streams or pre-recorded videos, and you want to be able to search the captions to find a topic or point that was discussed.</li>
<li>Searching for competitor or brand names from various types of media recordings, so you can gather information to help build strategies.</li>
<li>Searching for specific language phrases that imply dissatisfaction (such as profanity) so you can analyze situations when customers were not happy.</li>
</ul>
<p>Here are some other possible scenarios for when <strong>keywords</strong> would be a great tool:</p>
<ul>
<li>You havent had an opportunity to train a custom model, but you have an idea of context-unique words that can improve the transcription.</li>
<li>You want to improve real-time transcription of a meeting by including keywords that are names of participants or company terms that will be used during the meeting.</li>
</ul>
<h2 id="final-thoughts">Final Thoughts</h2>
<p>After chatting with Deepgrammers who build our Speech Recognition API, the incredible power of deep learning has become even more evident to me. Deepgrams <strong>search</strong> feature really exemplifies this. <strong>Search</strong> is a tool that can be used in so many different situations and gives us valuable information about a transcription. <strong>Keywords</strong> is more of an added layer to the already impressive Deepgram AI, used to improve the transcription.</p>
<p>ASR is not perfect and humans are not perfect, but we can use a tool like the Deepgram <strong>search</strong> feature to get data about a transcription and make really informed decisions about it. We can use the <strong>keywords</strong> feature to prompt the Deepgram technology to learn so that it performs better for our unique situation.</p>
<p>The AI of Deepgram isnt about to take over the world all by itself, but humans with the AI of Deepgram can build great things.</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/keywords-vs-search/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
