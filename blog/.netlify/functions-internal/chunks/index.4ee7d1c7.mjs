import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                           *//* empty css                          */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [], "source": "*This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.*\xA0\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0**94% accurate.**\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0[contact us](https://deepgram.com/contact-us/).*\n\n\\[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.\n\nSo I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.\n\nI\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.\n\nAnd so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.\n\nLooks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.\n\nOk. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m\n\n> I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.\n\nSo what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.\n\nAnd there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.\n\nAnd so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.\n\nSo take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.\n\nYou can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.\n\nBut if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.\n\nIt\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.\n\nAnd so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.\n\nAudio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.\n\nBut if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.\n\nSo, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.\n\nAnd Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.\n\nSo NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.\n\nAnd as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.\n\nAnd, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.\n\nOr if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.\n\nThese things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.\n\nI\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.\n\nSo Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.\n\nSo tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.\n\n\n\n ![](https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png)\n\n\n\n*To learn more about real-time voice analysis, check out [Cyrano.ai](https://www.cyrano.ai/).*", "html": '<p><em>This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.</em>\xA0</p>\n<p><em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0<strong>94% accurate.</strong>\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0<a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.</p>\n<p>So I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.</p>\n<p>I\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.</p>\n<p>And so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.</p>\n<p>Looks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.</p>\n<p>Ok. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m</p>\n<blockquote>\n<p>I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.</p>\n</blockquote>\n<p>So what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.</p>\n<p>And there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.</p>\n<p>And so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.</p>\n<p>So take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.</p>\n<p>You can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.</p>\n<p>But if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.</p>\n<p>It\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.</p>\n<p>And so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.</p>\n<p>Audio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.</p>\n<p>But if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.</p>\n<p>So, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.</p>\n<p>And Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.</p>\n<p>So NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.</p>\n<p>And as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.</p>\n<p>And, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.</p>\n<p>Or if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.</p>\n<p>These things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.</p>\n<p>I\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.</p>\n<p>So Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.</p>\n<p>So tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png" alt=""></p>\n<p><em>To learn more about real-time voice analysis, check out <a href="https://www.cyrano.ai/">Cyrano.ai</a>.</em></p>' };
const frontmatter = { "title": "Building the Future of Voice - Scott Stephenson, CEO, Deepgram - Project Voice X", "description": "Building the future of Voice presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.\xA0", "date": "2021-12-09T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981393/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/proj-voice-x-session-scott-stephenson-blog-thumb-5.png", "authors": ["claudia-ring"], "category": "speech-trends", "tags": ["project-voice-x"], "seo": { "title": "Building the Future of Voice - Scott Stephenson, CEO, Deepgram - Project Voice X", "description": "Building the future of Voice presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.\xA0" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981393/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/proj-voice-x-session-scott-stephenson-blog-thumb-5.png" }, "shorturls": { "share": "https://dpgr.am/124b7f6", "twitter": "https://dpgr.am/90b1aa4", "linkedin": "https://dpgr.am/350b8f6", "reddit": "https://dpgr.am/b61b7c2", "facebook": "https://dpgr.am/fcbf36f" }, "astro": { "headings": [], "source": "*This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.*\xA0\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0**94% accurate.**\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0[contact us](https://deepgram.com/contact-us/).*\n\n\\[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.\n\nSo I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.\n\nI\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.\n\nAnd so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.\n\nLooks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.\n\nOk. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m\n\n> I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.\n\nSo what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.\n\nAnd there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.\n\nAnd so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.\n\nSo take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.\n\nYou can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.\n\nBut if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.\n\nIt\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.\n\nAnd so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.\n\nAudio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.\n\nBut if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.\n\nSo, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.\n\nAnd Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.\n\nSo NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.\n\nAnd as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.\n\nAnd, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.\n\nOr if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.\n\nThese things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.\n\nI\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.\n\nSo Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.\n\nSo tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.\n\n\n\n ![](https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png)\n\n\n\n*To learn more about real-time voice analysis, check out [Cyrano.ai](https://www.cyrano.ai/).*", "html": '<p><em>This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.</em>\xA0</p>\n<p><em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0<strong>94% accurate.</strong>\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0<a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.</p>\n<p>So I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.</p>\n<p>I\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.</p>\n<p>And so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.</p>\n<p>Looks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.</p>\n<p>Ok. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m</p>\n<blockquote>\n<p>I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.</p>\n</blockquote>\n<p>So what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.</p>\n<p>And there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.</p>\n<p>And so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.</p>\n<p>So take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.</p>\n<p>You can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.</p>\n<p>But if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.</p>\n<p>It\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.</p>\n<p>And so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.</p>\n<p>Audio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.</p>\n<p>But if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.</p>\n<p>So, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.</p>\n<p>And Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.</p>\n<p>So NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.</p>\n<p>And as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.</p>\n<p>And, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.</p>\n<p>Or if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.</p>\n<p>These things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.</p>\n<p>I\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.</p>\n<p>So Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.</p>\n<p>So tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png" alt=""></p>\n<p><em>To learn more about real-time voice analysis, check out <a href="https://www.cyrano.ai/">Cyrano.ai</a>.</em></p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md" };
function rawContent() {
  return "*This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.*\xA0\n\n*The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0**94% accurate.**\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0[contact us](https://deepgram.com/contact-us/).*\n\n\\[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.\n\nSo I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.\n\nI\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.\n\nAnd so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.\n\nLooks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.\n\nOk. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m\n\n> I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.\n\nSo what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.\n\nAnd there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.\n\nAnd so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.\n\nSo take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.\n\nYou can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.\n\nBut if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.\n\nIt\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.\n\nAnd so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.\n\nAudio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.\n\nBut if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.\n\nSo, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.\n\nAnd Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.\n\nSo NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.\n\nAnd as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.\n\nAnd, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.\n\nOr if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.\n\nThese things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.\n\nI\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.\n\nSo Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.\n\nSo tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.\n\n\n\n ![](https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png)\n\n\n\n*To learn more about real-time voice analysis, check out [Cyrano.ai](https://www.cyrano.ai/).*";
}
function compiledContent() {
  return '<p><em>This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.</em>\xA0</p>\n<p><em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was\xA0<strong>94% accurate.</strong>\xA0 Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.\xA0 If you want to see if Deepgram is right for your use case,\xA0<a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know it\u2019s kind of interesting. Yeah. This\u2026 for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, I\u2019m happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And I\u2019m also going to talk about the future of voice, not the distant, distant future. I\u2019ll talk about the next couple years. But I\u2019ll give you a little bit of background on myself, though, before we do that.</p>\n<p>So I\u2019m Scott Stephenson. I\u2019m a CEO and cofounder of Deepgram. I\u2019m also\u2026 I\u2019m a technical CEO. So if people out there wanna talk, you know, shop with me, that\u2019s totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if you\u2019re interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you\u2026 in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.</p>\n<p>I\u2019m gonna I\u2019m gonna talk about something in particular, and then we\u2019ll maybe go a little bit broader after that. But I\u2019ll I\u2019ll talk about something that\u2019s happening right now, which is we\u2019re starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. You\u2019d try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didn\u2019t have an alternative otherwise.</p>\n<p>And so I\u2019m here to talk about that a little bit about what\u2019s actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because what\u2019s actually happening now is people are saying, hey. We already have piles of data. There\u2019s phone calls happening. There\u2019s voice bot interactions. There\u2019s emails. There\u2019s other things that are coming in here.</p>\n<p>Looks like we lost our connection. I\u2019m not sure why. K. Sorry about that. We\u2019ll see if I can\u2026 I don\u2019t think I hit anything, but let\u2019s see.</p>\n<p>Ok. So so it can be text interactions. It can be voice interactions. There\u2019s there\u2019s a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then you\u2019re gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so I\u2019m I\u2019m</p>\n<blockquote>\n<p>I\u2019m saying, hey. That was the old way. The new way is if if you want the act\u2026 if you want to figure out what\u2019s going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.</p>\n</blockquote>\n<p>So what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out what\u2019s going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If it\u2019s a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.</p>\n<p>And there\u2019s\u2026 essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if you\u2019re in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but it\u2019s a really, really small percentage. And, you know, it had marginal success, had some success but marginal.</p>\n<p>And so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what they\u2019re actually saying for real. And so now the question would be, though. Alright. That\u2019s that\u2019s great. You told us there was something previously that was really hard to do, and now it\u2019s really easy to do or it seems like it\u2019s easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and I\u2019m just going to describe that a little bit to you.</p>\n<p>So take five seconds and look at this image, and I won\u2019t say anything. Ok. Does anybody remember what happened in that image? There\u2019s there\u2019s a dog, has a blue Frisbee. There\u2019s a girl probably fifty feet in the background with a\u2026 with, like, a bodyboard. There\u2019s another bodyboard. And I don\u2019t know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It\u2026 trust me. It\u2019s still expensive. But compared to audio and and many other sources, it\u2019s it\u2019s, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, I\u2019m not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, you\u2019ll get five words, you know, ten, fifteen words, like, whatever. Right? You\u2019re\u2026 there isn\u2019t much you can do. Right? And so, hopefully, that illustrates a little bit about audio that\u2019s happening. It\u2019s real time. It\u2019s very hard for a person to parallel process.</p>\n<p>You can you can recognize that when you\u2019re in a cocktail-party problem, which we\u2019ll all have later today when we\u2019re all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason\u2026 or\u2026 you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we\u2026 and ears and everything that is tuned just for listening to other humans talk and computers don\u2019t. They they see the image like what is on the right here. And it\u2019s\u2026 that\u2019s that\u2019s not the whole, like, sixty hour lecture. That\u2019s, like, a couple seconds of it. You know? So it\u2019s a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea what\u2019s going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what they\u2019re saying.</p>\n<p>But if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And that\u2019s the real trick. Right? And if you have a machine that can do it, then it\u2019s probably very valuable because now you can understand humans at scale. And that\u2019s the type of thing that we build. I\u2019ll just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that you\u2019re using if you\u2019re a bank or whatever it is. There\u2019s also how it said, so maybe you\u2019re speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when you\u2019re having these conversations, they\u2019re in a conversational style.</p>\n<p>It\u2019s a fast type of speaking, like I\u2019m doing right now. Sorry about that. But but, nevertheless, it\u2019s very conversational. You\u2019re not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so that\u2019s a challenge you have to deal with. And then audio\u2026 actually, a lot of people might think of audio as not that big of, like, a file, but it\u2019s about the third the size of video. So there\u2019s a lot of information packed in audio in order to make it sound good enough for you to understand. And there\u2019s different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one\u2026 like like an agent and then, like, the customer that\u2019s calling in, so multiple channels.</p>\n<p>And so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I\u2026 I\u2019ll talk technically just for a second here. But as you need a lot of data that is labeled, but it\u2019s not just a lot of it. Like, the hour count matters, but it doesn\u2019t matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all\u2026 you need that all labeled as, like, a a a data plus truth pair, essentially. So here\u2019s the audio and here\u2019s the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.</p>\n<p>Audio comes in labeled, and that\u2019s converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of\u2026 I think of it like a like a periodic table of chemical elements, but, instead, it\u2019s just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and\u2026 which is if many, if people in here have used APIs before or, you know, you\u2019ve used your, like, keyboard on your phone, then, generally, what you\u2019re using is is a general model, which is a a model that\u2019s a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it\u2026 it\u2019s it\u2019s designed to hopefully do pretty ok with the general population.</p>\n<p>But if you have a specific task, like you\u2019re our customer, like Citibank, and you\u2019re like, hey. I have bankers, and they\u2019re talking about bank stuff. And we wanna make sure that they\u2019re compliant in the things that they\u2019re doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a\u2026 your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.</p>\n<p>So, anyway, if you wanna talk about the technical side of it later, I\u2019m happy to happy to discuss, but that\u2019s how it all works under the hood. But now you can get wet\u2026 you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys\u2026 Cyrano, one of our customers, you just saw the demo. We\u2019re we\u2019re pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But\u2026 about the use case there, so I I won\u2019t take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.</p>\n<p>And Valyant does fast-food ordering. So\u2026 well, it doesn\u2019t have to be fast food. It could be many other things, but, typically, it\u2019s fast food. So think of driving through and you\u2019re in your car or you\u2019re on your mobile phone or you\u2019re at a kiosk, and you\u2019re trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And you\u2019d like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.</p>\n<p>So NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what they\u2019re trying to do is under\u2026 essentially, there\u2019s always a link between the international space station and ground. And there\u2019s people listening all the time, like actual humans. Three of them sitting down and transcribing everything that\u2019s happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that they\u2019re working on, and it\u2019s it\u2019s a\u2026 it\u2019s a big challenge, and their their CTO says, hey. The problem\u2019s right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and it\u2019s a scratchy, you know, radio signal, and it\u2019s it\u2019s it\u2019s not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.</p>\n<p>And as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesn\u2019t work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But\u2026 so, yeah, moving on to act three here a little bit, which is just a comment on what\u2019s happening in the world. We\u2019re in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. We\u2019re in the intelligence revolution right now. Automation is going to happen. Thirty years from now we\u2019re gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and you\u2019re like, holy shit. This is crazy. You know, you can talk to anybody anywhere.</p>\n<p>And, anyway, same\u2026 we\u2019re we\u2019re in that we\u2019re in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasn\u2019t, and now it actually is. That\u2019s partially because the models are getting good enough to do things in real time, but it\u2019s also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other\u2026 like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time\u2026 high-accuracy real time actually real. And then these techniques, if you\u2019re if you\u2019re in a general domain, you can get, like, ninety percent plus accuracy with just a general model.</p>\n<p>Or if you if you need that kind of accuracy, but it\u2019s kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come\u2026 people come to me and say, like, hey. Is it possible too? And the the\u2026 in audio, the answer is almost like it\u2026 it\u2019s almost always yes, unless it\u2019s something that a human really can\u2019t do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May\u2026 I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when they\u2019re speaking, what words they\u2019re talking about, what topic they\u2019re talking about, that type of thing.</p>\n<p>These things are all possible, but it\u2019s still kind of, like, the railroad was built across the United States, but there\u2019s a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to\u2026 you know, the the railroad builders and the towns that sprout up along those railroad lines. But it\u2019s going to be in compliance spaces, voice bots, call centers, the meetings, podcast. It\u2019s gonna be all over the place. I mean, for voice, it\u2019s just like a\u2026 it\u2019s the natural human commit.</p>\n<p>I\u2019m up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now that\u2019s all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today we\u2019re all hinting at that. You know, our lives are gonna change a lot. So I\u2019d\u2026 I I don\u2019t know if there\u2019s\u2026 if I if\u2026 maybe people are tired of hands. I don\u2019t know. But are there start-up founders in the room here? Like, any\u2026 ok. Cool. So if you\u2019re if if you\u2019re into voice and you\u2019re a start-up founder, you might wanna know about this.</p>\n<p>So Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I don\u2019t I don\u2019t know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. You\u2019re you\u2019re a customer of Deepgram, and we\u2019re happy to help you. And another one.</p>\n<p>So tomorrow evening, we have\u2026 how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, we\u2019re going to\u2026 we\u2026 we\u2019re gonna have a private screening of Dune. So if if you haven\u2019t seen Dune yet, now\u2019s an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special\u2026 a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I won\u2019t say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png" alt=""></p>\n<p><em>To learn more about real-time voice analysis, check out <a href="https://www.cyrano.ai/">Cyrano.ai</a>.</em></p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p><em>This is the transcript for the opening keynote presented by Scott Stephenson, CEO of Deepgram, presented on day one of Project Voice X.</em></p>
<p><em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was<strong>94% accurate.</strong> Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgrams API. If you want to see if Deepgram is right for your use case,<a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>
<p>[Scott Stephenson:] Hey, everybody. Thank you for for listening and coming to the event. I know its kind of interesting. Yeah. This for us, at least, at Deepgram, this is a first in-person event back. Maybe for most people, that that might be the case. But, yeah, Im happy to be here, happy to meet all the new people. And I just wanna thank all the other speakers before me as well, a lot of great discussion today. And Im also going to talk about the future of voice, not the distant, distant future. Ill talk about the next couple years. But Ill give you a little bit of background on myself, though, before we do that.</p>
<p>So Im Scott Stephenson. Im a CEO and cofounder of Deepgram. Im also Im a technical CEO. So if people out there wanna talk, you know, shop with me, thats totally fine. I I was a particle physicist before, so I I built deep underground dark matter detectors and and that kind of thing. So if youre interested in that, happy to talk about that as well. But but, yeah, Deepgram is a company that builds APIs. You just you just saw some of the performance of it, but we build speech APIs in order to enable the next generation of voice products to be built. And that can be in real time. It can be in batch mode, but we want to enable developers to build that piece. So think of Deepgram, like, you in the same category that you would Stripe or Twilio or something like that in order to enable developers to build voice products.</p>
<p>Im gonna Im gonna talk about something in particular, and then well maybe go a little bit broader after that. But Ill Ill talk about something thats happening right now, which is were starting to see the adoption of automation and AI in the call center space and in other areas as well. But, in particular, you see it, like, really ramping up in meetings and call center. But one thing I see is this this this this trend of the old way, which was if you wanted to figure out if you had happy customers or how well things are going or that type of thing, what you would do is ask them. You you would send out a survey. Youd try to corner them. You know, you try to do these very unscalable things or things that maybe had questionable statistics behind them on on how how well they worked. But you basically didnt have an alternative otherwise.</p>
<p>And so Im here to talk about that a little bit about whats actually happening now is, yes, some of that old stuff is still happening, but you already hear me saying old stuff. Because whats actually happening now is people are saying, hey. We already have piles of data. Theres phone calls happening. Theres voice bot interactions. Theres emails. Theres other things that are coming in here.</p>
<p>Looks like we lost our connection. Im not sure why. K. Sorry about that. Well see if I can I dont think I hit anything, but lets see.</p>
<p>Ok. So so it can be text interactions. It can be voice interactions. Theres theres a a number of ways that customers will interact with your product. Could just be clicks on your website, that type of thing. But if if you have to ask for the survey, if you have to, you know, send a text afterward, that type of thing, then youre gonna get a very biased response. People that hate you or love you. What about the in between? What about what they were thinking in the moment? You know, that type of thing. And so Im Im</p>
<blockquote>
<p>Im saying, hey. That was the old way. The new way is if if you want the act if you want to figure out whats going on, doing product development with your customers, see how all that goes, stop asking them and just start listening to them.</p>
</blockquote>
<p>So what do I mean by that? I mean, just take the data as it sits and be able to analyze it at scale to figure out whats going on. And so this could be a retail space, where you have microphones and people are talking about certain clothes that they want to try on. Hey. I hate this. That looks great, etcetera, that type of thing. Just listen to them in situ. If its a call center, somebody calling in saying, I hate this. I like this, etcetera, like, right in right in situ in in in the data, looking inside.</p>
<p>And theres essentially, for data scientists, like myself and lots of other people, they look at this huge data lake, dark dataset that previously just was, like, untouched. And maybe if youre in the call center space, you know that there would be, like, a QA sampling kind of thing that would happen. Maybe one percent of calls would be listened to by a team of, like, ten people, where each person would listen to, like, ten calls a day or something like that. And they would try to randomly sample and and see what they could get out of it, but its a really, really small percentage. And, you know, it had marginal success, had some success but marginal.</p>
<p>And so the next step in obvious evolution there is if you can automate this process and make it actually good and and and, again, see previous demo, then you can learn a lot about it, about what theyre actually saying for real. And so now the question would be, though. Alright. Thats thats great. You told us there was something previously that was really hard to do, and now its really easy to do or it seems like its easy to do. How does that all work? It seems hard. Well, it is hard. Audio is a really hard problem, and Im just going to describe that a little bit to you.</p>
<p>So take five seconds and look at this image, and I wont say anything. Ok. Does anybody remember what happened in that image? Theres theres a dog, has a blue Frisbee. Theres a girl probably fifty feet in the background with a with, like, a bodyboard. Theres another bodyboard. And I dont know why I keep going out here. But from the from the image perspective, you can glean a lot of information from a very small amount of time. And this makes labeling images really easy and inexpensive in a lot of ways. It trust me. Its still expensive. But compared to audio and and many other sources, its its, like, a lot easier. But you can get this whole story just from that quick five seconds. But now what if what if you were asked to look at this hour-long lecture for five seconds and then tell me, like, what happened. Right? You know, you get five seconds. Like, Im not gonna play, you know, biochemistry lecture for you. But but but, nevertheless, five seconds with that, youll get five words, you know, ten, fifteen words, like, whatever. Right? Youre there isnt much you can do. Right? And so, hopefully, that illustrates a little bit about audio thats happening. Its real time. Its very hard for a person to parallel process.</p>
<p>You can you can recognize that when youre in a cocktail-party problem, which well all have later today when were all sitting around trying to talk to each other, many conversations going on, etcetera. You can only really track one, maybe one and a half. You know? That kinda thing. But but part part of the reason or you know, part of the reason for that is humans are actually pretty good in real time in conversation. They can understand what what other humans are doing, but we have specialized hardware for it. You know? We have our brain that we and ears and everything that is tuned just for listening to other humans talk and computers dont. They they see the image like what is on the right here. And its thats thats not the whole, like, sixty hour lecture. Thats, like, a couple seconds of it. You know? So its a whole mess. Right? And if you were to if you were to try to represent that entire lecture that way, you, as a human, would have no idea whats going on. You might be able to say, a person was talking in this area or not. That that might be a question you can answer, but but, otherwise, you have no idea what theyre saying.</p>
<p>But if you run it through our ears and through our brain in real time, then we can understand it pretty well. But now how do you get a machine to actually do that? And thats the real trick. Right? And if you have a machine that can do it, then its probably very valuable because now you can understand humans at scale. And thats the type of thing that we build. Ill just point out a few of these problems again, like, in a in a call center space. Maybe you have specific product names or company names or industry terms and jargon or certain acronyms that youre using if youre a bank or whatever it is. Theres also how it said, so maybe youre speaking in a specific language. English, for instance. You could have a certain accent, dialect, etcetera. You could be upset or not. There could be background noise. You could be using a phone or some other device to actually talk to people, like calling in on a meeting or something. And, generally, when youre having these conversations, theyre in a conversational style.</p>
<p>Its a fast type of speaking, like Im doing right now. Sorry about that. But but, nevertheless, its very conversational. Youre not trying to purposefully slow down and enunciate so that the machine can understand you. You know? And so thats a challenge you have to deal with. And then audio actually, a lot of people might think of audio as not that big of, like, a file, but its about the third the size of video. So theres a lot of information packed in audio in order to make it sound good enough for you to understand. And theres different encoding and different compression techniques, and, you know, there could be several channels, like one person talking on one like like an agent and then, like, the customer thats calling in, so multiple channels.</p>
<p>And so, anyway, audio is very varied and very difficult. But the the way that you have to handle this, and I I Ill talk technically just for a second here. But as you need a lot of data that is labeled, but its not just a lot of it. Like, the hour count matters, but it doesnt matter as much as you might think. But it definitely matters. You need a lot in different areas, so young, old, male, female, low noise, high noise, in the car, not, upset, not upset, you know, very happy, etcetera. And you need this in multiple languages, multiple accents, etcetera, and you all you need that all labeled as, like, a a a data plus truth pair, essentially. So heres the audio and heres the transcript that goes with that audio, and then the machine can learn by example. And you can kind of see that if you can read it here.</p>
<p>Audio comes in labeled, and thats converted into training data. And then that makes a trained model, which you use, like, convolutional neural networks, dense dense layers or recurrent neural networks or attention layers or that type of thing is just kind of I think of it like a like a periodic table of chemical elements, but, instead, its just for deep learning. You have these different things that care about spatial or temporal or that type of thing. But, nevertheless, mix those up, put them in the right order, expose them to a bunch of training data. And then you have a general model and which is if many, if people in here have used APIs before or, you know, youve used your, like, keyboard on your phone, then, generally, what youre using is is a general model, which is a a model thats a one size fits all or jack-of-all-trades. Not necessarily a master of one, but it its its designed to hopefully do pretty ok with the general population.</p>
<p>But if you have a specific task, like youre our customer, like Citibank, and youre like, hey. I have bankers, and theyre talking about bank stuff. And we wanna make sure that theyre compliant in the things that theyre doing, then you can go through this bottom loop here that says taking new data and label that to do transfer learning on it. So you have a your original model that is already really good at English or other languages, and then now you expose it to a specific domain and then serve that in order to accomplish to your task.</p>
<p>So, anyway, if you wanna talk about the technical side of it later, Im happy to happy to discuss, but thats how it all works under the hood. But now you can get wet you can get rid of the old way, or keep doing it if if you find value in it, but add the new way, which is doing a hundred percent analysis across all your data. Do it in a reliable way. Do it in a scalable way and with high accuracy. And so I think you you guys Cyrano, one of our customers, you just saw the demo. Were were pumped to have them, and we can discuss a little bit later. Again, we have a booth back there. But about the use case there, so I I wont take too much time talking about them, but another company that maybe some have heard heard of in the audience is Valyant.</p>
<p>And Valyant does fast-food ordering. So well, it doesnt have to be fast food. It could be many other things, but, typically, its fast food. So think of driving through and youre in your car or youre on your mobile phone or youre at a kiosk, and youre trying to order food inside a mall or something like that. And you have a very specific menu items, like Baconator or something like that. And youd like your model to be very good at those things in this very noisy environment. And how do you do that? Well, you just saw magic. You have a general model, and then you transfer learn that model into their domain, and then it works really well. And another place that this works really well is in space.</p>
<p>So NASA is one of our customers. They they do space-to-ground communication. So we worked with the CTO and their team at the Johnson Space Center in Houston in Houston. And what theyre trying to do is under essentially, theres always a link between the international space station and ground. And theres people listening all the time, like actual humans. Three of them sitting down and transcribing everything thats happening. And, hey, can we, like, reduce that number to, like, two people or one person or zero people or something like that. And this is the problem that theyre working on, and its its a its a big challenge, and their their CTO says, hey. The problems right in our faces. We have we have all of seventy five hundred pieces of jargon and acronyms, and its a scratchy, you know, radio signal, and its its its not a pleasant thing to try to transcribe. But, hey, if there were a machine that could listen to this and do a good job of it, then that would be very valuable to them. And so, anyway, they they tried a lot of vendors.</p>
<p>And as you can maybe imagine using a general model approach, which is what pretty much everybody else does, then it doesnt work that well. But if you use this transfer-learning approach, then it works really well handling background noise, jargon, multi speaker, etcetera. And, yeah, happy to talk about these use cases later if you want to. But so, yeah, moving on to act three here a little bit, which is just a comment on whats happening in the world. Were in the middle of the intelligence revolution, like it or not. There was the agricultural revolution, industrial revolution, information revolution. Were in the intelligence revolution right now. Automation is going to happen. Thirty years from now were gonna look back and be like, oh, yeah. Those were the beginning days. You know? Just like if anybody in here was around when the Internet was first starting to form. You remember, you know, when you got your first email account, and youre like, holy shit. This is crazy. You know, you can talk to anybody anywhere.</p>
<p>And, anyway, same were were in that were in that point in time right now. And another comment is that real time is finally real. A couple of years ago, it wasnt, and now it actually is. Thats partially because the models are getting good enough to do things in real time, but its also partially because the computational techniques have become sophisticated enough, but not as heavy lifting plus other like, NVIDIA has gotten good essentially at processing larger model sizes and that type of thing. So, essentially, confluence of hardware plus software plus new techniques being implemented in software can make real time high-accuracy real time actually real. And then these techniques, if youre if youre in a general domain, you can get, like, ninety percent plus accuracy with just a general model.</p>
<p>Or if you if you need that kind of accuracy, but its kind of a niche domain, then you can go after a custom-trained model. And this stuff all works now, basically. So a lot a lot of times come people come to me and say, like, hey. Is it possible too? And the the in audio, the answer is almost like it its almost always yes, unless its something that a human really cant do. Like, can you build a lie detector? Like, well, I mean, kind of. Yes. But as as well as maybe you could as a human. You know? May I mean, not quite that well, but close. Right? But think about all the other things that a human can do. Like, they can jump into a conversation, tell you how many people are speaking when theyre speaking, what words theyre talking about, what topic theyre talking about, that type of thing.</p>
<p>These things are all possible, but its still kind of, like, the railroad was built across the United States, but theres a whole bunch of other work that has to be done and so all that stuff is happening now. So, yeah, the the people who win the west, you know, over the next couple decades will be the ones that empower the developers to build this architecture to you know, the the railroad builders and the towns that sprout up along those railroad lines. But its going to be in compliance spaces, voice bots, call centers, the meetings, podcast. Its gonna be all over the place. I mean, for voice, its just like a its the natural human commit.</p>
<p>Im up here talking right now. Right? Like, this is the natural communication mechanism, and it just previously was enabled by connectivity. So you could call somebody. You could do that you could do that type of thing, but there was never any automation or intelligence behind it, and now thats all changing. And, you know, our lives are gonna change massively because of it. And I think the productivity of the world is gonna go up significantly, which previous, you know, talks that we heard today were all hinting at that. You know, our lives are gonna change a lot. So Id I I dont know if theres if I if maybe people are tired of hands. I dont know. But are there start-up founders in the room here? Like, any ok. Cool. So if youre if if youre into voice and youre a start-up founder, you might wanna know about this.</p>
<p>So Deepgram has a start-up program that we gave away ten million dollars in speech-recognition credit. And what you can do is apply. Get up to a hundred thousand dollars for your start-up. And the reason we do this is to help push forward the the new the new products being built. I I dont I dont know if people are quite aware of what happened with AWS, GCP, Azure, etcetera over the last ten, fifteen years. But they gave away a lot of credit, and a lot of tech companies benefited as they grew their company on that initial credit. And, of course, you know, for Deepgram, the goal here is if you grow a whole bunch and become awesome then, you know, great. Youre youre a customer of Deepgram, and were happy to help you. And another one.</p>
<p>So tomorrow evening, we have how far away is it? A mile, but we have buses. We have party buses. Ok. So tomorrow evening, were going to we were gonna have a private screening of Dune. So if if you havent seen Dune yet, nows an opportunity to do it at a place called Suds n n Cinema Suds n Cinema, which is a pretty dope theater, and we could all watch Dune together if you wanna come hang out and do that. We also have a booth. And we have a special a a new a new member of our Deepgram team. We call him our VP of Intrigue. If you want to come meet him, come talk to us at the booth. I wont say anymore. But thank you, everybody, for listening to the talk, and I I appreciate everybody coming out to hear it.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661976850/blog/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/scott-stephenson-script-analysis-chart%402x-1-300x16.png" alt=""></p>
<p><em>To learn more about real-time voice analysis, check out <a href="https://www.cyrano.ai/">Cyrano.ai</a>.</em></p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/building-the-future-of-voice-scott-stephenson-ceo-deepgram-project-voice-x/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
