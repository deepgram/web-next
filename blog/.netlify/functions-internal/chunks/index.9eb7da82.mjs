import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead, d as renderComponent } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
import 'axios';
/* empty css                           *//* empty css                           *//* empty css                           */import '@storyblok/js';
/* empty css                          *//* empty css                              */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           */import 'camelcase';
/* empty css                              */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [{ "depth": 2, "slug": "what-is-text-cleaning", "text": "What is Text Cleaning?" }, { "depth": 2, "slug": "why-text-cleaning-is-so-important", "text": "Why Text Cleaning is So Important" }, { "depth": 2, "slug": "how-text-cleaning-works", "text": "How Text Cleaning Works" }, { "depth": 2, "slug": "challenges-for-turkish-text-cleaning", "text": "Challenges for Turkish Text Cleaning" }, { "depth": 3, "slug": "the-turkish-apostrophe", "text": "The Turkish Apostrophe" }, { "depth": 3, "slug": "another-apostrophe-challenge", "text": "Another Apostrophe Challenge" }, { "depth": 3, "slug": "processing-currencies-in-turkish", "text": "Processing Currencies in Turkish" }, { "depth": 3, "slug": "text-cleaning-for-numbers", "text": "Text Cleaning for Numbers" }, { "depth": 2, "slug": "wrapping-up", "text": "Wrapping Up" }], "source": `Since Turkey is celebrating [Victory Day](https://en.wikipedia.org/wiki/Victory_Day_(Turkey)) today, and since we've been meaning to write a blog post about text cleaning, we figured we might as well write a post about text cleaning and Turkish! For every automatic speech recognition (ASR) system, cleaning the training data is a crucial part. Speech recognition systems learn how to map speech to its written counterpart by learning speech-text pairs. If the data that teaches it how to do this is messy, the system just won't work well. In this post, we'll explain what text cleaning is, how it works, and why it's so important for ASR. Let's look!

## What is Text Cleaning?

Simply put, text cleaning (also called text normalization) is a component of natural language processing that takes raw data and neatens it up, usually in order to use it for something like machine learning model training. While preparing our training data, we'd like the transcriptions to match the spoken phoneme sequences as much as possible. For example, if we hear the words "J. Lo" in the speech file, we'd like its transcription to be "J. Lo" as well. However, if the speech file includes the words "Jennifer Lopez", we wouldn't want the transcription to be "J. Lo"; we'd want it to be "Jennifer Lopez". Although "Jennifer Lopez" and "J. Lo" represent the same entity in the real world, phonetically they don't match at all. The former word is represented by the sequence of phonemes

> JH EH N AH F ER . L OW P EH Z 

where the latter word sequence maps to a much shorter phoneme sequence

> JH EY . L OW

All of this logic also applies to numbers, abbreviations, email addresses-i.e., nonstandard tokens, where we can end up with written text that doesn't necessarily match the pronunciation, need to match up to what's actually being said, and not the real-world entity. 

## Why Text Cleaning is So Important

There's an old saying in data science: garbage in, garbage out. This means that if you train a model with bad data, your model isn't going to be very good.  We want to make sure we have a good match between the phonetics-the actual speech sounds produced-and the phonetic transcription of that speech.

This is the first step in getting an accurate, readable transcript at the end of the transcription process. This is why text cleaning is so important for ASR training. By doing text cleaning before training, you give your model the best possibility of producing what you want. Let's take a look at an example of how text cleaning actually works.

## How Text Cleaning Works

Text cleaning is language-dependent. This is because abbreviations, shortenings, and other special text forms have a language-unique relationship to their spoken tongue. For example, "lb" and its relationship to "pound" in English isn't at all obvious if you didn't already know about it, and isn't relevant to any other language. To apply text cleaning to training data, we need a multi-step processing pipeline. Each step transforms the text into a "cleaner" variation of the original.

By "cleaner" here we mean "closer to the actual phonetics of what was said". Take a look at Figure 1, below. There you can see an example sentence which is transformed, step by step, into a "cleanest" final version. It starts in the usual written form in Step 1, but that doesn't accurately represent the phonetics of what's being said, so we need to clean it up if we want to use it for model training. In Step 2, we spell out the numbers into words, rather than numerals. And in Step 3, we spell out the components of an email* (which, when said, is a string of words, just as it is in English-"duygu at deepgram dot com"). In the final step, Step 4, we remove the double period typo so that the sentence has [standard punctuation](https://blog.deepgram.com/complete-guide-punctuation-capitalization-speech-to-text/). 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981074/blog/text-cleaning-asr-turkish/blog-image-1-1.png)

**Figure 1.** Text cleaning for the Turkish sentence "I sent an email to the email address \`hello@company.com\` yesterday."

## Challenges for Turkish Text Cleaning

In the above example, we mostly saw things that would also make sense in other languages-like reformatting an email address into words. But there are unique text cleaning challenges for each and every spoken language, and Turkish is no exception. Let's consider a few, Turkish-specific examples.

### The Turkish Apostrophe

One challenge that's specific to Turkish, and which we saw an example of in Figure 1 above, is with the use of the apostrophe. The apostrophe is used often in written Turkish, and is used for separating inflectional suffixes from the proper nouns, numbers, and abbreviations that they're attached to. We need to keep an eye on this when we do text cleaning because some of the changes that we make to the text will also influence whether or not we need to get rid of the apostrophe. For example, when we turn a numeral into a common noun (that is, we turn *3* into *\xFC\xE7)* we no longer need the apostrophe after we spell it out as text. In the above example, we saw the token *17.30'da* became *be\u015F bu\xE7ukta* and not *be\u015F bu\xE7uk'ta;* we omitted the apostrophe in order to keep the grammaticality. However, proper nouns will still stay as proper nouns, no matter what text cleaning we do, so they should never lose an apostrophe. But what if our proper noun has a number in it? In Figure 2 below, the apostrophe is used to separate a proper noun from an inflectional suffix, and hence a good question arises: should we keep the apostrophe or not? 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981075/blog/text-cleaning-asr-turkish/blog-image-2.png)

**Figure 2.** Example of a possible text cleaning for the Turkish sentence "150 accidents happened in the first 1000 km of the road E-5." 

In Figure 2, we deleted the second apostrophe safely. However, keeping the first apostrophe creates a not-very-good looking token: *be\u015F'in* (which should be *be\u015Fin* according to Turkish orthography). Here, *5* is part of an entity-the name of a road-and also a number itself. On the other hand, *be\u015F* 'five' is just an ordinary noun and *be\u015F'in* is an ordinary noun with an apostrophe (which is not quite grammatical). So what's the solution here? It totally depends on the model vocabulary of the ASR model that one wants to use. Here, it's indeed better to keep *E-5* intact and not normalize the *5* to *be\u015F* because it's part of an entity. Hence, the final cleaning result should look like this:

> ***E-5'in*** *ilk bin kilometresinde sadece ge\xE7en y\u0131l y\xFCz elli kaza oldu.* 150 accidents happened in the first 1000 km of the road E-5.

Long story short, we need to pay attention to apostrophes to do text cleaning in Turkish correctly. Sometimes we delete them, sometimes we keep them; the important part is to get grammatically correct sentences even after the cleaning and to do so consistently.

### Another Apostrophe Challenge

Because the apostrophe comes hand in hand with inflectional and derivational suffixes, we also have to pay attention to phonological processes such as consonant assimilation and vowel harmony rules. Let's look at what those terms mean, and an example of each. Consonant assimilation refers to two consonants becoming more similar to each other when they occur next to each other. We can see an easy example of this in English with the words *cat* and *dog*. When we add the plural *\\-s* to these words, we pronounce them differently based on the final sound: like an /s/ in *cats* but like a /z/ in *dogs*. (This is specifically an example of [voicing assimilation](https://en.wikipedia.org/wiki/Consonant_voicing_and_devoicing#Voicing_assimilation) if you're curious.) 

This consonant assimilation in English is pronounced but not written. Modern Turkish writing, however, does reflect consonant assimilation when it occurs. And when it comes to consonant assimilation in Turkish, we're actually sort of lucky because even if consonant assimilation occurs with a suffix, the apostrophe is still used.

> *D\xFCn saat* ***3'te*** *beni g\xF6rmeye geldi.* He came to visit me yesterday at 3PM.

In this example, the first consonant of the suffix /-dA/\\*\\* has assimilated and become /te/ after *\xFC\xE7* 'three', so we don't need to do any extra processing after deleting the apostrophe in this case. So far so good, but now let's look at a trickier case with a derivational suffix and abbreviation combo, along with a typo. One of the main issues that comes up with text cleaning is that you often can't assume that your input text follows the normal rules of grammar and orthography, so you have to do some checking.

> *Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.* *Yrd do\xE7 luk s\u0131nav\u0131 kalkm\u0131\u015F.* The entrance exam for assistant professorship is canceled.

In both sentences, the words *yrd* and *do\xE7* are abbreviations. *Yrd* is short for *yard\u0131mc\u0131* 'assistant' and *do\xE7* is short *do\xE7ent* 'professor', so *yrd do\xE7* is a bit like *asst prof*. Here, there are no apostrophes between *do\xE7* and *luk* at all to indicate a suffix is appended to an abbreviation. In the first sentence, the spelling *do\xE7luk* is not great grammatically but still acceptable, while in the second sentence, the two words *do\xE7* and *luk* should not be written separately. In perfect written Turkish, they should be written together, but with an apostrophe, like this: *Yrd do\xE7'luk s\u0131nav\u0131 kalkm\u0131\u015F.* Figuring out what is going on with either of the two original sentences could be difficult, but fortunately we have a hint-*luk* is not a Turkish word; however, it is a suffix. And, what's more, it is following the rules of Turkish vowel harmony here, as it harmonizes with the vowel of the previous word *do\xE7*. [Vowel harmony](https://en.wikipedia.org/wiki/Vowel_harmony), *very* briefly, is a process that forces all of the vowels in a word to be part of the same set. Because /u/ and /o/ are part of the same set, we can conclude the second sentence is a misspelling and we can correct this mistake by deleting the space in between to get a "cleaner" version: *Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.*

### Processing Currencies in Turkish

Phew, that was a lot! The good news is, we can relax a bit when processing some other types of tokens, though; processing currency symbols, for example, is quite easy. Unlike English, in Turkish we don't append a plural suffix to currencies whose value is more than 1. Please compare: 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981076/blog/text-cleaning-asr-turkish/blog-image-3-1.png)

Hence, a simple piece of code is enough to clean up currencies. We first parse out the currency symbols with a tiny regex, look them up from a predefined dictionary of currency words, and finally replace them in the text. The code for this is below in Figure 3. 

\`\`\`python
import re

CURR_SYMS = {
"$": "dolar",
"\u20AC": "euro",
"\xA3": "sterlin",
"\xA5": "yen",
"tl": "lira",
"ytl": "lira",
"\u20BA": "lira"
}

CURR_REGEX = r"([$\u20AC\xA3\xA5\u20BA]|y?tl|try)"

def convert_currency_syms(token):
  token = token.lower()
  words = CURR_SYMS.get(token, token)
  return words

def clean_currency_symbols(sentence):
  match = re.search(CURR_REGEX, sentence)
  while match:
    mstring = match.group()
    mstart, mend = match.span()
    sentence = sentence[:mstart] + " " + convert_currency_syms(mstring) + sentence[mend:]
    match = re.search(CURR_REGEX, sentence)
  return sentence

sent = "Hepsine 100$ verdim."
>> clean_currency_symbols(sent)
"Hepsine 100 dolar verdim."
\`\`\`

**Figure 3.** A sample code for running a regex to get correct Turkish currencies. 

We literally did **nothing** to currency words; no plural suffix or any other suffixes were added by us. If we were processing English, though, we would need to parse the currency symbol together with the number to check if the number is 1 or not; if it's not 1, we need to add a plural suffix to the currency word. Please note that the code segment in Figure 3 shows only one step of processing, that related to processing the currency symbols. As we mentioned previously, all conversions are done in individual steps, hence the number *100* would be processed in a separate number-cleaning step.

### Text Cleaning for Numbers

Numbers can introduce other challenges as well. For example, converting ordinal cardinal and decimal number tokens into words; as well as converting times, dates, telephone numbers, postal codes, and other sorts of number tokens into words.  Parsing number-type tokens can be tricky; numbers come in different shapes. There are decimal numbers (which include commas), big numbers (which include periods), phone numbers (including parenthesis, dashes, plus signs, and even more), postal codes (address strings should be extracted first to spot them), and so on. 

Cleaning numbers is challenging for each language indeed. Quite a number of regexes and other sorts of parsers (including NER and CF parsers) are used. For an example, this regex should parse out big numbers (such as 1,250,000) in English text: \`\\b\\d{1,3}(,\\d{3})+\\b\` Big numbers are written similarly in Turkish, but with a period instead of comma; hence, a tiny modification to this regex would suffice: \`\\b\\d{1,3}([.]\\d{3})+\\b\` It's totally expected, when we're building text cleaning pipelines, that we implement both language-specific and some "universal" cleaning methods. For each language we process, we find common token types and processing methods, but at the same time, we implement quite a lot of language specific text cleaning methods as well.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## Wrapping Up

When it comes down to it, all languages are beautiful and yet challenging at the same time; dealing with natural language is never easy. At Deepgram, we all enjoy working with speech and text, and we embrace both the difficulty and the beauty of natural language. We hope you enjoyed this article and share our joy for Turkish, too! Please join us for more content on our blog and follow us on [LinkedIn](https://www.linkedin.com/company/deepgram/).

And, if you'd like to give Deepgram a try for yourself, [sign up for Console](https://console.deepgram.com/signup) for free and get $150 in free credits to give it a try. 

- - -

\\* Of course, sometimes we might *want* an email to appear in a final transcript. In that case, we can do a bit of postprocessing to the model output to get the final transcription in order to reverse normalize *...-at-...-dot-com* word sequences into a single email token. But for the purposes of this post, because we're looking at the phonetic level, we'll want to represent the pronunciation rather than what we want the final output to look like. 

\\*\\* The /d/ becomes /t/ due to assimilation with the preceding sound, and the A represents a vowel that changes depending on vowel harmony, which in this word, becomes /e/. Thus, we end up with /te/ for the suffix.`, "html": '<p>Since Turkey is celebrating <a href="https://en.wikipedia.org/wiki/Victory_Day_(Turkey)">Victory Day</a> today, and since we\u2019ve been meaning to write a blog post about text cleaning, we figured we might as well write a post about text cleaning and Turkish! For every automatic speech recognition (ASR) system, cleaning the training data is a crucial part. Speech recognition systems learn how to map speech to its written counterpart by learning speech-text pairs. If the data that teaches it how to do this is messy, the system just won\u2019t work well. In this post, we\u2019ll explain what text cleaning is, how it works, and why it\u2019s so important for ASR. Let\u2019s look!</p>\n<h2 id="what-is-text-cleaning">What is Text Cleaning?</h2>\n<p>Simply put, text cleaning (also called text normalization) is a component of natural language processing that takes raw data and neatens it up, usually in order to use it for something like machine learning model training. While preparing our training data, we\u2019d like the transcriptions to match the spoken phoneme sequences as much as possible. For example, if we hear the words \u201CJ. Lo\u201D in the speech file, we\u2019d like its transcription to be \u201CJ. Lo\u201D as well. However, if the speech file includes the words \u201CJennifer Lopez\u201D, we wouldn\u2019t want the transcription to be \u201CJ. Lo\u201D; we\u2019d want it to be \u201CJennifer Lopez\u201D. Although \u201CJennifer Lopez\u201D and \u201CJ. Lo\u201D represent the same entity in the real world, phonetically they don\u2019t match at all. The former word is represented by the sequence of phonemes</p>\n<blockquote>\n<p>JH EH N AH F ER . L OW P EH Z</p>\n</blockquote>\n<p>where the latter word sequence maps to a much shorter phoneme sequence</p>\n<blockquote>\n<p>JH EY . L OW</p>\n</blockquote>\n<p>All of this logic also applies to numbers, abbreviations, email addresses-i.e., nonstandard tokens, where we can end up with written text that doesn\u2019t necessarily match the pronunciation, need to match up to what\u2019s actually being said, and not the real-world entity.</p>\n<h2 id="why-text-cleaning-is-so-important">Why Text Cleaning is So Important</h2>\n<p>There\u2019s an old saying in data science: garbage in, garbage out. This means that if you train a model with bad data, your model isn\u2019t going to be very good.  We want to make sure we have a good match between the phonetics-the actual speech sounds produced-and the phonetic transcription of that speech.</p>\n<p>This is the first step in getting an accurate, readable transcript at the end of the transcription process. This is why text cleaning is so important for ASR training. By doing text cleaning before training, you give your model the best possibility of producing what you want. Let\u2019s take a look at an example of how text cleaning actually works.</p>\n<h2 id="how-text-cleaning-works">How Text Cleaning Works</h2>\n<p>Text cleaning is language-dependent. This is because abbreviations, shortenings, and other special text forms have a language-unique relationship to their spoken tongue. For example, \u201Clb\u201D and its relationship to \u201Cpound\u201D in English isn\u2019t at all obvious if you didn\u2019t already know about it, and isn\u2019t relevant to any other language. To apply text cleaning to training data, we need a multi-step processing pipeline. Each step transforms the text into a \u201Ccleaner\u201D variation of the original.</p>\n<p>By \u201Ccleaner\u201D here we mean \u201Ccloser to the actual phonetics of what was said\u201D. Take a look at Figure 1, below. There you can see an example sentence which is transformed, step by step, into a \u201Ccleanest\u201D final version. It starts in the usual written form in Step 1, but that doesn\u2019t accurately represent the phonetics of what\u2019s being said, so we need to clean it up if we want to use it for model training. In Step 2, we spell out the numbers into words, rather than numerals. And in Step 3, we spell out the components of an email* (which, when said, is a string of words, just as it is in English-\u201Dduygu at deepgram dot com\u201D). In the final step, Step 4, we remove the double period typo so that the sentence has <a href="https://blog.deepgram.com/complete-guide-punctuation-capitalization-speech-to-text/">standard punctuation</a>.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981074/blog/text-cleaning-asr-turkish/blog-image-1-1.png" alt=""></p>\n<p><strong>Figure 1.</strong> Text cleaning for the Turkish sentence \u201CI sent an email to the email address <code is:raw>hello@company.com</code> yesterday.\u201D</p>\n<h2 id="challenges-for-turkish-text-cleaning">Challenges for Turkish Text Cleaning</h2>\n<p>In the above example, we mostly saw things that would also make sense in other languages-like reformatting an email address into words. But there are unique text cleaning challenges for each and every spoken language, and Turkish is no exception. Let\u2019s consider a few, Turkish-specific examples.</p>\n<h3 id="the-turkish-apostrophe">The Turkish Apostrophe</h3>\n<p>One challenge that\u2019s specific to Turkish, and which we saw an example of in Figure 1 above, is with the use of the apostrophe. The apostrophe is used often in written Turkish, and is used for separating inflectional suffixes from the proper nouns, numbers, and abbreviations that they\u2019re attached to. We need to keep an eye on this when we do text cleaning because some of the changes that we make to the text will also influence whether or not we need to get rid of the apostrophe. For example, when we turn a numeral into a common noun (that is, we turn <em>3</em> into <em>\xFC\xE7)</em> we no longer need the apostrophe after we spell it out as text. In the above example, we saw the token <em>17.30\u2019da</em> became <em>be\u015F bu\xE7ukta</em> and not <em>be\u015F bu\xE7uk\u2019ta;</em> we omitted the apostrophe in order to keep the grammaticality. However, proper nouns will still stay as proper nouns, no matter what text cleaning we do, so they should never lose an apostrophe. But what if our proper noun has a number in it? In Figure 2 below, the apostrophe is used to separate a proper noun from an inflectional suffix, and hence a good question arises: should we keep the apostrophe or not?</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981075/blog/text-cleaning-asr-turkish/blog-image-2.png" alt=""></p>\n<p><strong>Figure 2.</strong> Example of a possible text cleaning for the Turkish sentence \u201C150 accidents happened in the first 1000 km of the road E-5.\u201D</p>\n<p>In Figure 2, we deleted the second apostrophe safely. However, keeping the first apostrophe creates a not-very-good looking token: <em>be\u015F\u2019in</em> (which should be <em>be\u015Fin</em> according to Turkish orthography). Here, <em>5</em> is part of an entity-the name of a road-and also a number itself. On the other hand, <em>be\u015F</em> \u2018five\u2019 is just an ordinary noun and <em>be\u015F\u2019in</em> is an ordinary noun with an apostrophe (which is not quite grammatical). So what\u2019s the solution here? It totally depends on the model vocabulary of the ASR model that one wants to use. Here, it\u2019s indeed better to keep <em>E-5</em> intact and not normalize the <em>5</em> to <em>be\u015F</em> because it\u2019s part of an entity. Hence, the final cleaning result should look like this:</p>\n<blockquote>\n<p><em><strong>E-5\u2019in</strong></em> <em>ilk bin kilometresinde sadece ge\xE7en y\u0131l y\xFCz elli kaza oldu.</em> 150 accidents happened in the first 1000 km of the road E-5.</p>\n</blockquote>\n<p>Long story short, we need to pay attention to apostrophes to do text cleaning in Turkish correctly. Sometimes we delete them, sometimes we keep them; the important part is to get grammatically correct sentences even after the cleaning and to do so consistently.</p>\n<h3 id="another-apostrophe-challenge">Another Apostrophe Challenge</h3>\n<p>Because the apostrophe comes hand in hand with inflectional and derivational suffixes, we also have to pay attention to phonological processes such as consonant assimilation and vowel harmony rules. Let\u2019s look at what those terms mean, and an example of each. Consonant assimilation refers to two consonants becoming more similar to each other when they occur next to each other. We can see an easy example of this in English with the words <em>cat</em> and <em>dog</em>. When we add the plural <em>-s</em> to these words, we pronounce them differently based on the final sound: like an /s/ in <em>cats</em> but like a /z/ in <em>dogs</em>. (This is specifically an example of <a href="https://en.wikipedia.org/wiki/Consonant_voicing_and_devoicing#Voicing_assimilation">voicing assimilation</a> if you\u2019re curious.)</p>\n<p>This consonant assimilation in English is pronounced but not written. Modern Turkish writing, however, does reflect consonant assimilation when it occurs. And when it comes to consonant assimilation in Turkish, we\u2019re actually sort of lucky because even if consonant assimilation occurs with a suffix, the apostrophe is still used.</p>\n<blockquote>\n<p><em>D\xFCn saat</em> <em><strong>3\u2019te</strong></em> <em>beni g\xF6rmeye geldi.</em> He came to visit me yesterday at 3PM.</p>\n</blockquote>\n<p>In this example, the first consonant of the suffix /-dA/** has assimilated and become /te/ after <em>\xFC\xE7</em> \u2018three\u2019, so we don\u2019t need to do any extra processing after deleting the apostrophe in this case. So far so good, but now let\u2019s look at a trickier case with a derivational suffix and abbreviation combo, along with a typo. One of the main issues that comes up with text cleaning is that you often can\u2019t assume that your input text follows the normal rules of grammar and orthography, so you have to do some checking.</p>\n<blockquote>\n<p><em>Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> <em>Yrd do\xE7 luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> The entrance exam for assistant professorship is canceled.</p>\n</blockquote>\n<p>In both sentences, the words <em>yrd</em> and <em>do\xE7</em> are abbreviations. <em>Yrd</em> is short for <em>yard\u0131mc\u0131</em> \u2018assistant\u2019 and <em>do\xE7</em> is short <em>do\xE7ent</em> \u2018professor\u2019, so <em>yrd do\xE7</em> is a bit like <em>asst prof</em>. Here, there are no apostrophes between <em>do\xE7</em> and <em>luk</em> at all to indicate a suffix is appended to an abbreviation. In the first sentence, the spelling <em>do\xE7luk</em> is not great grammatically but still acceptable, while in the second sentence, the two words <em>do\xE7</em> and <em>luk</em> should not be written separately. In perfect written Turkish, they should be written together, but with an apostrophe, like this: <em>Yrd do\xE7\u2019luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> Figuring out what is going on with either of the two original sentences could be difficult, but fortunately we have a hint-<em>luk</em> is not a Turkish word; however, it is a suffix. And, what\u2019s more, it is following the rules of Turkish vowel harmony here, as it harmonizes with the vowel of the previous word <em>do\xE7</em>. <a href="https://en.wikipedia.org/wiki/Vowel_harmony">Vowel harmony</a>, <em>very</em> briefly, is a process that forces all of the vowels in a word to be part of the same set. Because /u/ and /o/ are part of the same set, we can conclude the second sentence is a misspelling and we can correct this mistake by deleting the space in between to get a \u201Ccleaner\u201D version: <em>Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.</em></p>\n<h3 id="processing-currencies-in-turkish">Processing Currencies in Turkish</h3>\n<p>Phew, that was a lot! The good news is, we can relax a bit when processing some other types of tokens, though; processing currency symbols, for example, is quite easy. Unlike English, in Turkish we don\u2019t append a plural suffix to currencies whose value is more than 1. Please compare:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981076/blog/text-cleaning-asr-turkish/blog-image-3-1.png" alt=""></p>\n<p>Hence, a simple piece of code is enough to clean up currencies. We first parse out the currency symbols with a tiny regex, look them up from a predefined dictionary of currency words, and finally replace them in the text. The code for this is below in Figure 3.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> re</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #79C0FF">CURR_SYMS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;$&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;dolar&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\u20AC&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;euro&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\xA3&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;sterlin&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\xA5&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;yen&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;tl&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;ytl&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\u20BA&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">r</span><span style="color: #A5D6FF">&quot;</span><span style="color: #79C0FF">([$\u20AC\xA3\xA5\u20BA]</span><span style="color: #FF7B72">|</span><span style="color: #A5D6FF">y</span><span style="color: #FF7B72">?</span><span style="color: #A5D6FF">tl</span><span style="color: #FF7B72">|</span><span style="color: #A5D6FF">try</span><span style="color: #79C0FF">)</span><span style="color: #A5D6FF">&quot;</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">convert_currency_syms</span><span style="color: #C9D1D9">(token):</span></span>\n<span class="line"><span style="color: #C9D1D9">  token </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> token.lower()</span></span>\n<span class="line"><span style="color: #C9D1D9">  words </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">CURR_SYMS</span><span style="color: #C9D1D9">.get(token, token)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> words</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">clean_currency_symbols</span><span style="color: #C9D1D9">(sentence):</span></span>\n<span class="line"><span style="color: #C9D1D9">  match </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> re.search(</span><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9">, sentence)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> match:</span></span>\n<span class="line"><span style="color: #C9D1D9">    mstring </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> match.group()</span></span>\n<span class="line"><span style="color: #C9D1D9">    mstart, mend </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> match.span()</span></span>\n<span class="line"><span style="color: #C9D1D9">    sentence </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> sentence[:mstart] </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> convert_currency_syms(mstring) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> sentence[mend:]</span></span>\n<span class="line"><span style="color: #C9D1D9">    match </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> re.search(</span><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9">, sentence)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> sentence</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">sent </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;Hepsine 100$ verdim.&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">&gt;&gt;</span><span style="color: #C9D1D9"> clean_currency_symbols(sent)</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;Hepsine 100 dolar verdim.&quot;</span></span></code></pre>\n<p><strong>Figure 3.</strong> A sample code for running a regex to get correct Turkish currencies.</p>\n<p>We literally did <strong>nothing</strong> to currency words; no plural suffix or any other suffixes were added by us. If we were processing English, though, we would need to parse the currency symbol together with the number to check if the number is 1 or not; if it\u2019s not 1, we need to add a plural suffix to the currency word. Please note that the code segment in Figure 3 shows only one step of processing, that related to processing the currency symbols. As we mentioned previously, all conversions are done in individual steps, hence the number <em>100</em> would be processed in a separate number-cleaning step.</p>\n<h3 id="text-cleaning-for-numbers">Text Cleaning for Numbers</h3>\n<p>Numbers can introduce other challenges as well. For example, converting ordinal cardinal and decimal number tokens into words; as well as converting times, dates, telephone numbers, postal codes, and other sorts of number tokens into words.  Parsing number-type tokens can be tricky; numbers come in different shapes. There are decimal numbers (which include commas), big numbers (which include periods), phone numbers (including parenthesis, dashes, plus signs, and even more), postal codes (address strings should be extracted first to spot them), and so on.</p>\n<p>Cleaning numbers is challenging for each language indeed. Quite a number of regexes and other sorts of parsers (including NER and CF parsers) are used. For an example, this regex should parse out big numbers (such as 1,250,000) in English text: <code is:raw>\\b\\d{1,3}(,\\d{3})+\\b</code> Big numbers are written similarly in Turkish, but with a period instead of comma; hence, a tiny modification to this regex would suffice: <code is:raw>\\b\\d{1,3}([.]\\d{3})+\\b</code> It\u2019s totally expected, when we\u2019re building text cleaning pipelines, that we implement both language-specific and some \u201Cuniversal\u201D cleaning methods. For each language we process, we find common token types and processing methods, but at the same time, we implement quite a lot of language specific text cleaning methods as well.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="wrapping-up">Wrapping Up</h2>\n<p>When it comes down to it, all languages are beautiful and yet challenging at the same time; dealing with natural language is never easy. At Deepgram, we all enjoy working with speech and text, and we embrace both the difficulty and the beauty of natural language. We hope you enjoyed this article and share our joy for Turkish, too! Please join us for more content on our blog and follow us on <a href="https://www.linkedin.com/company/deepgram/">LinkedIn</a>.</p>\n<p>And, if you\u2019d like to give Deepgram a try for yourself, <a href="https://console.deepgram.com/signup">sign up for Console</a> for free and get $150 in free credits to give it a try.</p>\n<hr>\n<p>* Of course, sometimes we might <em>want</em> an email to appear in a final transcript. In that case, we can do a bit of postprocessing to the model output to get the final transcription in order to reverse normalize <em>\u2026-at-\u2026-dot-com</em> word sequences into a single email token. But for the purposes of this post, because we\u2019re looking at the phonetic level, we\u2019ll want to represent the pronunciation rather than what we want the final output to look like.</p>\n<p>** The /d/ becomes /t/ due to assimilation with the preceding sound, and the A represents a vowel that changes depending on vowel harmony, which in this word, becomes /e/. Thus, we end up with /te/ for the suffix.</p>' };
const frontmatter = { "title": "Text Cleaning for ASR: The Case of Turkish", "description": "Text cleaning can be challenging in any language\u2014and Turkish is no exception! Let\u2019s look at what text cleaning is and how it works.", "date": "2022-08-30T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981434/blog/text-cleaning-asr-turkish/the-case-of-turkish-thumb-554x220-1.png", "authors": ["duygu-altinok", "morris-gevirtz", "chris-doty"], "category": "linguistics", "tags": ["deep-learning", "language", "nlu"], "seo": { "title": "Text Cleaning for ASR: The Case of Turkish", "description": "Text cleaning can be challenging in any language\u2014and Turkish is no exception! Let\u2019s look at what text cleaning is and how it works." }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981434/blog/text-cleaning-asr-turkish/the-case-of-turkish-thumb-554x220-1.png" }, "shorturls": { "share": "https://dpgr.am/be99ec0", "twitter": "https://dpgr.am/bd774ae", "linkedin": "https://dpgr.am/0cd3f95", "reddit": "https://dpgr.am/61d0221", "facebook": "https://dpgr.am/ad549d8" }, "astro": { "headings": [{ "depth": 2, "slug": "what-is-text-cleaning", "text": "What is Text Cleaning?" }, { "depth": 2, "slug": "why-text-cleaning-is-so-important", "text": "Why Text Cleaning is So Important" }, { "depth": 2, "slug": "how-text-cleaning-works", "text": "How Text Cleaning Works" }, { "depth": 2, "slug": "challenges-for-turkish-text-cleaning", "text": "Challenges for Turkish Text Cleaning" }, { "depth": 3, "slug": "the-turkish-apostrophe", "text": "The Turkish Apostrophe" }, { "depth": 3, "slug": "another-apostrophe-challenge", "text": "Another Apostrophe Challenge" }, { "depth": 3, "slug": "processing-currencies-in-turkish", "text": "Processing Currencies in Turkish" }, { "depth": 3, "slug": "text-cleaning-for-numbers", "text": "Text Cleaning for Numbers" }, { "depth": 2, "slug": "wrapping-up", "text": "Wrapping Up" }], "source": `Since Turkey is celebrating [Victory Day](https://en.wikipedia.org/wiki/Victory_Day_(Turkey)) today, and since we've been meaning to write a blog post about text cleaning, we figured we might as well write a post about text cleaning and Turkish! For every automatic speech recognition (ASR) system, cleaning the training data is a crucial part. Speech recognition systems learn how to map speech to its written counterpart by learning speech-text pairs. If the data that teaches it how to do this is messy, the system just won't work well. In this post, we'll explain what text cleaning is, how it works, and why it's so important for ASR. Let's look!

## What is Text Cleaning?

Simply put, text cleaning (also called text normalization) is a component of natural language processing that takes raw data and neatens it up, usually in order to use it for something like machine learning model training. While preparing our training data, we'd like the transcriptions to match the spoken phoneme sequences as much as possible. For example, if we hear the words "J. Lo" in the speech file, we'd like its transcription to be "J. Lo" as well. However, if the speech file includes the words "Jennifer Lopez", we wouldn't want the transcription to be "J. Lo"; we'd want it to be "Jennifer Lopez". Although "Jennifer Lopez" and "J. Lo" represent the same entity in the real world, phonetically they don't match at all. The former word is represented by the sequence of phonemes

> JH EH N AH F ER . L OW P EH Z 

where the latter word sequence maps to a much shorter phoneme sequence

> JH EY . L OW

All of this logic also applies to numbers, abbreviations, email addresses-i.e., nonstandard tokens, where we can end up with written text that doesn't necessarily match the pronunciation, need to match up to what's actually being said, and not the real-world entity. 

## Why Text Cleaning is So Important

There's an old saying in data science: garbage in, garbage out. This means that if you train a model with bad data, your model isn't going to be very good.  We want to make sure we have a good match between the phonetics-the actual speech sounds produced-and the phonetic transcription of that speech.

This is the first step in getting an accurate, readable transcript at the end of the transcription process. This is why text cleaning is so important for ASR training. By doing text cleaning before training, you give your model the best possibility of producing what you want. Let's take a look at an example of how text cleaning actually works.

## How Text Cleaning Works

Text cleaning is language-dependent. This is because abbreviations, shortenings, and other special text forms have a language-unique relationship to their spoken tongue. For example, "lb" and its relationship to "pound" in English isn't at all obvious if you didn't already know about it, and isn't relevant to any other language. To apply text cleaning to training data, we need a multi-step processing pipeline. Each step transforms the text into a "cleaner" variation of the original.

By "cleaner" here we mean "closer to the actual phonetics of what was said". Take a look at Figure 1, below. There you can see an example sentence which is transformed, step by step, into a "cleanest" final version. It starts in the usual written form in Step 1, but that doesn't accurately represent the phonetics of what's being said, so we need to clean it up if we want to use it for model training. In Step 2, we spell out the numbers into words, rather than numerals. And in Step 3, we spell out the components of an email* (which, when said, is a string of words, just as it is in English-"duygu at deepgram dot com"). In the final step, Step 4, we remove the double period typo so that the sentence has [standard punctuation](https://blog.deepgram.com/complete-guide-punctuation-capitalization-speech-to-text/). 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981074/blog/text-cleaning-asr-turkish/blog-image-1-1.png)

**Figure 1.** Text cleaning for the Turkish sentence "I sent an email to the email address \`hello@company.com\` yesterday."

## Challenges for Turkish Text Cleaning

In the above example, we mostly saw things that would also make sense in other languages-like reformatting an email address into words. But there are unique text cleaning challenges for each and every spoken language, and Turkish is no exception. Let's consider a few, Turkish-specific examples.

### The Turkish Apostrophe

One challenge that's specific to Turkish, and which we saw an example of in Figure 1 above, is with the use of the apostrophe. The apostrophe is used often in written Turkish, and is used for separating inflectional suffixes from the proper nouns, numbers, and abbreviations that they're attached to. We need to keep an eye on this when we do text cleaning because some of the changes that we make to the text will also influence whether or not we need to get rid of the apostrophe. For example, when we turn a numeral into a common noun (that is, we turn *3* into *\xFC\xE7)* we no longer need the apostrophe after we spell it out as text. In the above example, we saw the token *17.30'da* became *be\u015F bu\xE7ukta* and not *be\u015F bu\xE7uk'ta;* we omitted the apostrophe in order to keep the grammaticality. However, proper nouns will still stay as proper nouns, no matter what text cleaning we do, so they should never lose an apostrophe. But what if our proper noun has a number in it? In Figure 2 below, the apostrophe is used to separate a proper noun from an inflectional suffix, and hence a good question arises: should we keep the apostrophe or not? 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981075/blog/text-cleaning-asr-turkish/blog-image-2.png)

**Figure 2.** Example of a possible text cleaning for the Turkish sentence "150 accidents happened in the first 1000 km of the road E-5." 

In Figure 2, we deleted the second apostrophe safely. However, keeping the first apostrophe creates a not-very-good looking token: *be\u015F'in* (which should be *be\u015Fin* according to Turkish orthography). Here, *5* is part of an entity-the name of a road-and also a number itself. On the other hand, *be\u015F* 'five' is just an ordinary noun and *be\u015F'in* is an ordinary noun with an apostrophe (which is not quite grammatical). So what's the solution here? It totally depends on the model vocabulary of the ASR model that one wants to use. Here, it's indeed better to keep *E-5* intact and not normalize the *5* to *be\u015F* because it's part of an entity. Hence, the final cleaning result should look like this:

> ***E-5'in*** *ilk bin kilometresinde sadece ge\xE7en y\u0131l y\xFCz elli kaza oldu.* 150 accidents happened in the first 1000 km of the road E-5.

Long story short, we need to pay attention to apostrophes to do text cleaning in Turkish correctly. Sometimes we delete them, sometimes we keep them; the important part is to get grammatically correct sentences even after the cleaning and to do so consistently.

### Another Apostrophe Challenge

Because the apostrophe comes hand in hand with inflectional and derivational suffixes, we also have to pay attention to phonological processes such as consonant assimilation and vowel harmony rules. Let's look at what those terms mean, and an example of each. Consonant assimilation refers to two consonants becoming more similar to each other when they occur next to each other. We can see an easy example of this in English with the words *cat* and *dog*. When we add the plural *\\-s* to these words, we pronounce them differently based on the final sound: like an /s/ in *cats* but like a /z/ in *dogs*. (This is specifically an example of [voicing assimilation](https://en.wikipedia.org/wiki/Consonant_voicing_and_devoicing#Voicing_assimilation) if you're curious.) 

This consonant assimilation in English is pronounced but not written. Modern Turkish writing, however, does reflect consonant assimilation when it occurs. And when it comes to consonant assimilation in Turkish, we're actually sort of lucky because even if consonant assimilation occurs with a suffix, the apostrophe is still used.

> *D\xFCn saat* ***3'te*** *beni g\xF6rmeye geldi.* He came to visit me yesterday at 3PM.

In this example, the first consonant of the suffix /-dA/\\*\\* has assimilated and become /te/ after *\xFC\xE7* 'three', so we don't need to do any extra processing after deleting the apostrophe in this case. So far so good, but now let's look at a trickier case with a derivational suffix and abbreviation combo, along with a typo. One of the main issues that comes up with text cleaning is that you often can't assume that your input text follows the normal rules of grammar and orthography, so you have to do some checking.

> *Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.* *Yrd do\xE7 luk s\u0131nav\u0131 kalkm\u0131\u015F.* The entrance exam for assistant professorship is canceled.

In both sentences, the words *yrd* and *do\xE7* are abbreviations. *Yrd* is short for *yard\u0131mc\u0131* 'assistant' and *do\xE7* is short *do\xE7ent* 'professor', so *yrd do\xE7* is a bit like *asst prof*. Here, there are no apostrophes between *do\xE7* and *luk* at all to indicate a suffix is appended to an abbreviation. In the first sentence, the spelling *do\xE7luk* is not great grammatically but still acceptable, while in the second sentence, the two words *do\xE7* and *luk* should not be written separately. In perfect written Turkish, they should be written together, but with an apostrophe, like this: *Yrd do\xE7'luk s\u0131nav\u0131 kalkm\u0131\u015F.* Figuring out what is going on with either of the two original sentences could be difficult, but fortunately we have a hint-*luk* is not a Turkish word; however, it is a suffix. And, what's more, it is following the rules of Turkish vowel harmony here, as it harmonizes with the vowel of the previous word *do\xE7*. [Vowel harmony](https://en.wikipedia.org/wiki/Vowel_harmony), *very* briefly, is a process that forces all of the vowels in a word to be part of the same set. Because /u/ and /o/ are part of the same set, we can conclude the second sentence is a misspelling and we can correct this mistake by deleting the space in between to get a "cleaner" version: *Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.*

### Processing Currencies in Turkish

Phew, that was a lot! The good news is, we can relax a bit when processing some other types of tokens, though; processing currency symbols, for example, is quite easy. Unlike English, in Turkish we don't append a plural suffix to currencies whose value is more than 1. Please compare: 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981076/blog/text-cleaning-asr-turkish/blog-image-3-1.png)

Hence, a simple piece of code is enough to clean up currencies. We first parse out the currency symbols with a tiny regex, look them up from a predefined dictionary of currency words, and finally replace them in the text. The code for this is below in Figure 3. 

\`\`\`python
import re

CURR_SYMS = {
"$": "dolar",
"\u20AC": "euro",
"\xA3": "sterlin",
"\xA5": "yen",
"tl": "lira",
"ytl": "lira",
"\u20BA": "lira"
}

CURR_REGEX = r"([$\u20AC\xA3\xA5\u20BA]|y?tl|try)"

def convert_currency_syms(token):
  token = token.lower()
  words = CURR_SYMS.get(token, token)
  return words

def clean_currency_symbols(sentence):
  match = re.search(CURR_REGEX, sentence)
  while match:
    mstring = match.group()
    mstart, mend = match.span()
    sentence = sentence[:mstart] + " " + convert_currency_syms(mstring) + sentence[mend:]
    match = re.search(CURR_REGEX, sentence)
  return sentence

sent = "Hepsine 100$ verdim."
>> clean_currency_symbols(sent)
"Hepsine 100 dolar verdim."
\`\`\`

**Figure 3.** A sample code for running a regex to get correct Turkish currencies. 

We literally did **nothing** to currency words; no plural suffix or any other suffixes were added by us. If we were processing English, though, we would need to parse the currency symbol together with the number to check if the number is 1 or not; if it's not 1, we need to add a plural suffix to the currency word. Please note that the code segment in Figure 3 shows only one step of processing, that related to processing the currency symbols. As we mentioned previously, all conversions are done in individual steps, hence the number *100* would be processed in a separate number-cleaning step.

### Text Cleaning for Numbers

Numbers can introduce other challenges as well. For example, converting ordinal cardinal and decimal number tokens into words; as well as converting times, dates, telephone numbers, postal codes, and other sorts of number tokens into words.  Parsing number-type tokens can be tricky; numbers come in different shapes. There are decimal numbers (which include commas), big numbers (which include periods), phone numbers (including parenthesis, dashes, plus signs, and even more), postal codes (address strings should be extracted first to spot them), and so on. 

Cleaning numbers is challenging for each language indeed. Quite a number of regexes and other sorts of parsers (including NER and CF parsers) are used. For an example, this regex should parse out big numbers (such as 1,250,000) in English text: \`\\b\\d{1,3}(,\\d{3})+\\b\` Big numbers are written similarly in Turkish, but with a period instead of comma; hence, a tiny modification to this regex would suffice: \`\\b\\d{1,3}([.]\\d{3})+\\b\` It's totally expected, when we're building text cleaning pipelines, that we implement both language-specific and some "universal" cleaning methods. For each language we process, we find common token types and processing methods, but at the same time, we implement quite a lot of language specific text cleaning methods as well.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## Wrapping Up

When it comes down to it, all languages are beautiful and yet challenging at the same time; dealing with natural language is never easy. At Deepgram, we all enjoy working with speech and text, and we embrace both the difficulty and the beauty of natural language. We hope you enjoyed this article and share our joy for Turkish, too! Please join us for more content on our blog and follow us on [LinkedIn](https://www.linkedin.com/company/deepgram/).

And, if you'd like to give Deepgram a try for yourself, [sign up for Console](https://console.deepgram.com/signup) for free and get $150 in free credits to give it a try. 

- - -

\\* Of course, sometimes we might *want* an email to appear in a final transcript. In that case, we can do a bit of postprocessing to the model output to get the final transcription in order to reverse normalize *...-at-...-dot-com* word sequences into a single email token. But for the purposes of this post, because we're looking at the phonetic level, we'll want to represent the pronunciation rather than what we want the final output to look like. 

\\*\\* The /d/ becomes /t/ due to assimilation with the preceding sound, and the A represents a vowel that changes depending on vowel harmony, which in this word, becomes /e/. Thus, we end up with /te/ for the suffix.`, "html": '<p>Since Turkey is celebrating <a href="https://en.wikipedia.org/wiki/Victory_Day_(Turkey)">Victory Day</a> today, and since we\u2019ve been meaning to write a blog post about text cleaning, we figured we might as well write a post about text cleaning and Turkish! For every automatic speech recognition (ASR) system, cleaning the training data is a crucial part. Speech recognition systems learn how to map speech to its written counterpart by learning speech-text pairs. If the data that teaches it how to do this is messy, the system just won\u2019t work well. In this post, we\u2019ll explain what text cleaning is, how it works, and why it\u2019s so important for ASR. Let\u2019s look!</p>\n<h2 id="what-is-text-cleaning">What is Text Cleaning?</h2>\n<p>Simply put, text cleaning (also called text normalization) is a component of natural language processing that takes raw data and neatens it up, usually in order to use it for something like machine learning model training. While preparing our training data, we\u2019d like the transcriptions to match the spoken phoneme sequences as much as possible. For example, if we hear the words \u201CJ. Lo\u201D in the speech file, we\u2019d like its transcription to be \u201CJ. Lo\u201D as well. However, if the speech file includes the words \u201CJennifer Lopez\u201D, we wouldn\u2019t want the transcription to be \u201CJ. Lo\u201D; we\u2019d want it to be \u201CJennifer Lopez\u201D. Although \u201CJennifer Lopez\u201D and \u201CJ. Lo\u201D represent the same entity in the real world, phonetically they don\u2019t match at all. The former word is represented by the sequence of phonemes</p>\n<blockquote>\n<p>JH EH N AH F ER . L OW P EH Z</p>\n</blockquote>\n<p>where the latter word sequence maps to a much shorter phoneme sequence</p>\n<blockquote>\n<p>JH EY . L OW</p>\n</blockquote>\n<p>All of this logic also applies to numbers, abbreviations, email addresses-i.e., nonstandard tokens, where we can end up with written text that doesn\u2019t necessarily match the pronunciation, need to match up to what\u2019s actually being said, and not the real-world entity.</p>\n<h2 id="why-text-cleaning-is-so-important">Why Text Cleaning is So Important</h2>\n<p>There\u2019s an old saying in data science: garbage in, garbage out. This means that if you train a model with bad data, your model isn\u2019t going to be very good.  We want to make sure we have a good match between the phonetics-the actual speech sounds produced-and the phonetic transcription of that speech.</p>\n<p>This is the first step in getting an accurate, readable transcript at the end of the transcription process. This is why text cleaning is so important for ASR training. By doing text cleaning before training, you give your model the best possibility of producing what you want. Let\u2019s take a look at an example of how text cleaning actually works.</p>\n<h2 id="how-text-cleaning-works">How Text Cleaning Works</h2>\n<p>Text cleaning is language-dependent. This is because abbreviations, shortenings, and other special text forms have a language-unique relationship to their spoken tongue. For example, \u201Clb\u201D and its relationship to \u201Cpound\u201D in English isn\u2019t at all obvious if you didn\u2019t already know about it, and isn\u2019t relevant to any other language. To apply text cleaning to training data, we need a multi-step processing pipeline. Each step transforms the text into a \u201Ccleaner\u201D variation of the original.</p>\n<p>By \u201Ccleaner\u201D here we mean \u201Ccloser to the actual phonetics of what was said\u201D. Take a look at Figure 1, below. There you can see an example sentence which is transformed, step by step, into a \u201Ccleanest\u201D final version. It starts in the usual written form in Step 1, but that doesn\u2019t accurately represent the phonetics of what\u2019s being said, so we need to clean it up if we want to use it for model training. In Step 2, we spell out the numbers into words, rather than numerals. And in Step 3, we spell out the components of an email* (which, when said, is a string of words, just as it is in English-\u201Dduygu at deepgram dot com\u201D). In the final step, Step 4, we remove the double period typo so that the sentence has <a href="https://blog.deepgram.com/complete-guide-punctuation-capitalization-speech-to-text/">standard punctuation</a>.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981074/blog/text-cleaning-asr-turkish/blog-image-1-1.png" alt=""></p>\n<p><strong>Figure 1.</strong> Text cleaning for the Turkish sentence \u201CI sent an email to the email address <code is:raw>hello@company.com</code> yesterday.\u201D</p>\n<h2 id="challenges-for-turkish-text-cleaning">Challenges for Turkish Text Cleaning</h2>\n<p>In the above example, we mostly saw things that would also make sense in other languages-like reformatting an email address into words. But there are unique text cleaning challenges for each and every spoken language, and Turkish is no exception. Let\u2019s consider a few, Turkish-specific examples.</p>\n<h3 id="the-turkish-apostrophe">The Turkish Apostrophe</h3>\n<p>One challenge that\u2019s specific to Turkish, and which we saw an example of in Figure 1 above, is with the use of the apostrophe. The apostrophe is used often in written Turkish, and is used for separating inflectional suffixes from the proper nouns, numbers, and abbreviations that they\u2019re attached to. We need to keep an eye on this when we do text cleaning because some of the changes that we make to the text will also influence whether or not we need to get rid of the apostrophe. For example, when we turn a numeral into a common noun (that is, we turn <em>3</em> into <em>\xFC\xE7)</em> we no longer need the apostrophe after we spell it out as text. In the above example, we saw the token <em>17.30\u2019da</em> became <em>be\u015F bu\xE7ukta</em> and not <em>be\u015F bu\xE7uk\u2019ta;</em> we omitted the apostrophe in order to keep the grammaticality. However, proper nouns will still stay as proper nouns, no matter what text cleaning we do, so they should never lose an apostrophe. But what if our proper noun has a number in it? In Figure 2 below, the apostrophe is used to separate a proper noun from an inflectional suffix, and hence a good question arises: should we keep the apostrophe or not?</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981075/blog/text-cleaning-asr-turkish/blog-image-2.png" alt=""></p>\n<p><strong>Figure 2.</strong> Example of a possible text cleaning for the Turkish sentence \u201C150 accidents happened in the first 1000 km of the road E-5.\u201D</p>\n<p>In Figure 2, we deleted the second apostrophe safely. However, keeping the first apostrophe creates a not-very-good looking token: <em>be\u015F\u2019in</em> (which should be <em>be\u015Fin</em> according to Turkish orthography). Here, <em>5</em> is part of an entity-the name of a road-and also a number itself. On the other hand, <em>be\u015F</em> \u2018five\u2019 is just an ordinary noun and <em>be\u015F\u2019in</em> is an ordinary noun with an apostrophe (which is not quite grammatical). So what\u2019s the solution here? It totally depends on the model vocabulary of the ASR model that one wants to use. Here, it\u2019s indeed better to keep <em>E-5</em> intact and not normalize the <em>5</em> to <em>be\u015F</em> because it\u2019s part of an entity. Hence, the final cleaning result should look like this:</p>\n<blockquote>\n<p><em><strong>E-5\u2019in</strong></em> <em>ilk bin kilometresinde sadece ge\xE7en y\u0131l y\xFCz elli kaza oldu.</em> 150 accidents happened in the first 1000 km of the road E-5.</p>\n</blockquote>\n<p>Long story short, we need to pay attention to apostrophes to do text cleaning in Turkish correctly. Sometimes we delete them, sometimes we keep them; the important part is to get grammatically correct sentences even after the cleaning and to do so consistently.</p>\n<h3 id="another-apostrophe-challenge">Another Apostrophe Challenge</h3>\n<p>Because the apostrophe comes hand in hand with inflectional and derivational suffixes, we also have to pay attention to phonological processes such as consonant assimilation and vowel harmony rules. Let\u2019s look at what those terms mean, and an example of each. Consonant assimilation refers to two consonants becoming more similar to each other when they occur next to each other. We can see an easy example of this in English with the words <em>cat</em> and <em>dog</em>. When we add the plural <em>-s</em> to these words, we pronounce them differently based on the final sound: like an /s/ in <em>cats</em> but like a /z/ in <em>dogs</em>. (This is specifically an example of <a href="https://en.wikipedia.org/wiki/Consonant_voicing_and_devoicing#Voicing_assimilation">voicing assimilation</a> if you\u2019re curious.)</p>\n<p>This consonant assimilation in English is pronounced but not written. Modern Turkish writing, however, does reflect consonant assimilation when it occurs. And when it comes to consonant assimilation in Turkish, we\u2019re actually sort of lucky because even if consonant assimilation occurs with a suffix, the apostrophe is still used.</p>\n<blockquote>\n<p><em>D\xFCn saat</em> <em><strong>3\u2019te</strong></em> <em>beni g\xF6rmeye geldi.</em> He came to visit me yesterday at 3PM.</p>\n</blockquote>\n<p>In this example, the first consonant of the suffix /-dA/** has assimilated and become /te/ after <em>\xFC\xE7</em> \u2018three\u2019, so we don\u2019t need to do any extra processing after deleting the apostrophe in this case. So far so good, but now let\u2019s look at a trickier case with a derivational suffix and abbreviation combo, along with a typo. One of the main issues that comes up with text cleaning is that you often can\u2019t assume that your input text follows the normal rules of grammar and orthography, so you have to do some checking.</p>\n<blockquote>\n<p><em>Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> <em>Yrd do\xE7 luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> The entrance exam for assistant professorship is canceled.</p>\n</blockquote>\n<p>In both sentences, the words <em>yrd</em> and <em>do\xE7</em> are abbreviations. <em>Yrd</em> is short for <em>yard\u0131mc\u0131</em> \u2018assistant\u2019 and <em>do\xE7</em> is short <em>do\xE7ent</em> \u2018professor\u2019, so <em>yrd do\xE7</em> is a bit like <em>asst prof</em>. Here, there are no apostrophes between <em>do\xE7</em> and <em>luk</em> at all to indicate a suffix is appended to an abbreviation. In the first sentence, the spelling <em>do\xE7luk</em> is not great grammatically but still acceptable, while in the second sentence, the two words <em>do\xE7</em> and <em>luk</em> should not be written separately. In perfect written Turkish, they should be written together, but with an apostrophe, like this: <em>Yrd do\xE7\u2019luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> Figuring out what is going on with either of the two original sentences could be difficult, but fortunately we have a hint-<em>luk</em> is not a Turkish word; however, it is a suffix. And, what\u2019s more, it is following the rules of Turkish vowel harmony here, as it harmonizes with the vowel of the previous word <em>do\xE7</em>. <a href="https://en.wikipedia.org/wiki/Vowel_harmony">Vowel harmony</a>, <em>very</em> briefly, is a process that forces all of the vowels in a word to be part of the same set. Because /u/ and /o/ are part of the same set, we can conclude the second sentence is a misspelling and we can correct this mistake by deleting the space in between to get a \u201Ccleaner\u201D version: <em>Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.</em></p>\n<h3 id="processing-currencies-in-turkish">Processing Currencies in Turkish</h3>\n<p>Phew, that was a lot! The good news is, we can relax a bit when processing some other types of tokens, though; processing currency symbols, for example, is quite easy. Unlike English, in Turkish we don\u2019t append a plural suffix to currencies whose value is more than 1. Please compare:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981076/blog/text-cleaning-asr-turkish/blog-image-3-1.png" alt=""></p>\n<p>Hence, a simple piece of code is enough to clean up currencies. We first parse out the currency symbols with a tiny regex, look them up from a predefined dictionary of currency words, and finally replace them in the text. The code for this is below in Figure 3.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> re</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #79C0FF">CURR_SYMS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;$&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;dolar&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\u20AC&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;euro&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\xA3&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;sterlin&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\xA5&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;yen&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;tl&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;ytl&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\u20BA&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">r</span><span style="color: #A5D6FF">&quot;</span><span style="color: #79C0FF">([$\u20AC\xA3\xA5\u20BA]</span><span style="color: #FF7B72">|</span><span style="color: #A5D6FF">y</span><span style="color: #FF7B72">?</span><span style="color: #A5D6FF">tl</span><span style="color: #FF7B72">|</span><span style="color: #A5D6FF">try</span><span style="color: #79C0FF">)</span><span style="color: #A5D6FF">&quot;</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">convert_currency_syms</span><span style="color: #C9D1D9">(token):</span></span>\n<span class="line"><span style="color: #C9D1D9">  token </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> token.lower()</span></span>\n<span class="line"><span style="color: #C9D1D9">  words </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">CURR_SYMS</span><span style="color: #C9D1D9">.get(token, token)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> words</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">clean_currency_symbols</span><span style="color: #C9D1D9">(sentence):</span></span>\n<span class="line"><span style="color: #C9D1D9">  match </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> re.search(</span><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9">, sentence)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> match:</span></span>\n<span class="line"><span style="color: #C9D1D9">    mstring </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> match.group()</span></span>\n<span class="line"><span style="color: #C9D1D9">    mstart, mend </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> match.span()</span></span>\n<span class="line"><span style="color: #C9D1D9">    sentence </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> sentence[:mstart] </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> convert_currency_syms(mstring) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> sentence[mend:]</span></span>\n<span class="line"><span style="color: #C9D1D9">    match </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> re.search(</span><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9">, sentence)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> sentence</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">sent </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;Hepsine 100$ verdim.&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">&gt;&gt;</span><span style="color: #C9D1D9"> clean_currency_symbols(sent)</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;Hepsine 100 dolar verdim.&quot;</span></span></code></pre>\n<p><strong>Figure 3.</strong> A sample code for running a regex to get correct Turkish currencies.</p>\n<p>We literally did <strong>nothing</strong> to currency words; no plural suffix or any other suffixes were added by us. If we were processing English, though, we would need to parse the currency symbol together with the number to check if the number is 1 or not; if it\u2019s not 1, we need to add a plural suffix to the currency word. Please note that the code segment in Figure 3 shows only one step of processing, that related to processing the currency symbols. As we mentioned previously, all conversions are done in individual steps, hence the number <em>100</em> would be processed in a separate number-cleaning step.</p>\n<h3 id="text-cleaning-for-numbers">Text Cleaning for Numbers</h3>\n<p>Numbers can introduce other challenges as well. For example, converting ordinal cardinal and decimal number tokens into words; as well as converting times, dates, telephone numbers, postal codes, and other sorts of number tokens into words.  Parsing number-type tokens can be tricky; numbers come in different shapes. There are decimal numbers (which include commas), big numbers (which include periods), phone numbers (including parenthesis, dashes, plus signs, and even more), postal codes (address strings should be extracted first to spot them), and so on.</p>\n<p>Cleaning numbers is challenging for each language indeed. Quite a number of regexes and other sorts of parsers (including NER and CF parsers) are used. For an example, this regex should parse out big numbers (such as 1,250,000) in English text: <code is:raw>\\b\\d{1,3}(,\\d{3})+\\b</code> Big numbers are written similarly in Turkish, but with a period instead of comma; hence, a tiny modification to this regex would suffice: <code is:raw>\\b\\d{1,3}([.]\\d{3})+\\b</code> It\u2019s totally expected, when we\u2019re building text cleaning pipelines, that we implement both language-specific and some \u201Cuniversal\u201D cleaning methods. For each language we process, we find common token types and processing methods, but at the same time, we implement quite a lot of language specific text cleaning methods as well.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="wrapping-up">Wrapping Up</h2>\n<p>When it comes down to it, all languages are beautiful and yet challenging at the same time; dealing with natural language is never easy. At Deepgram, we all enjoy working with speech and text, and we embrace both the difficulty and the beauty of natural language. We hope you enjoyed this article and share our joy for Turkish, too! Please join us for more content on our blog and follow us on <a href="https://www.linkedin.com/company/deepgram/">LinkedIn</a>.</p>\n<p>And, if you\u2019d like to give Deepgram a try for yourself, <a href="https://console.deepgram.com/signup">sign up for Console</a> for free and get $150 in free credits to give it a try.</p>\n<hr>\n<p>* Of course, sometimes we might <em>want</em> an email to appear in a final transcript. In that case, we can do a bit of postprocessing to the model output to get the final transcription in order to reverse normalize <em>\u2026-at-\u2026-dot-com</em> word sequences into a single email token. But for the purposes of this post, because we\u2019re looking at the phonetic level, we\u2019ll want to represent the pronunciation rather than what we want the final output to look like.</p>\n<p>** The /d/ becomes /t/ due to assimilation with the preceding sound, and the A represents a vowel that changes depending on vowel harmony, which in this word, becomes /e/. Thus, we end up with /te/ for the suffix.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/text-cleaning-asr-turkish/index.md" };
function rawContent() {
  return `Since Turkey is celebrating [Victory Day](https://en.wikipedia.org/wiki/Victory_Day_(Turkey)) today, and since we've been meaning to write a blog post about text cleaning, we figured we might as well write a post about text cleaning and Turkish! For every automatic speech recognition (ASR) system, cleaning the training data is a crucial part. Speech recognition systems learn how to map speech to its written counterpart by learning speech-text pairs. If the data that teaches it how to do this is messy, the system just won't work well. In this post, we'll explain what text cleaning is, how it works, and why it's so important for ASR. Let's look!

## What is Text Cleaning?

Simply put, text cleaning (also called text normalization) is a component of natural language processing that takes raw data and neatens it up, usually in order to use it for something like machine learning model training. While preparing our training data, we'd like the transcriptions to match the spoken phoneme sequences as much as possible. For example, if we hear the words "J. Lo" in the speech file, we'd like its transcription to be "J. Lo" as well. However, if the speech file includes the words "Jennifer Lopez", we wouldn't want the transcription to be "J. Lo"; we'd want it to be "Jennifer Lopez". Although "Jennifer Lopez" and "J. Lo" represent the same entity in the real world, phonetically they don't match at all. The former word is represented by the sequence of phonemes

> JH EH N AH F ER . L OW P EH Z 

where the latter word sequence maps to a much shorter phoneme sequence

> JH EY . L OW

All of this logic also applies to numbers, abbreviations, email addresses-i.e., nonstandard tokens, where we can end up with written text that doesn't necessarily match the pronunciation, need to match up to what's actually being said, and not the real-world entity. 

## Why Text Cleaning is So Important

There's an old saying in data science: garbage in, garbage out. This means that if you train a model with bad data, your model isn't going to be very good.  We want to make sure we have a good match between the phonetics-the actual speech sounds produced-and the phonetic transcription of that speech.

This is the first step in getting an accurate, readable transcript at the end of the transcription process. This is why text cleaning is so important for ASR training. By doing text cleaning before training, you give your model the best possibility of producing what you want. Let's take a look at an example of how text cleaning actually works.

## How Text Cleaning Works

Text cleaning is language-dependent. This is because abbreviations, shortenings, and other special text forms have a language-unique relationship to their spoken tongue. For example, "lb" and its relationship to "pound" in English isn't at all obvious if you didn't already know about it, and isn't relevant to any other language. To apply text cleaning to training data, we need a multi-step processing pipeline. Each step transforms the text into a "cleaner" variation of the original.

By "cleaner" here we mean "closer to the actual phonetics of what was said". Take a look at Figure 1, below. There you can see an example sentence which is transformed, step by step, into a "cleanest" final version. It starts in the usual written form in Step 1, but that doesn't accurately represent the phonetics of what's being said, so we need to clean it up if we want to use it for model training. In Step 2, we spell out the numbers into words, rather than numerals. And in Step 3, we spell out the components of an email* (which, when said, is a string of words, just as it is in English-"duygu at deepgram dot com"). In the final step, Step 4, we remove the double period typo so that the sentence has [standard punctuation](https://blog.deepgram.com/complete-guide-punctuation-capitalization-speech-to-text/). 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981074/blog/text-cleaning-asr-turkish/blog-image-1-1.png)

**Figure 1.** Text cleaning for the Turkish sentence "I sent an email to the email address \`hello@company.com\` yesterday."

## Challenges for Turkish Text Cleaning

In the above example, we mostly saw things that would also make sense in other languages-like reformatting an email address into words. But there are unique text cleaning challenges for each and every spoken language, and Turkish is no exception. Let's consider a few, Turkish-specific examples.

### The Turkish Apostrophe

One challenge that's specific to Turkish, and which we saw an example of in Figure 1 above, is with the use of the apostrophe. The apostrophe is used often in written Turkish, and is used for separating inflectional suffixes from the proper nouns, numbers, and abbreviations that they're attached to. We need to keep an eye on this when we do text cleaning because some of the changes that we make to the text will also influence whether or not we need to get rid of the apostrophe. For example, when we turn a numeral into a common noun (that is, we turn *3* into *\xFC\xE7)* we no longer need the apostrophe after we spell it out as text. In the above example, we saw the token *17.30'da* became *be\u015F bu\xE7ukta* and not *be\u015F bu\xE7uk'ta;* we omitted the apostrophe in order to keep the grammaticality. However, proper nouns will still stay as proper nouns, no matter what text cleaning we do, so they should never lose an apostrophe. But what if our proper noun has a number in it? In Figure 2 below, the apostrophe is used to separate a proper noun from an inflectional suffix, and hence a good question arises: should we keep the apostrophe or not? 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981075/blog/text-cleaning-asr-turkish/blog-image-2.png)

**Figure 2.** Example of a possible text cleaning for the Turkish sentence "150 accidents happened in the first 1000 km of the road E-5." 

In Figure 2, we deleted the second apostrophe safely. However, keeping the first apostrophe creates a not-very-good looking token: *be\u015F'in* (which should be *be\u015Fin* according to Turkish orthography). Here, *5* is part of an entity-the name of a road-and also a number itself. On the other hand, *be\u015F* 'five' is just an ordinary noun and *be\u015F'in* is an ordinary noun with an apostrophe (which is not quite grammatical). So what's the solution here? It totally depends on the model vocabulary of the ASR model that one wants to use. Here, it's indeed better to keep *E-5* intact and not normalize the *5* to *be\u015F* because it's part of an entity. Hence, the final cleaning result should look like this:

> ***E-5'in*** *ilk bin kilometresinde sadece ge\xE7en y\u0131l y\xFCz elli kaza oldu.* 150 accidents happened in the first 1000 km of the road E-5.

Long story short, we need to pay attention to apostrophes to do text cleaning in Turkish correctly. Sometimes we delete them, sometimes we keep them; the important part is to get grammatically correct sentences even after the cleaning and to do so consistently.

### Another Apostrophe Challenge

Because the apostrophe comes hand in hand with inflectional and derivational suffixes, we also have to pay attention to phonological processes such as consonant assimilation and vowel harmony rules. Let's look at what those terms mean, and an example of each. Consonant assimilation refers to two consonants becoming more similar to each other when they occur next to each other. We can see an easy example of this in English with the words *cat* and *dog*. When we add the plural *\\-s* to these words, we pronounce them differently based on the final sound: like an /s/ in *cats* but like a /z/ in *dogs*. (This is specifically an example of [voicing assimilation](https://en.wikipedia.org/wiki/Consonant_voicing_and_devoicing#Voicing_assimilation) if you're curious.) 

This consonant assimilation in English is pronounced but not written. Modern Turkish writing, however, does reflect consonant assimilation when it occurs. And when it comes to consonant assimilation in Turkish, we're actually sort of lucky because even if consonant assimilation occurs with a suffix, the apostrophe is still used.

> *D\xFCn saat* ***3'te*** *beni g\xF6rmeye geldi.* He came to visit me yesterday at 3PM.

In this example, the first consonant of the suffix /-dA/\\*\\* has assimilated and become /te/ after *\xFC\xE7* 'three', so we don't need to do any extra processing after deleting the apostrophe in this case. So far so good, but now let's look at a trickier case with a derivational suffix and abbreviation combo, along with a typo. One of the main issues that comes up with text cleaning is that you often can't assume that your input text follows the normal rules of grammar and orthography, so you have to do some checking.

> *Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.* *Yrd do\xE7 luk s\u0131nav\u0131 kalkm\u0131\u015F.* The entrance exam for assistant professorship is canceled.

In both sentences, the words *yrd* and *do\xE7* are abbreviations. *Yrd* is short for *yard\u0131mc\u0131* 'assistant' and *do\xE7* is short *do\xE7ent* 'professor', so *yrd do\xE7* is a bit like *asst prof*. Here, there are no apostrophes between *do\xE7* and *luk* at all to indicate a suffix is appended to an abbreviation. In the first sentence, the spelling *do\xE7luk* is not great grammatically but still acceptable, while in the second sentence, the two words *do\xE7* and *luk* should not be written separately. In perfect written Turkish, they should be written together, but with an apostrophe, like this: *Yrd do\xE7'luk s\u0131nav\u0131 kalkm\u0131\u015F.* Figuring out what is going on with either of the two original sentences could be difficult, but fortunately we have a hint-*luk* is not a Turkish word; however, it is a suffix. And, what's more, it is following the rules of Turkish vowel harmony here, as it harmonizes with the vowel of the previous word *do\xE7*. [Vowel harmony](https://en.wikipedia.org/wiki/Vowel_harmony), *very* briefly, is a process that forces all of the vowels in a word to be part of the same set. Because /u/ and /o/ are part of the same set, we can conclude the second sentence is a misspelling and we can correct this mistake by deleting the space in between to get a "cleaner" version: *Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.*

### Processing Currencies in Turkish

Phew, that was a lot! The good news is, we can relax a bit when processing some other types of tokens, though; processing currency symbols, for example, is quite easy. Unlike English, in Turkish we don't append a plural suffix to currencies whose value is more than 1. Please compare: 

![](https://res.cloudinary.com/deepgram/image/upload/v1661981076/blog/text-cleaning-asr-turkish/blog-image-3-1.png)

Hence, a simple piece of code is enough to clean up currencies. We first parse out the currency symbols with a tiny regex, look them up from a predefined dictionary of currency words, and finally replace them in the text. The code for this is below in Figure 3. 

\`\`\`python
import re

CURR_SYMS = {
"$": "dolar",
"\u20AC": "euro",
"\xA3": "sterlin",
"\xA5": "yen",
"tl": "lira",
"ytl": "lira",
"\u20BA": "lira"
}

CURR_REGEX = r"([$\u20AC\xA3\xA5\u20BA]|y?tl|try)"

def convert_currency_syms(token):
  token = token.lower()
  words = CURR_SYMS.get(token, token)
  return words

def clean_currency_symbols(sentence):
  match = re.search(CURR_REGEX, sentence)
  while match:
    mstring = match.group()
    mstart, mend = match.span()
    sentence = sentence[:mstart] + " " + convert_currency_syms(mstring) + sentence[mend:]
    match = re.search(CURR_REGEX, sentence)
  return sentence

sent = "Hepsine 100$ verdim."
>> clean_currency_symbols(sent)
"Hepsine 100 dolar verdim."
\`\`\`

**Figure 3.** A sample code for running a regex to get correct Turkish currencies. 

We literally did **nothing** to currency words; no plural suffix or any other suffixes were added by us. If we were processing English, though, we would need to parse the currency symbol together with the number to check if the number is 1 or not; if it's not 1, we need to add a plural suffix to the currency word. Please note that the code segment in Figure 3 shows only one step of processing, that related to processing the currency symbols. As we mentioned previously, all conversions are done in individual steps, hence the number *100* would be processed in a separate number-cleaning step.

### Text Cleaning for Numbers

Numbers can introduce other challenges as well. For example, converting ordinal cardinal and decimal number tokens into words; as well as converting times, dates, telephone numbers, postal codes, and other sorts of number tokens into words.  Parsing number-type tokens can be tricky; numbers come in different shapes. There are decimal numbers (which include commas), big numbers (which include periods), phone numbers (including parenthesis, dashes, plus signs, and even more), postal codes (address strings should be extracted first to spot them), and so on. 

Cleaning numbers is challenging for each language indeed. Quite a number of regexes and other sorts of parsers (including NER and CF parsers) are used. For an example, this regex should parse out big numbers (such as 1,250,000) in English text: \`\\b\\d{1,3}(,\\d{3})+\\b\` Big numbers are written similarly in Turkish, but with a period instead of comma; hence, a tiny modification to this regex would suffice: \`\\b\\d{1,3}([.]\\d{3})+\\b\` It's totally expected, when we're building text cleaning pipelines, that we implement both language-specific and some "universal" cleaning methods. For each language we process, we find common token types and processing methods, but at the same time, we implement quite a lot of language specific text cleaning methods as well.

<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works"></WhitepaperPromo>

## Wrapping Up

When it comes down to it, all languages are beautiful and yet challenging at the same time; dealing with natural language is never easy. At Deepgram, we all enjoy working with speech and text, and we embrace both the difficulty and the beauty of natural language. We hope you enjoyed this article and share our joy for Turkish, too! Please join us for more content on our blog and follow us on [LinkedIn](https://www.linkedin.com/company/deepgram/).

And, if you'd like to give Deepgram a try for yourself, [sign up for Console](https://console.deepgram.com/signup) for free and get $150 in free credits to give it a try. 

- - -

\\* Of course, sometimes we might *want* an email to appear in a final transcript. In that case, we can do a bit of postprocessing to the model output to get the final transcription in order to reverse normalize *...-at-...-dot-com* word sequences into a single email token. But for the purposes of this post, because we're looking at the phonetic level, we'll want to represent the pronunciation rather than what we want the final output to look like. 

\\*\\* The /d/ becomes /t/ due to assimilation with the preceding sound, and the A represents a vowel that changes depending on vowel harmony, which in this word, becomes /e/. Thus, we end up with /te/ for the suffix.`;
}
function compiledContent() {
  return '<p>Since Turkey is celebrating <a href="https://en.wikipedia.org/wiki/Victory_Day_(Turkey)">Victory Day</a> today, and since we\u2019ve been meaning to write a blog post about text cleaning, we figured we might as well write a post about text cleaning and Turkish! For every automatic speech recognition (ASR) system, cleaning the training data is a crucial part. Speech recognition systems learn how to map speech to its written counterpart by learning speech-text pairs. If the data that teaches it how to do this is messy, the system just won\u2019t work well. In this post, we\u2019ll explain what text cleaning is, how it works, and why it\u2019s so important for ASR. Let\u2019s look!</p>\n<h2 id="what-is-text-cleaning">What is Text Cleaning?</h2>\n<p>Simply put, text cleaning (also called text normalization) is a component of natural language processing that takes raw data and neatens it up, usually in order to use it for something like machine learning model training. While preparing our training data, we\u2019d like the transcriptions to match the spoken phoneme sequences as much as possible. For example, if we hear the words \u201CJ. Lo\u201D in the speech file, we\u2019d like its transcription to be \u201CJ. Lo\u201D as well. However, if the speech file includes the words \u201CJennifer Lopez\u201D, we wouldn\u2019t want the transcription to be \u201CJ. Lo\u201D; we\u2019d want it to be \u201CJennifer Lopez\u201D. Although \u201CJennifer Lopez\u201D and \u201CJ. Lo\u201D represent the same entity in the real world, phonetically they don\u2019t match at all. The former word is represented by the sequence of phonemes</p>\n<blockquote>\n<p>JH EH N AH F ER . L OW P EH Z</p>\n</blockquote>\n<p>where the latter word sequence maps to a much shorter phoneme sequence</p>\n<blockquote>\n<p>JH EY . L OW</p>\n</blockquote>\n<p>All of this logic also applies to numbers, abbreviations, email addresses-i.e., nonstandard tokens, where we can end up with written text that doesn\u2019t necessarily match the pronunciation, need to match up to what\u2019s actually being said, and not the real-world entity.</p>\n<h2 id="why-text-cleaning-is-so-important">Why Text Cleaning is So Important</h2>\n<p>There\u2019s an old saying in data science: garbage in, garbage out. This means that if you train a model with bad data, your model isn\u2019t going to be very good.  We want to make sure we have a good match between the phonetics-the actual speech sounds produced-and the phonetic transcription of that speech.</p>\n<p>This is the first step in getting an accurate, readable transcript at the end of the transcription process. This is why text cleaning is so important for ASR training. By doing text cleaning before training, you give your model the best possibility of producing what you want. Let\u2019s take a look at an example of how text cleaning actually works.</p>\n<h2 id="how-text-cleaning-works">How Text Cleaning Works</h2>\n<p>Text cleaning is language-dependent. This is because abbreviations, shortenings, and other special text forms have a language-unique relationship to their spoken tongue. For example, \u201Clb\u201D and its relationship to \u201Cpound\u201D in English isn\u2019t at all obvious if you didn\u2019t already know about it, and isn\u2019t relevant to any other language. To apply text cleaning to training data, we need a multi-step processing pipeline. Each step transforms the text into a \u201Ccleaner\u201D variation of the original.</p>\n<p>By \u201Ccleaner\u201D here we mean \u201Ccloser to the actual phonetics of what was said\u201D. Take a look at Figure 1, below. There you can see an example sentence which is transformed, step by step, into a \u201Ccleanest\u201D final version. It starts in the usual written form in Step 1, but that doesn\u2019t accurately represent the phonetics of what\u2019s being said, so we need to clean it up if we want to use it for model training. In Step 2, we spell out the numbers into words, rather than numerals. And in Step 3, we spell out the components of an email* (which, when said, is a string of words, just as it is in English-\u201Dduygu at deepgram dot com\u201D). In the final step, Step 4, we remove the double period typo so that the sentence has <a href="https://blog.deepgram.com/complete-guide-punctuation-capitalization-speech-to-text/">standard punctuation</a>.</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981074/blog/text-cleaning-asr-turkish/blog-image-1-1.png" alt=""></p>\n<p><strong>Figure 1.</strong> Text cleaning for the Turkish sentence \u201CI sent an email to the email address <code is:raw>hello@company.com</code> yesterday.\u201D</p>\n<h2 id="challenges-for-turkish-text-cleaning">Challenges for Turkish Text Cleaning</h2>\n<p>In the above example, we mostly saw things that would also make sense in other languages-like reformatting an email address into words. But there are unique text cleaning challenges for each and every spoken language, and Turkish is no exception. Let\u2019s consider a few, Turkish-specific examples.</p>\n<h3 id="the-turkish-apostrophe">The Turkish Apostrophe</h3>\n<p>One challenge that\u2019s specific to Turkish, and which we saw an example of in Figure 1 above, is with the use of the apostrophe. The apostrophe is used often in written Turkish, and is used for separating inflectional suffixes from the proper nouns, numbers, and abbreviations that they\u2019re attached to. We need to keep an eye on this when we do text cleaning because some of the changes that we make to the text will also influence whether or not we need to get rid of the apostrophe. For example, when we turn a numeral into a common noun (that is, we turn <em>3</em> into <em>\xFC\xE7)</em> we no longer need the apostrophe after we spell it out as text. In the above example, we saw the token <em>17.30\u2019da</em> became <em>be\u015F bu\xE7ukta</em> and not <em>be\u015F bu\xE7uk\u2019ta;</em> we omitted the apostrophe in order to keep the grammaticality. However, proper nouns will still stay as proper nouns, no matter what text cleaning we do, so they should never lose an apostrophe. But what if our proper noun has a number in it? In Figure 2 below, the apostrophe is used to separate a proper noun from an inflectional suffix, and hence a good question arises: should we keep the apostrophe or not?</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981075/blog/text-cleaning-asr-turkish/blog-image-2.png" alt=""></p>\n<p><strong>Figure 2.</strong> Example of a possible text cleaning for the Turkish sentence \u201C150 accidents happened in the first 1000 km of the road E-5.\u201D</p>\n<p>In Figure 2, we deleted the second apostrophe safely. However, keeping the first apostrophe creates a not-very-good looking token: <em>be\u015F\u2019in</em> (which should be <em>be\u015Fin</em> according to Turkish orthography). Here, <em>5</em> is part of an entity-the name of a road-and also a number itself. On the other hand, <em>be\u015F</em> \u2018five\u2019 is just an ordinary noun and <em>be\u015F\u2019in</em> is an ordinary noun with an apostrophe (which is not quite grammatical). So what\u2019s the solution here? It totally depends on the model vocabulary of the ASR model that one wants to use. Here, it\u2019s indeed better to keep <em>E-5</em> intact and not normalize the <em>5</em> to <em>be\u015F</em> because it\u2019s part of an entity. Hence, the final cleaning result should look like this:</p>\n<blockquote>\n<p><em><strong>E-5\u2019in</strong></em> <em>ilk bin kilometresinde sadece ge\xE7en y\u0131l y\xFCz elli kaza oldu.</em> 150 accidents happened in the first 1000 km of the road E-5.</p>\n</blockquote>\n<p>Long story short, we need to pay attention to apostrophes to do text cleaning in Turkish correctly. Sometimes we delete them, sometimes we keep them; the important part is to get grammatically correct sentences even after the cleaning and to do so consistently.</p>\n<h3 id="another-apostrophe-challenge">Another Apostrophe Challenge</h3>\n<p>Because the apostrophe comes hand in hand with inflectional and derivational suffixes, we also have to pay attention to phonological processes such as consonant assimilation and vowel harmony rules. Let\u2019s look at what those terms mean, and an example of each. Consonant assimilation refers to two consonants becoming more similar to each other when they occur next to each other. We can see an easy example of this in English with the words <em>cat</em> and <em>dog</em>. When we add the plural <em>-s</em> to these words, we pronounce them differently based on the final sound: like an /s/ in <em>cats</em> but like a /z/ in <em>dogs</em>. (This is specifically an example of <a href="https://en.wikipedia.org/wiki/Consonant_voicing_and_devoicing#Voicing_assimilation">voicing assimilation</a> if you\u2019re curious.)</p>\n<p>This consonant assimilation in English is pronounced but not written. Modern Turkish writing, however, does reflect consonant assimilation when it occurs. And when it comes to consonant assimilation in Turkish, we\u2019re actually sort of lucky because even if consonant assimilation occurs with a suffix, the apostrophe is still used.</p>\n<blockquote>\n<p><em>D\xFCn saat</em> <em><strong>3\u2019te</strong></em> <em>beni g\xF6rmeye geldi.</em> He came to visit me yesterday at 3PM.</p>\n</blockquote>\n<p>In this example, the first consonant of the suffix /-dA/** has assimilated and become /te/ after <em>\xFC\xE7</em> \u2018three\u2019, so we don\u2019t need to do any extra processing after deleting the apostrophe in this case. So far so good, but now let\u2019s look at a trickier case with a derivational suffix and abbreviation combo, along with a typo. One of the main issues that comes up with text cleaning is that you often can\u2019t assume that your input text follows the normal rules of grammar and orthography, so you have to do some checking.</p>\n<blockquote>\n<p><em>Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> <em>Yrd do\xE7 luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> The entrance exam for assistant professorship is canceled.</p>\n</blockquote>\n<p>In both sentences, the words <em>yrd</em> and <em>do\xE7</em> are abbreviations. <em>Yrd</em> is short for <em>yard\u0131mc\u0131</em> \u2018assistant\u2019 and <em>do\xE7</em> is short <em>do\xE7ent</em> \u2018professor\u2019, so <em>yrd do\xE7</em> is a bit like <em>asst prof</em>. Here, there are no apostrophes between <em>do\xE7</em> and <em>luk</em> at all to indicate a suffix is appended to an abbreviation. In the first sentence, the spelling <em>do\xE7luk</em> is not great grammatically but still acceptable, while in the second sentence, the two words <em>do\xE7</em> and <em>luk</em> should not be written separately. In perfect written Turkish, they should be written together, but with an apostrophe, like this: <em>Yrd do\xE7\u2019luk s\u0131nav\u0131 kalkm\u0131\u015F.</em> Figuring out what is going on with either of the two original sentences could be difficult, but fortunately we have a hint-<em>luk</em> is not a Turkish word; however, it is a suffix. And, what\u2019s more, it is following the rules of Turkish vowel harmony here, as it harmonizes with the vowel of the previous word <em>do\xE7</em>. <a href="https://en.wikipedia.org/wiki/Vowel_harmony">Vowel harmony</a>, <em>very</em> briefly, is a process that forces all of the vowels in a word to be part of the same set. Because /u/ and /o/ are part of the same set, we can conclude the second sentence is a misspelling and we can correct this mistake by deleting the space in between to get a \u201Ccleaner\u201D version: <em>Yrd do\xE7luk s\u0131nav\u0131 kalkm\u0131\u015F.</em></p>\n<h3 id="processing-currencies-in-turkish">Processing Currencies in Turkish</h3>\n<p>Phew, that was a lot! The good news is, we can relax a bit when processing some other types of tokens, though; processing currency symbols, for example, is quite easy. Unlike English, in Turkish we don\u2019t append a plural suffix to currencies whose value is more than 1. Please compare:</p>\n<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981076/blog/text-cleaning-asr-turkish/blog-image-3-1.png" alt=""></p>\n<p>Hence, a simple piece of code is enough to clean up currencies. We first parse out the currency symbols with a tiny regex, look them up from a predefined dictionary of currency words, and finally replace them in the text. The code for this is below in Figure 3.</p>\n<pre is:raw class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> re</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #79C0FF">CURR_SYMS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;$&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;dolar&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\u20AC&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;euro&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\xA3&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;sterlin&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\xA5&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;yen&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;tl&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;ytl&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span><span style="color: #C9D1D9">,</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;\u20BA&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span></span>\n<span class="line"><span style="color: #C9D1D9">}</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">r</span><span style="color: #A5D6FF">&quot;</span><span style="color: #79C0FF">([$\u20AC\xA3\xA5\u20BA]</span><span style="color: #FF7B72">|</span><span style="color: #A5D6FF">y</span><span style="color: #FF7B72">?</span><span style="color: #A5D6FF">tl</span><span style="color: #FF7B72">|</span><span style="color: #A5D6FF">try</span><span style="color: #79C0FF">)</span><span style="color: #A5D6FF">&quot;</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">convert_currency_syms</span><span style="color: #C9D1D9">(token):</span></span>\n<span class="line"><span style="color: #C9D1D9">  token </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> token.lower()</span></span>\n<span class="line"><span style="color: #C9D1D9">  words </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">CURR_SYMS</span><span style="color: #C9D1D9">.get(token, token)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> words</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">clean_currency_symbols</span><span style="color: #C9D1D9">(sentence):</span></span>\n<span class="line"><span style="color: #C9D1D9">  match </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> re.search(</span><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9">, sentence)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> match:</span></span>\n<span class="line"><span style="color: #C9D1D9">    mstring </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> match.group()</span></span>\n<span class="line"><span style="color: #C9D1D9">    mstart, mend </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> match.span()</span></span>\n<span class="line"><span style="color: #C9D1D9">    sentence </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> sentence[:mstart] </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> convert_currency_syms(mstring) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> sentence[mend:]</span></span>\n<span class="line"><span style="color: #C9D1D9">    match </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> re.search(</span><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9">, sentence)</span></span>\n<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> sentence</span></span>\n<span class="line"></span>\n<span class="line"><span style="color: #C9D1D9">sent </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;Hepsine 100$ verdim.&quot;</span></span>\n<span class="line"><span style="color: #FF7B72">&gt;&gt;</span><span style="color: #C9D1D9"> clean_currency_symbols(sent)</span></span>\n<span class="line"><span style="color: #A5D6FF">&quot;Hepsine 100 dolar verdim.&quot;</span></span></code></pre>\n<p><strong>Figure 3.</strong> A sample code for running a regex to get correct Turkish currencies.</p>\n<p>We literally did <strong>nothing</strong> to currency words; no plural suffix or any other suffixes were added by us. If we were processing English, though, we would need to parse the currency symbol together with the number to check if the number is 1 or not; if it\u2019s not 1, we need to add a plural suffix to the currency word. Please note that the code segment in Figure 3 shows only one step of processing, that related to processing the currency symbols. As we mentioned previously, all conversions are done in individual steps, hence the number <em>100</em> would be processed in a separate number-cleaning step.</p>\n<h3 id="text-cleaning-for-numbers">Text Cleaning for Numbers</h3>\n<p>Numbers can introduce other challenges as well. For example, converting ordinal cardinal and decimal number tokens into words; as well as converting times, dates, telephone numbers, postal codes, and other sorts of number tokens into words.  Parsing number-type tokens can be tricky; numbers come in different shapes. There are decimal numbers (which include commas), big numbers (which include periods), phone numbers (including parenthesis, dashes, plus signs, and even more), postal codes (address strings should be extracted first to spot them), and so on.</p>\n<p>Cleaning numbers is challenging for each language indeed. Quite a number of regexes and other sorts of parsers (including NER and CF parsers) are used. For an example, this regex should parse out big numbers (such as 1,250,000) in English text: <code is:raw>\\b\\d{1,3}(,\\d{3})+\\b</code> Big numbers are written similarly in Turkish, but with a period instead of comma; hence, a tiny modification to this regex would suffice: <code is:raw>\\b\\d{1,3}([.]\\d{3})+\\b</code> It\u2019s totally expected, when we\u2019re building text cleaning pipelines, that we implement both language-specific and some \u201Cuniversal\u201D cleaning methods. For each language we process, we find common token types and processing methods, but at the same time, we implement quite a lot of language specific text cleaning methods as well.</p>\n<WhitepaperPromo whitepaper="deepgram-whitepaper-how-deepgram-works" />\n<h2 id="wrapping-up">Wrapping Up</h2>\n<p>When it comes down to it, all languages are beautiful and yet challenging at the same time; dealing with natural language is never easy. At Deepgram, we all enjoy working with speech and text, and we embrace both the difficulty and the beauty of natural language. We hope you enjoyed this article and share our joy for Turkish, too! Please join us for more content on our blog and follow us on <a href="https://www.linkedin.com/company/deepgram/">LinkedIn</a>.</p>\n<p>And, if you\u2019d like to give Deepgram a try for yourself, <a href="https://console.deepgram.com/signup">sign up for Console</a> for free and get $150 in free credits to give it a try.</p>\n<hr>\n<p>* Of course, sometimes we might <em>want</em> an email to appear in a final transcript. In that case, we can do a bit of postprocessing to the model output to get the final transcription in order to reverse normalize <em>\u2026-at-\u2026-dot-com</em> word sequences into a single email token. But for the purposes of this post, because we\u2019re looking at the phonetic level, we\u2019ll want to represent the pronunciation rather than what we want the final output to look like.</p>\n<p>** The /d/ becomes /t/ due to assimilation with the preceding sound, and the A represents a vowel that changes depending on vowel harmony, which in this word, becomes /e/. Thus, we end up with /te/ for the suffix.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/text-cleaning-asr-turkish/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p>Since Turkey is celebrating <a href="https://en.wikipedia.org/wiki/Victory_Day_(Turkey)">Victory Day</a> today, and since weve been meaning to write a blog post about text cleaning, we figured we might as well write a post about text cleaning and Turkish! For every automatic speech recognition (ASR) system, cleaning the training data is a crucial part. Speech recognition systems learn how to map speech to its written counterpart by learning speech-text pairs. If the data that teaches it how to do this is messy, the system just wont work well. In this post, well explain what text cleaning is, how it works, and why its so important for ASR. Lets look!</p>
<h2 id="what-is-text-cleaning">What is Text Cleaning?</h2>
<p>Simply put, text cleaning (also called text normalization) is a component of natural language processing that takes raw data and neatens it up, usually in order to use it for something like machine learning model training. While preparing our training data, wed like the transcriptions to match the spoken phoneme sequences as much as possible. For example, if we hear the words J. Lo in the speech file, wed like its transcription to be J. Lo as well. However, if the speech file includes the words Jennifer Lopez, we wouldnt want the transcription to be J. Lo; wed want it to be Jennifer Lopez. Although Jennifer Lopez and J. Lo represent the same entity in the real world, phonetically they dont match at all. The former word is represented by the sequence of phonemes</p>
<blockquote>
<p>JH EH N AH F ER . L OW P EH Z</p>
</blockquote>
<p>where the latter word sequence maps to a much shorter phoneme sequence</p>
<blockquote>
<p>JH EY . L OW</p>
</blockquote>
<p>All of this logic also applies to numbers, abbreviations, email addresses-i.e., nonstandard tokens, where we can end up with written text that doesnt necessarily match the pronunciation, need to match up to whats actually being said, and not the real-world entity.</p>
<h2 id="why-text-cleaning-is-so-important">Why Text Cleaning is So Important</h2>
<p>Theres an old saying in data science: garbage in, garbage out. This means that if you train a model with bad data, your model isnt going to be very good.  We want to make sure we have a good match between the phonetics-the actual speech sounds produced-and the phonetic transcription of that speech.</p>
<p>This is the first step in getting an accurate, readable transcript at the end of the transcription process. This is why text cleaning is so important for ASR training. By doing text cleaning before training, you give your model the best possibility of producing what you want. Lets take a look at an example of how text cleaning actually works.</p>
<h2 id="how-text-cleaning-works">How Text Cleaning Works</h2>
<p>Text cleaning is language-dependent. This is because abbreviations, shortenings, and other special text forms have a language-unique relationship to their spoken tongue. For example, lb and its relationship to pound in English isnt at all obvious if you didnt already know about it, and isnt relevant to any other language. To apply text cleaning to training data, we need a multi-step processing pipeline. Each step transforms the text into a cleaner variation of the original.</p>
<p>By cleaner here we mean closer to the actual phonetics of what was said. Take a look at Figure 1, below. There you can see an example sentence which is transformed, step by step, into a cleanest final version. It starts in the usual written form in Step 1, but that doesnt accurately represent the phonetics of whats being said, so we need to clean it up if we want to use it for model training. In Step 2, we spell out the numbers into words, rather than numerals. And in Step 3, we spell out the components of an email* (which, when said, is a string of words, just as it is in English-duygu at deepgram dot com). In the final step, Step 4, we remove the double period typo so that the sentence has <a href="https://blog.deepgram.com/complete-guide-punctuation-capitalization-speech-to-text/">standard punctuation</a>.</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981074/blog/text-cleaning-asr-turkish/blog-image-1-1.png" alt=""></p>
<p><strong>Figure 1.</strong> Text cleaning for the Turkish sentence I sent an email to the email address <code>hello@company.com</code> yesterday.</p>
<h2 id="challenges-for-turkish-text-cleaning">Challenges for Turkish Text Cleaning</h2>
<p>In the above example, we mostly saw things that would also make sense in other languages-like reformatting an email address into words. But there are unique text cleaning challenges for each and every spoken language, and Turkish is no exception. Lets consider a few, Turkish-specific examples.</p>
<h3 id="the-turkish-apostrophe">The Turkish Apostrophe</h3>
<p>One challenge thats specific to Turkish, and which we saw an example of in Figure 1 above, is with the use of the apostrophe. The apostrophe is used often in written Turkish, and is used for separating inflectional suffixes from the proper nouns, numbers, and abbreviations that theyre attached to. We need to keep an eye on this when we do text cleaning because some of the changes that we make to the text will also influence whether or not we need to get rid of the apostrophe. For example, when we turn a numeral into a common noun (that is, we turn <em>3</em> into <em>)</em> we no longer need the apostrophe after we spell it out as text. In the above example, we saw the token <em>17.30da</em> became <em>be buukta</em> and not <em>be buukta;</em> we omitted the apostrophe in order to keep the grammaticality. However, proper nouns will still stay as proper nouns, no matter what text cleaning we do, so they should never lose an apostrophe. But what if our proper noun has a number in it? In Figure 2 below, the apostrophe is used to separate a proper noun from an inflectional suffix, and hence a good question arises: should we keep the apostrophe or not?</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981075/blog/text-cleaning-asr-turkish/blog-image-2.png" alt=""></p>
<p><strong>Figure 2.</strong> Example of a possible text cleaning for the Turkish sentence 150 accidents happened in the first 1000 km of the road E-5.</p>
<p>In Figure 2, we deleted the second apostrophe safely. However, keeping the first apostrophe creates a not-very-good looking token: <em>bein</em> (which should be <em>bein</em> according to Turkish orthography). Here, <em>5</em> is part of an entity-the name of a road-and also a number itself. On the other hand, <em>be</em> five is just an ordinary noun and <em>bein</em> is an ordinary noun with an apostrophe (which is not quite grammatical). So whats the solution here? It totally depends on the model vocabulary of the ASR model that one wants to use. Here, its indeed better to keep <em>E-5</em> intact and not normalize the <em>5</em> to <em>be</em> because its part of an entity. Hence, the final cleaning result should look like this:</p>
<blockquote>
<p><em><strong>E-5in</strong></em> <em>ilk bin kilometresinde sadece geen yl yz elli kaza oldu.</em> 150 accidents happened in the first 1000 km of the road E-5.</p>
</blockquote>
<p>Long story short, we need to pay attention to apostrophes to do text cleaning in Turkish correctly. Sometimes we delete them, sometimes we keep them; the important part is to get grammatically correct sentences even after the cleaning and to do so consistently.</p>
<h3 id="another-apostrophe-challenge">Another Apostrophe Challenge</h3>
<p>Because the apostrophe comes hand in hand with inflectional and derivational suffixes, we also have to pay attention to phonological processes such as consonant assimilation and vowel harmony rules. Lets look at what those terms mean, and an example of each. Consonant assimilation refers to two consonants becoming more similar to each other when they occur next to each other. We can see an easy example of this in English with the words <em>cat</em> and <em>dog</em>. When we add the plural <em>-s</em> to these words, we pronounce them differently based on the final sound: like an /s/ in <em>cats</em> but like a /z/ in <em>dogs</em>. (This is specifically an example of <a href="https://en.wikipedia.org/wiki/Consonant_voicing_and_devoicing#Voicing_assimilation">voicing assimilation</a> if youre curious.)</p>
<p>This consonant assimilation in English is pronounced but not written. Modern Turkish writing, however, does reflect consonant assimilation when it occurs. And when it comes to consonant assimilation in Turkish, were actually sort of lucky because even if consonant assimilation occurs with a suffix, the apostrophe is still used.</p>
<blockquote>
<p><em>Dn saat</em> <em><strong>3te</strong></em> <em>beni grmeye geldi.</em> He came to visit me yesterday at 3PM.</p>
</blockquote>
<p>In this example, the first consonant of the suffix /-dA/** has assimilated and become /te/ after <em></em> three, so we dont need to do any extra processing after deleting the apostrophe in this case. So far so good, but now lets look at a trickier case with a derivational suffix and abbreviation combo, along with a typo. One of the main issues that comes up with text cleaning is that you often cant assume that your input text follows the normal rules of grammar and orthography, so you have to do some checking.</p>
<blockquote>
<p><em>Yrd doluk snav kalkm.</em> <em>Yrd do luk snav kalkm.</em> The entrance exam for assistant professorship is canceled.</p>
</blockquote>
<p>In both sentences, the words <em>yrd</em> and <em>do</em> are abbreviations. <em>Yrd</em> is short for <em>yardmc</em> assistant and <em>do</em> is short <em>doent</em> professor, so <em>yrd do</em> is a bit like <em>asst prof</em>. Here, there are no apostrophes between <em>do</em> and <em>luk</em> at all to indicate a suffix is appended to an abbreviation. In the first sentence, the spelling <em>doluk</em> is not great grammatically but still acceptable, while in the second sentence, the two words <em>do</em> and <em>luk</em> should not be written separately. In perfect written Turkish, they should be written together, but with an apostrophe, like this: <em>Yrd doluk snav kalkm.</em> Figuring out what is going on with either of the two original sentences could be difficult, but fortunately we have a hint-<em>luk</em> is not a Turkish word; however, it is a suffix. And, whats more, it is following the rules of Turkish vowel harmony here, as it harmonizes with the vowel of the previous word <em>do</em>. <a href="https://en.wikipedia.org/wiki/Vowel_harmony">Vowel harmony</a>, <em>very</em> briefly, is a process that forces all of the vowels in a word to be part of the same set. Because /u/ and /o/ are part of the same set, we can conclude the second sentence is a misspelling and we can correct this mistake by deleting the space in between to get a cleaner version: <em>Yrd doluk snav kalkm.</em></p>
<h3 id="processing-currencies-in-turkish">Processing Currencies in Turkish</h3>
<p>Phew, that was a lot! The good news is, we can relax a bit when processing some other types of tokens, though; processing currency symbols, for example, is quite easy. Unlike English, in Turkish we dont append a plural suffix to currencies whose value is more than 1. Please compare:</p>
<p><img src="https://res.cloudinary.com/deepgram/image/upload/v1661981076/blog/text-cleaning-asr-turkish/blog-image-3-1.png" alt=""></p>
<p>Hence, a simple piece of code is enough to clean up currencies. We first parse out the currency symbols with a tiny regex, look them up from a predefined dictionary of currency words, and finally replace them in the text. The code for this is below in Figure 3.</p>
<pre class="astro-code" style="background-color: #0d1117; overflow-x: auto;"><code><span class="line"><span style="color: #FF7B72">import</span><span style="color: #C9D1D9"> re</span></span>
<span class="line"></span>
<span class="line"><span style="color: #79C0FF">CURR_SYMS</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> {</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;$&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;dolar&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;euro&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;sterlin&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;yen&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;tl&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;ytl&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span><span style="color: #C9D1D9">,</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;&quot;</span><span style="color: #C9D1D9">: </span><span style="color: #A5D6FF">&quot;lira&quot;</span></span>
<span class="line"><span style="color: #C9D1D9">}</span></span>
<span class="line"></span>
<span class="line"><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">r</span><span style="color: #A5D6FF">&quot;</span><span style="color: #79C0FF">([$]</span><span style="color: #FF7B72">|</span><span style="color: #A5D6FF">y</span><span style="color: #FF7B72">?</span><span style="color: #A5D6FF">tl</span><span style="color: #FF7B72">|</span><span style="color: #A5D6FF">try</span><span style="color: #79C0FF">)</span><span style="color: #A5D6FF">&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">convert_currency_syms</span><span style="color: #C9D1D9">(token):</span></span>
<span class="line"><span style="color: #C9D1D9">  token </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> token.lower()</span></span>
<span class="line"><span style="color: #C9D1D9">  words </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #79C0FF">CURR_SYMS</span><span style="color: #C9D1D9">.get(token, token)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> words</span></span>
<span class="line"></span>
<span class="line"><span style="color: #FF7B72">def</span><span style="color: #C9D1D9"> </span><span style="color: #D2A8FF">clean_currency_symbols</span><span style="color: #C9D1D9">(sentence):</span></span>
<span class="line"><span style="color: #C9D1D9">  match </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> re.search(</span><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9">, sentence)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">while</span><span style="color: #C9D1D9"> match:</span></span>
<span class="line"><span style="color: #C9D1D9">    mstring </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> match.group()</span></span>
<span class="line"><span style="color: #C9D1D9">    mstart, mend </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> match.span()</span></span>
<span class="line"><span style="color: #C9D1D9">    sentence </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> sentence[:mstart] </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot; &quot;</span><span style="color: #C9D1D9"> </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> convert_currency_syms(mstring) </span><span style="color: #FF7B72">+</span><span style="color: #C9D1D9"> sentence[mend:]</span></span>
<span class="line"><span style="color: #C9D1D9">    match </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> re.search(</span><span style="color: #79C0FF">CURR_REGEX</span><span style="color: #C9D1D9">, sentence)</span></span>
<span class="line"><span style="color: #C9D1D9">  </span><span style="color: #FF7B72">return</span><span style="color: #C9D1D9"> sentence</span></span>
<span class="line"></span>
<span class="line"><span style="color: #C9D1D9">sent </span><span style="color: #FF7B72">=</span><span style="color: #C9D1D9"> </span><span style="color: #A5D6FF">&quot;Hepsine 100$ verdim.&quot;</span></span>
<span class="line"><span style="color: #FF7B72">&gt;&gt;</span><span style="color: #C9D1D9"> clean_currency_symbols(sent)</span></span>
<span class="line"><span style="color: #A5D6FF">&quot;Hepsine 100 dolar verdim.&quot;</span></span></code></pre>
<p><strong>Figure 3.</strong> A sample code for running a regex to get correct Turkish currencies.</p>
<p>We literally did <strong>nothing</strong> to currency words; no plural suffix or any other suffixes were added by us. If we were processing English, though, we would need to parse the currency symbol together with the number to check if the number is 1 or not; if its not 1, we need to add a plural suffix to the currency word. Please note that the code segment in Figure 3 shows only one step of processing, that related to processing the currency symbols. As we mentioned previously, all conversions are done in individual steps, hence the number <em>100</em> would be processed in a separate number-cleaning step.</p>
<h3 id="text-cleaning-for-numbers">Text Cleaning for Numbers</h3>
<p>Numbers can introduce other challenges as well. For example, converting ordinal cardinal and decimal number tokens into words; as well as converting times, dates, telephone numbers, postal codes, and other sorts of number tokens into words.  Parsing number-type tokens can be tricky; numbers come in different shapes. There are decimal numbers (which include commas), big numbers (which include periods), phone numbers (including parenthesis, dashes, plus signs, and even more), postal codes (address strings should be extracted first to spot them), and so on.</p>
<p>Cleaning numbers is challenging for each language indeed. Quite a number of regexes and other sorts of parsers (including NER and CF parsers) are used. For an example, this regex should parse out big numbers (such as 1,250,000) in English text: <code>\\b\\d{1,3}(,\\d{3})+\\b</code> Big numbers are written similarly in Turkish, but with a period instead of comma; hence, a tiny modification to this regex would suffice: <code>\\b\\d{1,3}([.]\\d{3})+\\b</code> Its totally expected, when were building text cleaning pipelines, that we implement both language-specific and some universal cleaning methods. For each language we process, we find common token types and processing methods, but at the same time, we implement quite a lot of language specific text cleaning methods as well.</p>
${renderComponent($$result, "WhitepaperPromo", WhitepaperPromo, { "whitepaper": "deepgram-whitepaper-how-deepgram-works" })}
<h2 id="wrapping-up">Wrapping Up</h2>
<p>When it comes down to it, all languages are beautiful and yet challenging at the same time; dealing with natural language is never easy. At Deepgram, we all enjoy working with speech and text, and we embrace both the difficulty and the beauty of natural language. We hope you enjoyed this article and share our joy for Turkish, too! Please join us for more content on our blog and follow us on <a href="https://www.linkedin.com/company/deepgram/">LinkedIn</a>.</p>
<p>And, if youd like to give Deepgram a try for yourself, <a href="https://console.deepgram.com/signup">sign up for Console</a> for free and get $150 in free credits to give it a try.</p>
<hr>
<p>* Of course, sometimes we might <em>want</em> an email to appear in a final transcript. In that case, we can do a bit of postprocessing to the model output to get the final transcription in order to reverse normalize <em>-at--dot-com</em> word sequences into a single email token. But for the purposes of this post, because were looking at the phonetic level, well want to represent the pronunciation rather than what we want the final output to look like.</p>
<p>** The /d/ becomes /t/ due to assimilation with the preceding sound, and the A represents a vowel that changes depending on vowel harmony, which in this word, becomes /e/. Thus, we end up with /te/ for the suffix.</p>`;
}, "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/text-cleaning-asr-turkish/index.md");

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
