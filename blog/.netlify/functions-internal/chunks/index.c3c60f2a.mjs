import { c as createAstro, a as createComponent, r as renderTemplate, b as renderHead } from '../entry.mjs';
import Slugger from 'github-slugger';
import '@astrojs/netlify/netlify-functions.js';
import 'preact';
import 'preact-render-to-string';
import 'vue';
import 'vue/server-renderer';
import 'html-escaper';
import 'node-html-parser';
/* empty css                           */import 'axios';
/* empty css                          *//* empty css                           *//* empty css                          *//* empty css                              *//* empty css                              */import 'clone-deep';
import 'slugify';
import 'shiki';
/* empty css                           *//* empty css                              */import '@astrojs/rss';
/* empty css                           */import 'mime';
import 'cookie';
import 'kleur/colors';
import 'string-width';
import 'path-browserify';
import 'path-to-regexp';

const metadata = { "headings": [], "source": `*This is the transcript for "Voice in Healthcare," presented by Henry O'Connell, CEO at Canary Speech, presented on day one of Project Voice X.* *The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram's API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*



\\[John Kelvie:] Thanks, Bradley. It's really nice to be here. It's nice to see Bradley again. When the pandemic started, I guess, March of twenty twenty, I remember seeing Bradley on Twitter, and he was saying, oh, just wait. You all be begging to get back to in-person events. And I thought, no. I'm definitely not gonna be begging for that. But I have to say, Bradley, I... I'm so excited to be here. I'm really excited to get these events going again. It's not necessarily that I love traveling, but I really like having the chance to meet people, and the virtual events just really didn't work very well, so great to see everybody here. And so I'm gonna talk today about a case study that we did. We have carved out this niche for testing, and as we've gone farther along as a company working with voice and we've seen these use cases emerge, we've just seen really great applications of testing, and I would even expand it.

We think of it now as just managing the overall conversational AI life cycle. We see that as our responsibility, and we really try to help our customers do that. And so I'm gonna walk you through one of the scenarios where we did do that. But just to start off, I am gonna briefly opine on where AI is today. You know, there are folks who think that this is the current state of it. This is probably... it looks a little bit more menacing than it actually is. But people who think that the AI is really self-learning, autonomous, incredibly intelligent, etcetera, etcetera... we do talk to a lot of customers, and when we talk about how they're gonna train their models, where they're gonna get data from, they're just kinda like, I thought it just trained itself. That's not an uncommon answer. I've even heard people who work at Nuance say that kinda confusingly. You know? And so even though it's quite advanced, I I would say it's a it's a long way off from Skynet. Instead, the mental model I like people to work with is if you just... our new customer of Dialogflow?

Think of it as Google dropped off a beautiful child that looks like this for you. This is what you've received. Yes. It's extremely intelligent. It's very capable. It someday will be very skillful and will respond to customer queries in a useful way, but that day is not today. It's not the day that you start using it. It's not the day that you launch it. You need to constantly train and mature this intelligent being so that it does something meaningful for your customers. And it's not gonna take eighteen years, thankfully, but it's probably gonna take... you know, think eighteen months. Think about that longer time frame. Alright. So onto the case study. I'm not gonna use the customer's name for this. We don't yet have permission for that. Hopefully, we will have that shortly. But it's it's a pretty basic one, and I think the simplicity of it is something I really love about it. It illustrates even for sort of basic cases how tricky it can be to do this stuff.

So this... the part that we helped the customer with initially was just doing an initial lookup of a user based on a member ID, and so that's an alphanumeric sequence. It's three letters, six numbers, and a dash. Really simple. Right? And then once that's been input, they're doing a lookup into their database, and then they know the first name and last name and ZIP code, and they're gonna verify the user as who they purport to be by matching that information. So they ask them what's your name, what's your date of birth, all this other verifying information, and they have to give the right response. I mean, that sounds easy. That sounds like something in twenty twenty one. This should not be hard. And they used what I would say is a pretty stay... state-of-the-art tech stack. They're using Genesys. We do a lot of work with Genesys. We really love those guys. They're using the Genesys flow architect.

I think Voiceflow should really take a lot of business away from that partly 'cause it's not quite as good. And then they have Lex integrated for doing the speech recognition in the NLU, and then they've got a back end that's based on AWS Lambda. So this is this is a a pretty standard tech stack. If you were like, I'm gonna build a call center today, it's very likely that you would see this pop up at the top of your Google searches. You would find a lot of things that suggest this is a great way to go, but it's not. It does absolutely terrible on the alphanumeric recognition. We're not terrible. It does it does poorly on the alphanumeric recognition. On the first name and last name, it does horrendously. Even though you know what the first and last name is, it still can't get it right because Lex is missing features, like recognition hints.

It has an inability to measure... or the customer had inability to measure performance accurately and repeatedly. They didn't have the tools in place for those processes, and they didn't have an ability to improve Lex over time. So one sort of theme for this presentation is, you know, you need to have good models, but then you need to have the engines around it that have good capabilities because, you know, you could have... like, let's pick on Google. Obviously, their models are incredible, but they don't have as much tooling for actually tuning those models. And so that can be a limitation if you're using, like, Google's ASR. So what did we see with the member ID? We saw a thirteen... greater than thirteen percent error rate on matching the the correct member ID that the user said. Like I said, really not very good. 

A lot of users are not being understood with that. And then on the first name and last name, it was a forty one point five percent and fifty six point eight percent, respectively, error rates. So that's the part that's really, you know, horrendous and just really not workable. The type of project... and we hear this a lot from people using Lex and Dialogflow where people are like, gee. I don't know that we can go forward with this. We saw this as a big strategic initiative for our company, but these numbers, this is not gonna work for our customers. They're just gonna get frustrated. We're not gonna be delivering a satisfying customer experience, whatsoever. So then we do some training on it. And in this case, it was just actually... the training is in the form... because there's not actually much that we can do with Lex there, we're actually just applying a fuzzy search algorithm, a really simple algorithm after we get the results from the ASR slash NLU. And and that does actually really help a lot, putting that in place. On the first name, last name, though... I mean, we tried a bunch of stuff with Lex.

There really wasn't much that actually improves it. So there's some... in a way, these improvements are substantial, but they don't actually get it to a place where it really becomes usable. Thirty five point one percent error rate, forty eight point eight percent error rate are really not in an acceptable range. So then we said, well, let's try Azure speech. And, you know, in general, that's a product that we like. We think it works really well. By switching to that, what we found was that on that first name and last name, initially, it was seventy five percent correct. And then by just adding in recognition hints... and this almost feels like cheating, but when you say, ok. This is the name we're expecting.

If you feed that to Azure first, it then understands it correctly ninety eight point four percent of the time. Same thing on the last name. So vastly reducing the number of errors, really improving that experience, you know, getting it to a range that is gonna feel almost perfect for users. And this is not... I mean, I think most modern ASRs would deliver a performance that are similar to this with their models, so it's not really a model limitation that we're looking in here. Instead, it's about the tooling that's provided by these different vendors. And some of it's a limitation of Lex. Some of it's a limitation of Genesys. But a lot of these tech stacks are really early. They don't have, you know, features like recognition hints. I mean, on one hand, that's been something that's been around for twenty plus years for IVRs. 

At the same time, though, some of the more newfangled ones... you know, the Dialogflow team has not had a chance to enter... to introduce this yet. Same thing with Lex. So they're sort of behind in a certain way while, at the same time, they have pushed the state of innovation forward. On the member IDs using Azure, here, we did not see a very good performance measure, though. It really struggled with the member IDs. Even with doing an extensive amount of training, it still did not do very well. I personally found this kinda surprising. I mean, we do like, in general, the Azure tool kit. It does give you a lot of power and flexibility, someone who's doing modeling, but this was a limitation that we did run into. 

I will say at the same time, I think this is probably overcomeable. We're reaching out to some people at Microsoft. There's likely a way to resolve this and get to a better performance level with Azure. That said, it's still... it's interesting to see that Lex here, out of the box, has really done better than Azure. And so that sorta leads us to, well, a couple overall larger points. You know, one, Azure provides more leverage for optimization than Lex. It performs very well for transcription. It does not perform as well on a tightly prescribed domain. And so that leads us to these sort of larger observations. If you're building a conversational application, the things that you should be looking for, the qualities of that system that you build are that it should be trainable, each component with it... within it should be able to be tuned and trained quite easily. It should be testable. It should be easy to measure. You should know what those measurements are. You should know what the accuracy level is. It should be modular. Each component should be able to be easily swapped out for another when and if necessary. 

I mean, the state of the art in this space is just changing all the time. And there's companies like Deepgram. I'm very interested in their presentation. They're doing really innovative stuff. There's a lot of other companies doing innovative stuff, and so you wanna be able to take the best of what's out there and easily bring it into your system. And then finally, it should be contextual. One thing I like about this case study a lot is you're just talking about... it's really just one intent, and we're really focused on two slots within that single intent. But even within them, you really need two different models. 

The actual solution that we've ended up using going forward with this specific case is a a grammar that's built with IBM Watson for that member ID. Right? And so that... that's a specific way to solve that slot-filling challenge. But then you get to the first name, last name, and there's a different approach that you wanna take. That's not something people should be afraid of. In fact, that's that's just kind of the nature of the beast. You... even within using a single vendor, you may end up with using different models between those different slots between the different intents.

So you want it to be contextual. You wanna embrace making very granular decisions and doing really granular processing on these transactions to get results that are really gonna wow customers. And so we see this overall as part of a a larger ecosystem.

We are, as a company, expanding the footprint of our software where we we really see ourselves as a platform that is supporting this whole life cycle. We really see that as essential. And the analytics and the management that are coming out of this, we really see as essential. We're helping customers with this orchestrator piece that sits in the center that'll actually act as a runtime to interact with the different ASRs and NLUs that are out there, and then at the same time, capture all this data.

Now, ideally, it would not be necessary for us to supply that orchestration piece. But, again, it's it's not easy. You know, if you use Twilio or Genesys, they don't really supply that. So we see this as a whole platform that then is gonna make it easy to maintain and manage your AI system over time. And so what are some... the benefits of this? You're removing the risk of any single vendor. You're creating a system that can be easily improved and optimized over time, really, nearly endlessly. That's that's not necessarily such a great thing. I mean... but it is optimization, so there's always room for improvement. You're never really gonna get to a hundred percent, and it allows you to hire the best tech for each job. You know? So you might use Lex for one thing, Watson for another, Azure for a third, Deepgram for another.

And so just to kinda zoom in on that platform a little bit, this is what we see as that overall cycle where, you know, there's a gathering of data, testing, training, monitoring on an ongoing basis, and then that is basically repeated ad infinitum. I'm not saying that right. But, basically, it's it's repeated indefinitely as your system evolves and until it becomes truly a sort of mature, intelligent adult. And I did set a timer just to remind myself when it's fifteen minutes. So I'm on my last slide here. You wanna go... you wanna take your little intelligent baby, get it walking, running, flying, and this does come back to an ROI. I put in here... we have a nice ROI calculator on our website now. We think it's pretty cool. If you plug in the numbers, the ROI, if you make significant improvements to your accuracy, really huge. It's really huge. So there's an immense benefit from that. You know, for even fairly moderate usage systems, you can really see substantial savings. So it really does matter. This is not just an academic exercise that we're talking about this for. It's gonna make customers happier, and it's gonna really deliver to your bottom line. So that's my talk. Thanks a lot for having me, and I'm John Kelvie. I don't have a slide with my name, but it's j-p-k b-s-t on Twitter, and, you know, look forward to talking with and meeting all you.`, "html": '<p><em>This is the transcript for \u201CVoice in Healthcare,\u201D presented by Henry O\u2019Connell, CEO at Canary Speech, presented on day one of Project Voice X.</em> <em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was <strong>94% accurate.</strong>  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.  If you want to see if Deepgram is right for your use case, <a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[John Kelvie:] Thanks, Bradley. It\u2019s really nice to be here. It\u2019s nice to see Bradley again. When the pandemic started, I guess, March of twenty twenty, I remember seeing Bradley on Twitter, and he was saying, oh, just wait. You all be begging to get back to in-person events. And I thought, no. I\u2019m definitely not gonna be begging for that. But I have to say, Bradley, I\u2026 I\u2019m so excited to be here. I\u2019m really excited to get these events going again. It\u2019s not necessarily that I love traveling, but I really like having the chance to meet people, and the virtual events just really didn\u2019t work very well, so great to see everybody here. And so I\u2019m gonna talk today about a case study that we did. We have carved out this niche for testing, and as we\u2019ve gone farther along as a company working with voice and we\u2019ve seen these use cases emerge, we\u2019ve just seen really great applications of testing, and I would even expand it.</p>\n<p>We think of it now as just managing the overall conversational AI life cycle. We see that as our responsibility, and we really try to help our customers do that. And so I\u2019m gonna walk you through one of the scenarios where we did do that. But just to start off, I am gonna briefly opine on where AI is today. You know, there are folks who think that this is the current state of it. This is probably\u2026 it looks a little bit more menacing than it actually is. But people who think that the AI is really self-learning, autonomous, incredibly intelligent, etcetera, etcetera\u2026 we do talk to a lot of customers, and when we talk about how they\u2019re gonna train their models, where they\u2019re gonna get data from, they\u2019re just kinda like, I thought it just trained itself. That\u2019s not an uncommon answer. I\u2019ve even heard people who work at Nuance say that kinda confusingly. You know? And so even though it\u2019s quite advanced, I I would say it\u2019s a it\u2019s a long way off from Skynet. Instead, the mental model I like people to work with is if you just\u2026 our new customer of Dialogflow?</p>\n<p>Think of it as Google dropped off a beautiful child that looks like this for you. This is what you\u2019ve received. Yes. It\u2019s extremely intelligent. It\u2019s very capable. It someday will be very skillful and will respond to customer queries in a useful way, but that day is not today. It\u2019s not the day that you start using it. It\u2019s not the day that you launch it. You need to constantly train and mature this intelligent being so that it does something meaningful for your customers. And it\u2019s not gonna take eighteen years, thankfully, but it\u2019s probably gonna take\u2026 you know, think eighteen months. Think about that longer time frame. Alright. So onto the case study. I\u2019m not gonna use the customer\u2019s name for this. We don\u2019t yet have permission for that. Hopefully, we will have that shortly. But it\u2019s it\u2019s a pretty basic one, and I think the simplicity of it is something I really love about it. It illustrates even for sort of basic cases how tricky it can be to do this stuff.</p>\n<p>So this\u2026 the part that we helped the customer with initially was just doing an initial lookup of a user based on a member ID, and so that\u2019s an alphanumeric sequence. It\u2019s three letters, six numbers, and a dash. Really simple. Right? And then once that\u2019s been input, they\u2019re doing a lookup into their database, and then they know the first name and last name and ZIP code, and they\u2019re gonna verify the user as who they purport to be by matching that information. So they ask them what\u2019s your name, what\u2019s your date of birth, all this other verifying information, and they have to give the right response. I mean, that sounds easy. That sounds like something in twenty twenty one. This should not be hard. And they used what I would say is a pretty stay\u2026 state-of-the-art tech stack. They\u2019re using Genesys. We do a lot of work with Genesys. We really love those guys. They\u2019re using the Genesys flow architect.</p>\n<p>I think Voiceflow should really take a lot of business away from that partly \u2018cause it\u2019s not quite as good. And then they have Lex integrated for doing the speech recognition in the NLU, and then they\u2019ve got a back end that\u2019s based on AWS Lambda. So this is this is a a pretty standard tech stack. If you were like, I\u2019m gonna build a call center today, it\u2019s very likely that you would see this pop up at the top of your Google searches. You would find a lot of things that suggest this is a great way to go, but it\u2019s not. It does absolutely terrible on the alphanumeric recognition. We\u2019re not terrible. It does it does poorly on the alphanumeric recognition. On the first name and last name, it does horrendously. Even though you know what the first and last name is, it still can\u2019t get it right because Lex is missing features, like recognition hints.</p>\n<p>It has an inability to measure\u2026 or the customer had inability to measure performance accurately and repeatedly. They didn\u2019t have the tools in place for those processes, and they didn\u2019t have an ability to improve Lex over time. So one sort of theme for this presentation is, you know, you need to have good models, but then you need to have the engines around it that have good capabilities because, you know, you could have\u2026 like, let\u2019s pick on Google. Obviously, their models are incredible, but they don\u2019t have as much tooling for actually tuning those models. And so that can be a limitation if you\u2019re using, like, Google\u2019s ASR. So what did we see with the member ID? We saw a thirteen\u2026 greater than thirteen percent error rate on matching the the correct member ID that the user said. Like I said, really not very good.</p>\n<p>A lot of users are not being understood with that. And then on the first name and last name, it was a forty one point five percent and fifty six point eight percent, respectively, error rates. So that\u2019s the part that\u2019s really, you know, horrendous and just really not workable. The type of project\u2026 and we hear this a lot from people using Lex and Dialogflow where people are like, gee. I don\u2019t know that we can go forward with this. We saw this as a big strategic initiative for our company, but these numbers, this is not gonna work for our customers. They\u2019re just gonna get frustrated. We\u2019re not gonna be delivering a satisfying customer experience, whatsoever. So then we do some training on it. And in this case, it was just actually\u2026 the training is in the form\u2026 because there\u2019s not actually much that we can do with Lex there, we\u2019re actually just applying a fuzzy search algorithm, a really simple algorithm after we get the results from the ASR slash NLU. And and that does actually really help a lot, putting that in place. On the first name, last name, though\u2026 I mean, we tried a bunch of stuff with Lex.</p>\n<p>There really wasn\u2019t much that actually improves it. So there\u2019s some\u2026 in a way, these improvements are substantial, but they don\u2019t actually get it to a place where it really becomes usable. Thirty five point one percent error rate, forty eight point eight percent error rate are really not in an acceptable range. So then we said, well, let\u2019s try Azure speech. And, you know, in general, that\u2019s a product that we like. We think it works really well. By switching to that, what we found was that on that first name and last name, initially, it was seventy five percent correct. And then by just adding in recognition hints\u2026 and this almost feels like cheating, but when you say, ok. This is the name we\u2019re expecting.</p>\n<p>If you feed that to Azure first, it then understands it correctly ninety eight point four percent of the time. Same thing on the last name. So vastly reducing the number of errors, really improving that experience, you know, getting it to a range that is gonna feel almost perfect for users. And this is not\u2026 I mean, I think most modern ASRs would deliver a performance that are similar to this with their models, so it\u2019s not really a model limitation that we\u2019re looking in here. Instead, it\u2019s about the tooling that\u2019s provided by these different vendors. And some of it\u2019s a limitation of Lex. Some of it\u2019s a limitation of Genesys. But a lot of these tech stacks are really early. They don\u2019t have, you know, features like recognition hints. I mean, on one hand, that\u2019s been something that\u2019s been around for twenty plus years for IVRs.</p>\n<p>At the same time, though, some of the more newfangled ones\u2026 you know, the Dialogflow team has not had a chance to enter\u2026 to introduce this yet. Same thing with Lex. So they\u2019re sort of behind in a certain way while, at the same time, they have pushed the state of innovation forward. On the member IDs using Azure, here, we did not see a very good performance measure, though. It really struggled with the member IDs. Even with doing an extensive amount of training, it still did not do very well. I personally found this kinda surprising. I mean, we do like, in general, the Azure tool kit. It does give you a lot of power and flexibility, someone who\u2019s doing modeling, but this was a limitation that we did run into.</p>\n<p>I will say at the same time, I think this is probably overcomeable. We\u2019re reaching out to some people at Microsoft. There\u2019s likely a way to resolve this and get to a better performance level with Azure. That said, it\u2019s still\u2026 it\u2019s interesting to see that Lex here, out of the box, has really done better than Azure. And so that sorta leads us to, well, a couple overall larger points. You know, one, Azure provides more leverage for optimization than Lex. It performs very well for transcription. It does not perform as well on a tightly prescribed domain. And so that leads us to these sort of larger observations. If you\u2019re building a conversational application, the things that you should be looking for, the qualities of that system that you build are that it should be trainable, each component with it\u2026 within it should be able to be tuned and trained quite easily. It should be testable. It should be easy to measure. You should know what those measurements are. You should know what the accuracy level is. It should be modular. Each component should be able to be easily swapped out for another when and if necessary.</p>\n<p>I mean, the state of the art in this space is just changing all the time. And there\u2019s companies like Deepgram. I\u2019m very interested in their presentation. They\u2019re doing really innovative stuff. There\u2019s a lot of other companies doing innovative stuff, and so you wanna be able to take the best of what\u2019s out there and easily bring it into your system. And then finally, it should be contextual. One thing I like about this case study a lot is you\u2019re just talking about\u2026 it\u2019s really just one intent, and we\u2019re really focused on two slots within that single intent. But even within them, you really need two different models.</p>\n<p>The actual solution that we\u2019ve ended up using going forward with this specific case is a a grammar that\u2019s built with IBM Watson for that member ID. Right? And so that\u2026 that\u2019s a specific way to solve that slot-filling challenge. But then you get to the first name, last name, and there\u2019s a different approach that you wanna take. That\u2019s not something people should be afraid of. In fact, that\u2019s that\u2019s just kind of the nature of the beast. You\u2026 even within using a single vendor, you may end up with using different models between those different slots between the different intents.</p>\n<p>So you want it to be contextual. You wanna embrace making very granular decisions and doing really granular processing on these transactions to get results that are really gonna wow customers. And so we see this overall as part of a a larger ecosystem.</p>\n<p>We are, as a company, expanding the footprint of our software where we we really see ourselves as a platform that is supporting this whole life cycle. We really see that as essential. And the analytics and the management that are coming out of this, we really see as essential. We\u2019re helping customers with this orchestrator piece that sits in the center that\u2019ll actually act as a runtime to interact with the different ASRs and NLUs that are out there, and then at the same time, capture all this data.</p>\n<p>Now, ideally, it would not be necessary for us to supply that orchestration piece. But, again, it\u2019s it\u2019s not easy. You know, if you use Twilio or Genesys, they don\u2019t really supply that. So we see this as a whole platform that then is gonna make it easy to maintain and manage your AI system over time. And so what are some\u2026 the benefits of this? You\u2019re removing the risk of any single vendor. You\u2019re creating a system that can be easily improved and optimized over time, really, nearly endlessly. That\u2019s that\u2019s not necessarily such a great thing. I mean\u2026 but it is optimization, so there\u2019s always room for improvement. You\u2019re never really gonna get to a hundred percent, and it allows you to hire the best tech for each job. You know? So you might use Lex for one thing, Watson for another, Azure for a third, Deepgram for another.</p>\n<p>And so just to kinda zoom in on that platform a little bit, this is what we see as that overall cycle where, you know, there\u2019s a gathering of data, testing, training, monitoring on an ongoing basis, and then that is basically repeated ad infinitum. I\u2019m not saying that right. But, basically, it\u2019s it\u2019s repeated indefinitely as your system evolves and until it becomes truly a sort of mature, intelligent adult. And I did set a timer just to remind myself when it\u2019s fifteen minutes. So I\u2019m on my last slide here. You wanna go\u2026 you wanna take your little intelligent baby, get it walking, running, flying, and this does come back to an ROI. I put in here\u2026 we have a nice ROI calculator on our website now. We think it\u2019s pretty cool. If you plug in the numbers, the ROI, if you make significant improvements to your accuracy, really huge. It\u2019s really huge. So there\u2019s an immense benefit from that. You know, for even fairly moderate usage systems, you can really see substantial savings. So it really does matter. This is not just an academic exercise that we\u2019re talking about this for. It\u2019s gonna make customers happier, and it\u2019s gonna really deliver to your bottom line. So that\u2019s my talk. Thanks a lot for having me, and I\u2019m John Kelvie. I don\u2019t have a slide with my name, but it\u2019s j-p-k b-s-t on Twitter, and, you know, look forward to talking with and meeting all you.</p>' };
const frontmatter = { "title": "The Importance of Testing with Voice Experiences and Conversational AI - John Kelvie, CEO, Bespoken - Project Voice X", "description": "The Importance of Testing with Voice Experiences and Conversational AI presented by John Kelvie, CEO of Bespoken, presented on day one of Project Voice X.\xA0", "date": "2021-12-09T00:00:00.000Z", "cover": "https://res.cloudinary.com/deepgram/image/upload/v1661981398/blog/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/proj-voice-x-session-john-kelvie-blog-thumb-554x22.png", "authors": ["claudia-ring"], "category": "speech-trends", "tags": ["conversational-ai", "genesys", "project-voice-x"], "seo": { "title": "The Importance of Testing with Voice Experiences and Conversational AI - John Kelvie, CEO, Bespoken - Project Voice X", "description": "The Importance of Testing with Voice Experiences and Conversational AI presented by John Kelvie, CEO of Bespoken, presented on day one of Project Voice X.\xA0" }, "og": { "image": "https://res.cloudinary.com/deepgram/image/upload/v1661981398/blog/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/proj-voice-x-session-john-kelvie-blog-thumb-554x22.png" }, "shorturls": { "share": "https://dpgr.am/f82942c", "twitter": "https://dpgr.am/153675f", "linkedin": "https://dpgr.am/f622203", "reddit": "https://dpgr.am/3f8025b", "facebook": "https://dpgr.am/2ae1d1c" }, "astro": { "headings": [], "source": `*This is the transcript for "Voice in Healthcare," presented by Henry O'Connell, CEO at Canary Speech, presented on day one of Project Voice X.* *The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram's API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*



\\[John Kelvie:] Thanks, Bradley. It's really nice to be here. It's nice to see Bradley again. When the pandemic started, I guess, March of twenty twenty, I remember seeing Bradley on Twitter, and he was saying, oh, just wait. You all be begging to get back to in-person events. And I thought, no. I'm definitely not gonna be begging for that. But I have to say, Bradley, I... I'm so excited to be here. I'm really excited to get these events going again. It's not necessarily that I love traveling, but I really like having the chance to meet people, and the virtual events just really didn't work very well, so great to see everybody here. And so I'm gonna talk today about a case study that we did. We have carved out this niche for testing, and as we've gone farther along as a company working with voice and we've seen these use cases emerge, we've just seen really great applications of testing, and I would even expand it.

We think of it now as just managing the overall conversational AI life cycle. We see that as our responsibility, and we really try to help our customers do that. And so I'm gonna walk you through one of the scenarios where we did do that. But just to start off, I am gonna briefly opine on where AI is today. You know, there are folks who think that this is the current state of it. This is probably... it looks a little bit more menacing than it actually is. But people who think that the AI is really self-learning, autonomous, incredibly intelligent, etcetera, etcetera... we do talk to a lot of customers, and when we talk about how they're gonna train their models, where they're gonna get data from, they're just kinda like, I thought it just trained itself. That's not an uncommon answer. I've even heard people who work at Nuance say that kinda confusingly. You know? And so even though it's quite advanced, I I would say it's a it's a long way off from Skynet. Instead, the mental model I like people to work with is if you just... our new customer of Dialogflow?

Think of it as Google dropped off a beautiful child that looks like this for you. This is what you've received. Yes. It's extremely intelligent. It's very capable. It someday will be very skillful and will respond to customer queries in a useful way, but that day is not today. It's not the day that you start using it. It's not the day that you launch it. You need to constantly train and mature this intelligent being so that it does something meaningful for your customers. And it's not gonna take eighteen years, thankfully, but it's probably gonna take... you know, think eighteen months. Think about that longer time frame. Alright. So onto the case study. I'm not gonna use the customer's name for this. We don't yet have permission for that. Hopefully, we will have that shortly. But it's it's a pretty basic one, and I think the simplicity of it is something I really love about it. It illustrates even for sort of basic cases how tricky it can be to do this stuff.

So this... the part that we helped the customer with initially was just doing an initial lookup of a user based on a member ID, and so that's an alphanumeric sequence. It's three letters, six numbers, and a dash. Really simple. Right? And then once that's been input, they're doing a lookup into their database, and then they know the first name and last name and ZIP code, and they're gonna verify the user as who they purport to be by matching that information. So they ask them what's your name, what's your date of birth, all this other verifying information, and they have to give the right response. I mean, that sounds easy. That sounds like something in twenty twenty one. This should not be hard. And they used what I would say is a pretty stay... state-of-the-art tech stack. They're using Genesys. We do a lot of work with Genesys. We really love those guys. They're using the Genesys flow architect.

I think Voiceflow should really take a lot of business away from that partly 'cause it's not quite as good. And then they have Lex integrated for doing the speech recognition in the NLU, and then they've got a back end that's based on AWS Lambda. So this is this is a a pretty standard tech stack. If you were like, I'm gonna build a call center today, it's very likely that you would see this pop up at the top of your Google searches. You would find a lot of things that suggest this is a great way to go, but it's not. It does absolutely terrible on the alphanumeric recognition. We're not terrible. It does it does poorly on the alphanumeric recognition. On the first name and last name, it does horrendously. Even though you know what the first and last name is, it still can't get it right because Lex is missing features, like recognition hints.

It has an inability to measure... or the customer had inability to measure performance accurately and repeatedly. They didn't have the tools in place for those processes, and they didn't have an ability to improve Lex over time. So one sort of theme for this presentation is, you know, you need to have good models, but then you need to have the engines around it that have good capabilities because, you know, you could have... like, let's pick on Google. Obviously, their models are incredible, but they don't have as much tooling for actually tuning those models. And so that can be a limitation if you're using, like, Google's ASR. So what did we see with the member ID? We saw a thirteen... greater than thirteen percent error rate on matching the the correct member ID that the user said. Like I said, really not very good. 

A lot of users are not being understood with that. And then on the first name and last name, it was a forty one point five percent and fifty six point eight percent, respectively, error rates. So that's the part that's really, you know, horrendous and just really not workable. The type of project... and we hear this a lot from people using Lex and Dialogflow where people are like, gee. I don't know that we can go forward with this. We saw this as a big strategic initiative for our company, but these numbers, this is not gonna work for our customers. They're just gonna get frustrated. We're not gonna be delivering a satisfying customer experience, whatsoever. So then we do some training on it. And in this case, it was just actually... the training is in the form... because there's not actually much that we can do with Lex there, we're actually just applying a fuzzy search algorithm, a really simple algorithm after we get the results from the ASR slash NLU. And and that does actually really help a lot, putting that in place. On the first name, last name, though... I mean, we tried a bunch of stuff with Lex.

There really wasn't much that actually improves it. So there's some... in a way, these improvements are substantial, but they don't actually get it to a place where it really becomes usable. Thirty five point one percent error rate, forty eight point eight percent error rate are really not in an acceptable range. So then we said, well, let's try Azure speech. And, you know, in general, that's a product that we like. We think it works really well. By switching to that, what we found was that on that first name and last name, initially, it was seventy five percent correct. And then by just adding in recognition hints... and this almost feels like cheating, but when you say, ok. This is the name we're expecting.

If you feed that to Azure first, it then understands it correctly ninety eight point four percent of the time. Same thing on the last name. So vastly reducing the number of errors, really improving that experience, you know, getting it to a range that is gonna feel almost perfect for users. And this is not... I mean, I think most modern ASRs would deliver a performance that are similar to this with their models, so it's not really a model limitation that we're looking in here. Instead, it's about the tooling that's provided by these different vendors. And some of it's a limitation of Lex. Some of it's a limitation of Genesys. But a lot of these tech stacks are really early. They don't have, you know, features like recognition hints. I mean, on one hand, that's been something that's been around for twenty plus years for IVRs. 

At the same time, though, some of the more newfangled ones... you know, the Dialogflow team has not had a chance to enter... to introduce this yet. Same thing with Lex. So they're sort of behind in a certain way while, at the same time, they have pushed the state of innovation forward. On the member IDs using Azure, here, we did not see a very good performance measure, though. It really struggled with the member IDs. Even with doing an extensive amount of training, it still did not do very well. I personally found this kinda surprising. I mean, we do like, in general, the Azure tool kit. It does give you a lot of power and flexibility, someone who's doing modeling, but this was a limitation that we did run into. 

I will say at the same time, I think this is probably overcomeable. We're reaching out to some people at Microsoft. There's likely a way to resolve this and get to a better performance level with Azure. That said, it's still... it's interesting to see that Lex here, out of the box, has really done better than Azure. And so that sorta leads us to, well, a couple overall larger points. You know, one, Azure provides more leverage for optimization than Lex. It performs very well for transcription. It does not perform as well on a tightly prescribed domain. And so that leads us to these sort of larger observations. If you're building a conversational application, the things that you should be looking for, the qualities of that system that you build are that it should be trainable, each component with it... within it should be able to be tuned and trained quite easily. It should be testable. It should be easy to measure. You should know what those measurements are. You should know what the accuracy level is. It should be modular. Each component should be able to be easily swapped out for another when and if necessary. 

I mean, the state of the art in this space is just changing all the time. And there's companies like Deepgram. I'm very interested in their presentation. They're doing really innovative stuff. There's a lot of other companies doing innovative stuff, and so you wanna be able to take the best of what's out there and easily bring it into your system. And then finally, it should be contextual. One thing I like about this case study a lot is you're just talking about... it's really just one intent, and we're really focused on two slots within that single intent. But even within them, you really need two different models. 

The actual solution that we've ended up using going forward with this specific case is a a grammar that's built with IBM Watson for that member ID. Right? And so that... that's a specific way to solve that slot-filling challenge. But then you get to the first name, last name, and there's a different approach that you wanna take. That's not something people should be afraid of. In fact, that's that's just kind of the nature of the beast. You... even within using a single vendor, you may end up with using different models between those different slots between the different intents.

So you want it to be contextual. You wanna embrace making very granular decisions and doing really granular processing on these transactions to get results that are really gonna wow customers. And so we see this overall as part of a a larger ecosystem.

We are, as a company, expanding the footprint of our software where we we really see ourselves as a platform that is supporting this whole life cycle. We really see that as essential. And the analytics and the management that are coming out of this, we really see as essential. We're helping customers with this orchestrator piece that sits in the center that'll actually act as a runtime to interact with the different ASRs and NLUs that are out there, and then at the same time, capture all this data.

Now, ideally, it would not be necessary for us to supply that orchestration piece. But, again, it's it's not easy. You know, if you use Twilio or Genesys, they don't really supply that. So we see this as a whole platform that then is gonna make it easy to maintain and manage your AI system over time. And so what are some... the benefits of this? You're removing the risk of any single vendor. You're creating a system that can be easily improved and optimized over time, really, nearly endlessly. That's that's not necessarily such a great thing. I mean... but it is optimization, so there's always room for improvement. You're never really gonna get to a hundred percent, and it allows you to hire the best tech for each job. You know? So you might use Lex for one thing, Watson for another, Azure for a third, Deepgram for another.

And so just to kinda zoom in on that platform a little bit, this is what we see as that overall cycle where, you know, there's a gathering of data, testing, training, monitoring on an ongoing basis, and then that is basically repeated ad infinitum. I'm not saying that right. But, basically, it's it's repeated indefinitely as your system evolves and until it becomes truly a sort of mature, intelligent adult. And I did set a timer just to remind myself when it's fifteen minutes. So I'm on my last slide here. You wanna go... you wanna take your little intelligent baby, get it walking, running, flying, and this does come back to an ROI. I put in here... we have a nice ROI calculator on our website now. We think it's pretty cool. If you plug in the numbers, the ROI, if you make significant improvements to your accuracy, really huge. It's really huge. So there's an immense benefit from that. You know, for even fairly moderate usage systems, you can really see substantial savings. So it really does matter. This is not just an academic exercise that we're talking about this for. It's gonna make customers happier, and it's gonna really deliver to your bottom line. So that's my talk. Thanks a lot for having me, and I'm John Kelvie. I don't have a slide with my name, but it's j-p-k b-s-t on Twitter, and, you know, look forward to talking with and meeting all you.`, "html": '<p><em>This is the transcript for \u201CVoice in Healthcare,\u201D presented by Henry O\u2019Connell, CEO at Canary Speech, presented on day one of Project Voice X.</em> <em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was <strong>94% accurate.</strong>  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.  If you want to see if Deepgram is right for your use case, <a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[John Kelvie:] Thanks, Bradley. It\u2019s really nice to be here. It\u2019s nice to see Bradley again. When the pandemic started, I guess, March of twenty twenty, I remember seeing Bradley on Twitter, and he was saying, oh, just wait. You all be begging to get back to in-person events. And I thought, no. I\u2019m definitely not gonna be begging for that. But I have to say, Bradley, I\u2026 I\u2019m so excited to be here. I\u2019m really excited to get these events going again. It\u2019s not necessarily that I love traveling, but I really like having the chance to meet people, and the virtual events just really didn\u2019t work very well, so great to see everybody here. And so I\u2019m gonna talk today about a case study that we did. We have carved out this niche for testing, and as we\u2019ve gone farther along as a company working with voice and we\u2019ve seen these use cases emerge, we\u2019ve just seen really great applications of testing, and I would even expand it.</p>\n<p>We think of it now as just managing the overall conversational AI life cycle. We see that as our responsibility, and we really try to help our customers do that. And so I\u2019m gonna walk you through one of the scenarios where we did do that. But just to start off, I am gonna briefly opine on where AI is today. You know, there are folks who think that this is the current state of it. This is probably\u2026 it looks a little bit more menacing than it actually is. But people who think that the AI is really self-learning, autonomous, incredibly intelligent, etcetera, etcetera\u2026 we do talk to a lot of customers, and when we talk about how they\u2019re gonna train their models, where they\u2019re gonna get data from, they\u2019re just kinda like, I thought it just trained itself. That\u2019s not an uncommon answer. I\u2019ve even heard people who work at Nuance say that kinda confusingly. You know? And so even though it\u2019s quite advanced, I I would say it\u2019s a it\u2019s a long way off from Skynet. Instead, the mental model I like people to work with is if you just\u2026 our new customer of Dialogflow?</p>\n<p>Think of it as Google dropped off a beautiful child that looks like this for you. This is what you\u2019ve received. Yes. It\u2019s extremely intelligent. It\u2019s very capable. It someday will be very skillful and will respond to customer queries in a useful way, but that day is not today. It\u2019s not the day that you start using it. It\u2019s not the day that you launch it. You need to constantly train and mature this intelligent being so that it does something meaningful for your customers. And it\u2019s not gonna take eighteen years, thankfully, but it\u2019s probably gonna take\u2026 you know, think eighteen months. Think about that longer time frame. Alright. So onto the case study. I\u2019m not gonna use the customer\u2019s name for this. We don\u2019t yet have permission for that. Hopefully, we will have that shortly. But it\u2019s it\u2019s a pretty basic one, and I think the simplicity of it is something I really love about it. It illustrates even for sort of basic cases how tricky it can be to do this stuff.</p>\n<p>So this\u2026 the part that we helped the customer with initially was just doing an initial lookup of a user based on a member ID, and so that\u2019s an alphanumeric sequence. It\u2019s three letters, six numbers, and a dash. Really simple. Right? And then once that\u2019s been input, they\u2019re doing a lookup into their database, and then they know the first name and last name and ZIP code, and they\u2019re gonna verify the user as who they purport to be by matching that information. So they ask them what\u2019s your name, what\u2019s your date of birth, all this other verifying information, and they have to give the right response. I mean, that sounds easy. That sounds like something in twenty twenty one. This should not be hard. And they used what I would say is a pretty stay\u2026 state-of-the-art tech stack. They\u2019re using Genesys. We do a lot of work with Genesys. We really love those guys. They\u2019re using the Genesys flow architect.</p>\n<p>I think Voiceflow should really take a lot of business away from that partly \u2018cause it\u2019s not quite as good. And then they have Lex integrated for doing the speech recognition in the NLU, and then they\u2019ve got a back end that\u2019s based on AWS Lambda. So this is this is a a pretty standard tech stack. If you were like, I\u2019m gonna build a call center today, it\u2019s very likely that you would see this pop up at the top of your Google searches. You would find a lot of things that suggest this is a great way to go, but it\u2019s not. It does absolutely terrible on the alphanumeric recognition. We\u2019re not terrible. It does it does poorly on the alphanumeric recognition. On the first name and last name, it does horrendously. Even though you know what the first and last name is, it still can\u2019t get it right because Lex is missing features, like recognition hints.</p>\n<p>It has an inability to measure\u2026 or the customer had inability to measure performance accurately and repeatedly. They didn\u2019t have the tools in place for those processes, and they didn\u2019t have an ability to improve Lex over time. So one sort of theme for this presentation is, you know, you need to have good models, but then you need to have the engines around it that have good capabilities because, you know, you could have\u2026 like, let\u2019s pick on Google. Obviously, their models are incredible, but they don\u2019t have as much tooling for actually tuning those models. And so that can be a limitation if you\u2019re using, like, Google\u2019s ASR. So what did we see with the member ID? We saw a thirteen\u2026 greater than thirteen percent error rate on matching the the correct member ID that the user said. Like I said, really not very good.</p>\n<p>A lot of users are not being understood with that. And then on the first name and last name, it was a forty one point five percent and fifty six point eight percent, respectively, error rates. So that\u2019s the part that\u2019s really, you know, horrendous and just really not workable. The type of project\u2026 and we hear this a lot from people using Lex and Dialogflow where people are like, gee. I don\u2019t know that we can go forward with this. We saw this as a big strategic initiative for our company, but these numbers, this is not gonna work for our customers. They\u2019re just gonna get frustrated. We\u2019re not gonna be delivering a satisfying customer experience, whatsoever. So then we do some training on it. And in this case, it was just actually\u2026 the training is in the form\u2026 because there\u2019s not actually much that we can do with Lex there, we\u2019re actually just applying a fuzzy search algorithm, a really simple algorithm after we get the results from the ASR slash NLU. And and that does actually really help a lot, putting that in place. On the first name, last name, though\u2026 I mean, we tried a bunch of stuff with Lex.</p>\n<p>There really wasn\u2019t much that actually improves it. So there\u2019s some\u2026 in a way, these improvements are substantial, but they don\u2019t actually get it to a place where it really becomes usable. Thirty five point one percent error rate, forty eight point eight percent error rate are really not in an acceptable range. So then we said, well, let\u2019s try Azure speech. And, you know, in general, that\u2019s a product that we like. We think it works really well. By switching to that, what we found was that on that first name and last name, initially, it was seventy five percent correct. And then by just adding in recognition hints\u2026 and this almost feels like cheating, but when you say, ok. This is the name we\u2019re expecting.</p>\n<p>If you feed that to Azure first, it then understands it correctly ninety eight point four percent of the time. Same thing on the last name. So vastly reducing the number of errors, really improving that experience, you know, getting it to a range that is gonna feel almost perfect for users. And this is not\u2026 I mean, I think most modern ASRs would deliver a performance that are similar to this with their models, so it\u2019s not really a model limitation that we\u2019re looking in here. Instead, it\u2019s about the tooling that\u2019s provided by these different vendors. And some of it\u2019s a limitation of Lex. Some of it\u2019s a limitation of Genesys. But a lot of these tech stacks are really early. They don\u2019t have, you know, features like recognition hints. I mean, on one hand, that\u2019s been something that\u2019s been around for twenty plus years for IVRs.</p>\n<p>At the same time, though, some of the more newfangled ones\u2026 you know, the Dialogflow team has not had a chance to enter\u2026 to introduce this yet. Same thing with Lex. So they\u2019re sort of behind in a certain way while, at the same time, they have pushed the state of innovation forward. On the member IDs using Azure, here, we did not see a very good performance measure, though. It really struggled with the member IDs. Even with doing an extensive amount of training, it still did not do very well. I personally found this kinda surprising. I mean, we do like, in general, the Azure tool kit. It does give you a lot of power and flexibility, someone who\u2019s doing modeling, but this was a limitation that we did run into.</p>\n<p>I will say at the same time, I think this is probably overcomeable. We\u2019re reaching out to some people at Microsoft. There\u2019s likely a way to resolve this and get to a better performance level with Azure. That said, it\u2019s still\u2026 it\u2019s interesting to see that Lex here, out of the box, has really done better than Azure. And so that sorta leads us to, well, a couple overall larger points. You know, one, Azure provides more leverage for optimization than Lex. It performs very well for transcription. It does not perform as well on a tightly prescribed domain. And so that leads us to these sort of larger observations. If you\u2019re building a conversational application, the things that you should be looking for, the qualities of that system that you build are that it should be trainable, each component with it\u2026 within it should be able to be tuned and trained quite easily. It should be testable. It should be easy to measure. You should know what those measurements are. You should know what the accuracy level is. It should be modular. Each component should be able to be easily swapped out for another when and if necessary.</p>\n<p>I mean, the state of the art in this space is just changing all the time. And there\u2019s companies like Deepgram. I\u2019m very interested in their presentation. They\u2019re doing really innovative stuff. There\u2019s a lot of other companies doing innovative stuff, and so you wanna be able to take the best of what\u2019s out there and easily bring it into your system. And then finally, it should be contextual. One thing I like about this case study a lot is you\u2019re just talking about\u2026 it\u2019s really just one intent, and we\u2019re really focused on two slots within that single intent. But even within them, you really need two different models.</p>\n<p>The actual solution that we\u2019ve ended up using going forward with this specific case is a a grammar that\u2019s built with IBM Watson for that member ID. Right? And so that\u2026 that\u2019s a specific way to solve that slot-filling challenge. But then you get to the first name, last name, and there\u2019s a different approach that you wanna take. That\u2019s not something people should be afraid of. In fact, that\u2019s that\u2019s just kind of the nature of the beast. You\u2026 even within using a single vendor, you may end up with using different models between those different slots between the different intents.</p>\n<p>So you want it to be contextual. You wanna embrace making very granular decisions and doing really granular processing on these transactions to get results that are really gonna wow customers. And so we see this overall as part of a a larger ecosystem.</p>\n<p>We are, as a company, expanding the footprint of our software where we we really see ourselves as a platform that is supporting this whole life cycle. We really see that as essential. And the analytics and the management that are coming out of this, we really see as essential. We\u2019re helping customers with this orchestrator piece that sits in the center that\u2019ll actually act as a runtime to interact with the different ASRs and NLUs that are out there, and then at the same time, capture all this data.</p>\n<p>Now, ideally, it would not be necessary for us to supply that orchestration piece. But, again, it\u2019s it\u2019s not easy. You know, if you use Twilio or Genesys, they don\u2019t really supply that. So we see this as a whole platform that then is gonna make it easy to maintain and manage your AI system over time. And so what are some\u2026 the benefits of this? You\u2019re removing the risk of any single vendor. You\u2019re creating a system that can be easily improved and optimized over time, really, nearly endlessly. That\u2019s that\u2019s not necessarily such a great thing. I mean\u2026 but it is optimization, so there\u2019s always room for improvement. You\u2019re never really gonna get to a hundred percent, and it allows you to hire the best tech for each job. You know? So you might use Lex for one thing, Watson for another, Azure for a third, Deepgram for another.</p>\n<p>And so just to kinda zoom in on that platform a little bit, this is what we see as that overall cycle where, you know, there\u2019s a gathering of data, testing, training, monitoring on an ongoing basis, and then that is basically repeated ad infinitum. I\u2019m not saying that right. But, basically, it\u2019s it\u2019s repeated indefinitely as your system evolves and until it becomes truly a sort of mature, intelligent adult. And I did set a timer just to remind myself when it\u2019s fifteen minutes. So I\u2019m on my last slide here. You wanna go\u2026 you wanna take your little intelligent baby, get it walking, running, flying, and this does come back to an ROI. I put in here\u2026 we have a nice ROI calculator on our website now. We think it\u2019s pretty cool. If you plug in the numbers, the ROI, if you make significant improvements to your accuracy, really huge. It\u2019s really huge. So there\u2019s an immense benefit from that. You know, for even fairly moderate usage systems, you can really see substantial savings. So it really does matter. This is not just an academic exercise that we\u2019re talking about this for. It\u2019s gonna make customers happier, and it\u2019s gonna really deliver to your bottom line. So that\u2019s my talk. Thanks a lot for having me, and I\u2019m John Kelvie. I don\u2019t have a slide with my name, but it\u2019s j-p-k b-s-t on Twitter, and, you know, look forward to talking with and meeting all you.</p>' }, "file": "/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/index.md" };
function rawContent() {
  return `*This is the transcript for "Voice in Healthcare," presented by Henry O'Connell, CEO at Canary Speech, presented on day one of Project Voice X.* *The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was **94% accurate.**  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram's API.  If you want to see if Deepgram is right for your use case, [contact us](https://deepgram.com/contact-us/).*



\\[John Kelvie:] Thanks, Bradley. It's really nice to be here. It's nice to see Bradley again. When the pandemic started, I guess, March of twenty twenty, I remember seeing Bradley on Twitter, and he was saying, oh, just wait. You all be begging to get back to in-person events. And I thought, no. I'm definitely not gonna be begging for that. But I have to say, Bradley, I... I'm so excited to be here. I'm really excited to get these events going again. It's not necessarily that I love traveling, but I really like having the chance to meet people, and the virtual events just really didn't work very well, so great to see everybody here. And so I'm gonna talk today about a case study that we did. We have carved out this niche for testing, and as we've gone farther along as a company working with voice and we've seen these use cases emerge, we've just seen really great applications of testing, and I would even expand it.

We think of it now as just managing the overall conversational AI life cycle. We see that as our responsibility, and we really try to help our customers do that. And so I'm gonna walk you through one of the scenarios where we did do that. But just to start off, I am gonna briefly opine on where AI is today. You know, there are folks who think that this is the current state of it. This is probably... it looks a little bit more menacing than it actually is. But people who think that the AI is really self-learning, autonomous, incredibly intelligent, etcetera, etcetera... we do talk to a lot of customers, and when we talk about how they're gonna train their models, where they're gonna get data from, they're just kinda like, I thought it just trained itself. That's not an uncommon answer. I've even heard people who work at Nuance say that kinda confusingly. You know? And so even though it's quite advanced, I I would say it's a it's a long way off from Skynet. Instead, the mental model I like people to work with is if you just... our new customer of Dialogflow?

Think of it as Google dropped off a beautiful child that looks like this for you. This is what you've received. Yes. It's extremely intelligent. It's very capable. It someday will be very skillful and will respond to customer queries in a useful way, but that day is not today. It's not the day that you start using it. It's not the day that you launch it. You need to constantly train and mature this intelligent being so that it does something meaningful for your customers. And it's not gonna take eighteen years, thankfully, but it's probably gonna take... you know, think eighteen months. Think about that longer time frame. Alright. So onto the case study. I'm not gonna use the customer's name for this. We don't yet have permission for that. Hopefully, we will have that shortly. But it's it's a pretty basic one, and I think the simplicity of it is something I really love about it. It illustrates even for sort of basic cases how tricky it can be to do this stuff.

So this... the part that we helped the customer with initially was just doing an initial lookup of a user based on a member ID, and so that's an alphanumeric sequence. It's three letters, six numbers, and a dash. Really simple. Right? And then once that's been input, they're doing a lookup into their database, and then they know the first name and last name and ZIP code, and they're gonna verify the user as who they purport to be by matching that information. So they ask them what's your name, what's your date of birth, all this other verifying information, and they have to give the right response. I mean, that sounds easy. That sounds like something in twenty twenty one. This should not be hard. And they used what I would say is a pretty stay... state-of-the-art tech stack. They're using Genesys. We do a lot of work with Genesys. We really love those guys. They're using the Genesys flow architect.

I think Voiceflow should really take a lot of business away from that partly 'cause it's not quite as good. And then they have Lex integrated for doing the speech recognition in the NLU, and then they've got a back end that's based on AWS Lambda. So this is this is a a pretty standard tech stack. If you were like, I'm gonna build a call center today, it's very likely that you would see this pop up at the top of your Google searches. You would find a lot of things that suggest this is a great way to go, but it's not. It does absolutely terrible on the alphanumeric recognition. We're not terrible. It does it does poorly on the alphanumeric recognition. On the first name and last name, it does horrendously. Even though you know what the first and last name is, it still can't get it right because Lex is missing features, like recognition hints.

It has an inability to measure... or the customer had inability to measure performance accurately and repeatedly. They didn't have the tools in place for those processes, and they didn't have an ability to improve Lex over time. So one sort of theme for this presentation is, you know, you need to have good models, but then you need to have the engines around it that have good capabilities because, you know, you could have... like, let's pick on Google. Obviously, their models are incredible, but they don't have as much tooling for actually tuning those models. And so that can be a limitation if you're using, like, Google's ASR. So what did we see with the member ID? We saw a thirteen... greater than thirteen percent error rate on matching the the correct member ID that the user said. Like I said, really not very good. 

A lot of users are not being understood with that. And then on the first name and last name, it was a forty one point five percent and fifty six point eight percent, respectively, error rates. So that's the part that's really, you know, horrendous and just really not workable. The type of project... and we hear this a lot from people using Lex and Dialogflow where people are like, gee. I don't know that we can go forward with this. We saw this as a big strategic initiative for our company, but these numbers, this is not gonna work for our customers. They're just gonna get frustrated. We're not gonna be delivering a satisfying customer experience, whatsoever. So then we do some training on it. And in this case, it was just actually... the training is in the form... because there's not actually much that we can do with Lex there, we're actually just applying a fuzzy search algorithm, a really simple algorithm after we get the results from the ASR slash NLU. And and that does actually really help a lot, putting that in place. On the first name, last name, though... I mean, we tried a bunch of stuff with Lex.

There really wasn't much that actually improves it. So there's some... in a way, these improvements are substantial, but they don't actually get it to a place where it really becomes usable. Thirty five point one percent error rate, forty eight point eight percent error rate are really not in an acceptable range. So then we said, well, let's try Azure speech. And, you know, in general, that's a product that we like. We think it works really well. By switching to that, what we found was that on that first name and last name, initially, it was seventy five percent correct. And then by just adding in recognition hints... and this almost feels like cheating, but when you say, ok. This is the name we're expecting.

If you feed that to Azure first, it then understands it correctly ninety eight point four percent of the time. Same thing on the last name. So vastly reducing the number of errors, really improving that experience, you know, getting it to a range that is gonna feel almost perfect for users. And this is not... I mean, I think most modern ASRs would deliver a performance that are similar to this with their models, so it's not really a model limitation that we're looking in here. Instead, it's about the tooling that's provided by these different vendors. And some of it's a limitation of Lex. Some of it's a limitation of Genesys. But a lot of these tech stacks are really early. They don't have, you know, features like recognition hints. I mean, on one hand, that's been something that's been around for twenty plus years for IVRs. 

At the same time, though, some of the more newfangled ones... you know, the Dialogflow team has not had a chance to enter... to introduce this yet. Same thing with Lex. So they're sort of behind in a certain way while, at the same time, they have pushed the state of innovation forward. On the member IDs using Azure, here, we did not see a very good performance measure, though. It really struggled with the member IDs. Even with doing an extensive amount of training, it still did not do very well. I personally found this kinda surprising. I mean, we do like, in general, the Azure tool kit. It does give you a lot of power and flexibility, someone who's doing modeling, but this was a limitation that we did run into. 

I will say at the same time, I think this is probably overcomeable. We're reaching out to some people at Microsoft. There's likely a way to resolve this and get to a better performance level with Azure. That said, it's still... it's interesting to see that Lex here, out of the box, has really done better than Azure. And so that sorta leads us to, well, a couple overall larger points. You know, one, Azure provides more leverage for optimization than Lex. It performs very well for transcription. It does not perform as well on a tightly prescribed domain. And so that leads us to these sort of larger observations. If you're building a conversational application, the things that you should be looking for, the qualities of that system that you build are that it should be trainable, each component with it... within it should be able to be tuned and trained quite easily. It should be testable. It should be easy to measure. You should know what those measurements are. You should know what the accuracy level is. It should be modular. Each component should be able to be easily swapped out for another when and if necessary. 

I mean, the state of the art in this space is just changing all the time. And there's companies like Deepgram. I'm very interested in their presentation. They're doing really innovative stuff. There's a lot of other companies doing innovative stuff, and so you wanna be able to take the best of what's out there and easily bring it into your system. And then finally, it should be contextual. One thing I like about this case study a lot is you're just talking about... it's really just one intent, and we're really focused on two slots within that single intent. But even within them, you really need two different models. 

The actual solution that we've ended up using going forward with this specific case is a a grammar that's built with IBM Watson for that member ID. Right? And so that... that's a specific way to solve that slot-filling challenge. But then you get to the first name, last name, and there's a different approach that you wanna take. That's not something people should be afraid of. In fact, that's that's just kind of the nature of the beast. You... even within using a single vendor, you may end up with using different models between those different slots between the different intents.

So you want it to be contextual. You wanna embrace making very granular decisions and doing really granular processing on these transactions to get results that are really gonna wow customers. And so we see this overall as part of a a larger ecosystem.

We are, as a company, expanding the footprint of our software where we we really see ourselves as a platform that is supporting this whole life cycle. We really see that as essential. And the analytics and the management that are coming out of this, we really see as essential. We're helping customers with this orchestrator piece that sits in the center that'll actually act as a runtime to interact with the different ASRs and NLUs that are out there, and then at the same time, capture all this data.

Now, ideally, it would not be necessary for us to supply that orchestration piece. But, again, it's it's not easy. You know, if you use Twilio or Genesys, they don't really supply that. So we see this as a whole platform that then is gonna make it easy to maintain and manage your AI system over time. And so what are some... the benefits of this? You're removing the risk of any single vendor. You're creating a system that can be easily improved and optimized over time, really, nearly endlessly. That's that's not necessarily such a great thing. I mean... but it is optimization, so there's always room for improvement. You're never really gonna get to a hundred percent, and it allows you to hire the best tech for each job. You know? So you might use Lex for one thing, Watson for another, Azure for a third, Deepgram for another.

And so just to kinda zoom in on that platform a little bit, this is what we see as that overall cycle where, you know, there's a gathering of data, testing, training, monitoring on an ongoing basis, and then that is basically repeated ad infinitum. I'm not saying that right. But, basically, it's it's repeated indefinitely as your system evolves and until it becomes truly a sort of mature, intelligent adult. And I did set a timer just to remind myself when it's fifteen minutes. So I'm on my last slide here. You wanna go... you wanna take your little intelligent baby, get it walking, running, flying, and this does come back to an ROI. I put in here... we have a nice ROI calculator on our website now. We think it's pretty cool. If you plug in the numbers, the ROI, if you make significant improvements to your accuracy, really huge. It's really huge. So there's an immense benefit from that. You know, for even fairly moderate usage systems, you can really see substantial savings. So it really does matter. This is not just an academic exercise that we're talking about this for. It's gonna make customers happier, and it's gonna really deliver to your bottom line. So that's my talk. Thanks a lot for having me, and I'm John Kelvie. I don't have a slide with my name, but it's j-p-k b-s-t on Twitter, and, you know, look forward to talking with and meeting all you.`;
}
function compiledContent() {
  return '<p><em>This is the transcript for \u201CVoice in Healthcare,\u201D presented by Henry O\u2019Connell, CEO at Canary Speech, presented on day one of Project Voice X.</em> <em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was <strong>94% accurate.</strong>  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgram\u2019s API.  If you want to see if Deepgram is right for your use case, <a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>\n<p>[John Kelvie:] Thanks, Bradley. It\u2019s really nice to be here. It\u2019s nice to see Bradley again. When the pandemic started, I guess, March of twenty twenty, I remember seeing Bradley on Twitter, and he was saying, oh, just wait. You all be begging to get back to in-person events. And I thought, no. I\u2019m definitely not gonna be begging for that. But I have to say, Bradley, I\u2026 I\u2019m so excited to be here. I\u2019m really excited to get these events going again. It\u2019s not necessarily that I love traveling, but I really like having the chance to meet people, and the virtual events just really didn\u2019t work very well, so great to see everybody here. And so I\u2019m gonna talk today about a case study that we did. We have carved out this niche for testing, and as we\u2019ve gone farther along as a company working with voice and we\u2019ve seen these use cases emerge, we\u2019ve just seen really great applications of testing, and I would even expand it.</p>\n<p>We think of it now as just managing the overall conversational AI life cycle. We see that as our responsibility, and we really try to help our customers do that. And so I\u2019m gonna walk you through one of the scenarios where we did do that. But just to start off, I am gonna briefly opine on where AI is today. You know, there are folks who think that this is the current state of it. This is probably\u2026 it looks a little bit more menacing than it actually is. But people who think that the AI is really self-learning, autonomous, incredibly intelligent, etcetera, etcetera\u2026 we do talk to a lot of customers, and when we talk about how they\u2019re gonna train their models, where they\u2019re gonna get data from, they\u2019re just kinda like, I thought it just trained itself. That\u2019s not an uncommon answer. I\u2019ve even heard people who work at Nuance say that kinda confusingly. You know? And so even though it\u2019s quite advanced, I I would say it\u2019s a it\u2019s a long way off from Skynet. Instead, the mental model I like people to work with is if you just\u2026 our new customer of Dialogflow?</p>\n<p>Think of it as Google dropped off a beautiful child that looks like this for you. This is what you\u2019ve received. Yes. It\u2019s extremely intelligent. It\u2019s very capable. It someday will be very skillful and will respond to customer queries in a useful way, but that day is not today. It\u2019s not the day that you start using it. It\u2019s not the day that you launch it. You need to constantly train and mature this intelligent being so that it does something meaningful for your customers. And it\u2019s not gonna take eighteen years, thankfully, but it\u2019s probably gonna take\u2026 you know, think eighteen months. Think about that longer time frame. Alright. So onto the case study. I\u2019m not gonna use the customer\u2019s name for this. We don\u2019t yet have permission for that. Hopefully, we will have that shortly. But it\u2019s it\u2019s a pretty basic one, and I think the simplicity of it is something I really love about it. It illustrates even for sort of basic cases how tricky it can be to do this stuff.</p>\n<p>So this\u2026 the part that we helped the customer with initially was just doing an initial lookup of a user based on a member ID, and so that\u2019s an alphanumeric sequence. It\u2019s three letters, six numbers, and a dash. Really simple. Right? And then once that\u2019s been input, they\u2019re doing a lookup into their database, and then they know the first name and last name and ZIP code, and they\u2019re gonna verify the user as who they purport to be by matching that information. So they ask them what\u2019s your name, what\u2019s your date of birth, all this other verifying information, and they have to give the right response. I mean, that sounds easy. That sounds like something in twenty twenty one. This should not be hard. And they used what I would say is a pretty stay\u2026 state-of-the-art tech stack. They\u2019re using Genesys. We do a lot of work with Genesys. We really love those guys. They\u2019re using the Genesys flow architect.</p>\n<p>I think Voiceflow should really take a lot of business away from that partly \u2018cause it\u2019s not quite as good. And then they have Lex integrated for doing the speech recognition in the NLU, and then they\u2019ve got a back end that\u2019s based on AWS Lambda. So this is this is a a pretty standard tech stack. If you were like, I\u2019m gonna build a call center today, it\u2019s very likely that you would see this pop up at the top of your Google searches. You would find a lot of things that suggest this is a great way to go, but it\u2019s not. It does absolutely terrible on the alphanumeric recognition. We\u2019re not terrible. It does it does poorly on the alphanumeric recognition. On the first name and last name, it does horrendously. Even though you know what the first and last name is, it still can\u2019t get it right because Lex is missing features, like recognition hints.</p>\n<p>It has an inability to measure\u2026 or the customer had inability to measure performance accurately and repeatedly. They didn\u2019t have the tools in place for those processes, and they didn\u2019t have an ability to improve Lex over time. So one sort of theme for this presentation is, you know, you need to have good models, but then you need to have the engines around it that have good capabilities because, you know, you could have\u2026 like, let\u2019s pick on Google. Obviously, their models are incredible, but they don\u2019t have as much tooling for actually tuning those models. And so that can be a limitation if you\u2019re using, like, Google\u2019s ASR. So what did we see with the member ID? We saw a thirteen\u2026 greater than thirteen percent error rate on matching the the correct member ID that the user said. Like I said, really not very good.</p>\n<p>A lot of users are not being understood with that. And then on the first name and last name, it was a forty one point five percent and fifty six point eight percent, respectively, error rates. So that\u2019s the part that\u2019s really, you know, horrendous and just really not workable. The type of project\u2026 and we hear this a lot from people using Lex and Dialogflow where people are like, gee. I don\u2019t know that we can go forward with this. We saw this as a big strategic initiative for our company, but these numbers, this is not gonna work for our customers. They\u2019re just gonna get frustrated. We\u2019re not gonna be delivering a satisfying customer experience, whatsoever. So then we do some training on it. And in this case, it was just actually\u2026 the training is in the form\u2026 because there\u2019s not actually much that we can do with Lex there, we\u2019re actually just applying a fuzzy search algorithm, a really simple algorithm after we get the results from the ASR slash NLU. And and that does actually really help a lot, putting that in place. On the first name, last name, though\u2026 I mean, we tried a bunch of stuff with Lex.</p>\n<p>There really wasn\u2019t much that actually improves it. So there\u2019s some\u2026 in a way, these improvements are substantial, but they don\u2019t actually get it to a place where it really becomes usable. Thirty five point one percent error rate, forty eight point eight percent error rate are really not in an acceptable range. So then we said, well, let\u2019s try Azure speech. And, you know, in general, that\u2019s a product that we like. We think it works really well. By switching to that, what we found was that on that first name and last name, initially, it was seventy five percent correct. And then by just adding in recognition hints\u2026 and this almost feels like cheating, but when you say, ok. This is the name we\u2019re expecting.</p>\n<p>If you feed that to Azure first, it then understands it correctly ninety eight point four percent of the time. Same thing on the last name. So vastly reducing the number of errors, really improving that experience, you know, getting it to a range that is gonna feel almost perfect for users. And this is not\u2026 I mean, I think most modern ASRs would deliver a performance that are similar to this with their models, so it\u2019s not really a model limitation that we\u2019re looking in here. Instead, it\u2019s about the tooling that\u2019s provided by these different vendors. And some of it\u2019s a limitation of Lex. Some of it\u2019s a limitation of Genesys. But a lot of these tech stacks are really early. They don\u2019t have, you know, features like recognition hints. I mean, on one hand, that\u2019s been something that\u2019s been around for twenty plus years for IVRs.</p>\n<p>At the same time, though, some of the more newfangled ones\u2026 you know, the Dialogflow team has not had a chance to enter\u2026 to introduce this yet. Same thing with Lex. So they\u2019re sort of behind in a certain way while, at the same time, they have pushed the state of innovation forward. On the member IDs using Azure, here, we did not see a very good performance measure, though. It really struggled with the member IDs. Even with doing an extensive amount of training, it still did not do very well. I personally found this kinda surprising. I mean, we do like, in general, the Azure tool kit. It does give you a lot of power and flexibility, someone who\u2019s doing modeling, but this was a limitation that we did run into.</p>\n<p>I will say at the same time, I think this is probably overcomeable. We\u2019re reaching out to some people at Microsoft. There\u2019s likely a way to resolve this and get to a better performance level with Azure. That said, it\u2019s still\u2026 it\u2019s interesting to see that Lex here, out of the box, has really done better than Azure. And so that sorta leads us to, well, a couple overall larger points. You know, one, Azure provides more leverage for optimization than Lex. It performs very well for transcription. It does not perform as well on a tightly prescribed domain. And so that leads us to these sort of larger observations. If you\u2019re building a conversational application, the things that you should be looking for, the qualities of that system that you build are that it should be trainable, each component with it\u2026 within it should be able to be tuned and trained quite easily. It should be testable. It should be easy to measure. You should know what those measurements are. You should know what the accuracy level is. It should be modular. Each component should be able to be easily swapped out for another when and if necessary.</p>\n<p>I mean, the state of the art in this space is just changing all the time. And there\u2019s companies like Deepgram. I\u2019m very interested in their presentation. They\u2019re doing really innovative stuff. There\u2019s a lot of other companies doing innovative stuff, and so you wanna be able to take the best of what\u2019s out there and easily bring it into your system. And then finally, it should be contextual. One thing I like about this case study a lot is you\u2019re just talking about\u2026 it\u2019s really just one intent, and we\u2019re really focused on two slots within that single intent. But even within them, you really need two different models.</p>\n<p>The actual solution that we\u2019ve ended up using going forward with this specific case is a a grammar that\u2019s built with IBM Watson for that member ID. Right? And so that\u2026 that\u2019s a specific way to solve that slot-filling challenge. But then you get to the first name, last name, and there\u2019s a different approach that you wanna take. That\u2019s not something people should be afraid of. In fact, that\u2019s that\u2019s just kind of the nature of the beast. You\u2026 even within using a single vendor, you may end up with using different models between those different slots between the different intents.</p>\n<p>So you want it to be contextual. You wanna embrace making very granular decisions and doing really granular processing on these transactions to get results that are really gonna wow customers. And so we see this overall as part of a a larger ecosystem.</p>\n<p>We are, as a company, expanding the footprint of our software where we we really see ourselves as a platform that is supporting this whole life cycle. We really see that as essential. And the analytics and the management that are coming out of this, we really see as essential. We\u2019re helping customers with this orchestrator piece that sits in the center that\u2019ll actually act as a runtime to interact with the different ASRs and NLUs that are out there, and then at the same time, capture all this data.</p>\n<p>Now, ideally, it would not be necessary for us to supply that orchestration piece. But, again, it\u2019s it\u2019s not easy. You know, if you use Twilio or Genesys, they don\u2019t really supply that. So we see this as a whole platform that then is gonna make it easy to maintain and manage your AI system over time. And so what are some\u2026 the benefits of this? You\u2019re removing the risk of any single vendor. You\u2019re creating a system that can be easily improved and optimized over time, really, nearly endlessly. That\u2019s that\u2019s not necessarily such a great thing. I mean\u2026 but it is optimization, so there\u2019s always room for improvement. You\u2019re never really gonna get to a hundred percent, and it allows you to hire the best tech for each job. You know? So you might use Lex for one thing, Watson for another, Azure for a third, Deepgram for another.</p>\n<p>And so just to kinda zoom in on that platform a little bit, this is what we see as that overall cycle where, you know, there\u2019s a gathering of data, testing, training, monitoring on an ongoing basis, and then that is basically repeated ad infinitum. I\u2019m not saying that right. But, basically, it\u2019s it\u2019s repeated indefinitely as your system evolves and until it becomes truly a sort of mature, intelligent adult. And I did set a timer just to remind myself when it\u2019s fifteen minutes. So I\u2019m on my last slide here. You wanna go\u2026 you wanna take your little intelligent baby, get it walking, running, flying, and this does come back to an ROI. I put in here\u2026 we have a nice ROI calculator on our website now. We think it\u2019s pretty cool. If you plug in the numbers, the ROI, if you make significant improvements to your accuracy, really huge. It\u2019s really huge. So there\u2019s an immense benefit from that. You know, for even fairly moderate usage systems, you can really see substantial savings. So it really does matter. This is not just an academic exercise that we\u2019re talking about this for. It\u2019s gonna make customers happier, and it\u2019s gonna really deliver to your bottom line. So that\u2019s my talk. Thanks a lot for having me, and I\u2019m John Kelvie. I don\u2019t have a slide with my name, but it\u2019s j-p-k b-s-t on Twitter, and, you know, look forward to talking with and meeting all you.</p>';
}
const $$Astro = createAstro("/Users/sandrarodgers/web-next/blog/src/content/blog/posts/the-importance-of-testing-with-voice-experiences-and-conversational-ai-john-kelvie-ceo-bespoken-project-voice-x/index.md", "", "file:///Users/sandrarodgers/web-next/blog/");
const $$Index = createComponent(async ($$result, $$props, $$slots) => {
  const Astro2 = $$result.createAstro($$Astro, $$props, $$slots);
  Astro2.self = $$Index;
  new Slugger();
  return renderTemplate`<head>${renderHead($$result)}</head><p><em>This is the transcript for Voice in Healthcare, presented by Henry OConnell, CEO at Canary Speech, presented on day one of Project Voice X.</em> <em>The transcript below has been modified by the Deepgram team for readability as a blog post, but the original Deepgram ASR-generated transcript was <strong>94% accurate.</strong>  Features like diarization, custom vocabulary (keyword boosting), redaction, punctuation, profanity filtering and numeral formatting are all available through Deepgrams API.  If you want to see if Deepgram is right for your use case, <a href="https://deepgram.com/contact-us/">contact us</a>.</em></p>
<p>[John Kelvie:] Thanks, Bradley. Its really nice to be here. Its nice to see Bradley again. When the pandemic started, I guess, March of twenty twenty, I remember seeing Bradley on Twitter, and he was saying, oh, just wait. You all be begging to get back to in-person events. And I thought, no. Im definitely not gonna be begging for that. But I have to say, Bradley, I Im so excited to be here. Im really excited to get these events going again. Its not necessarily that I love traveling, but I really like having the chance to meet people, and the virtual events just really didnt work very well, so great to see everybody here. And so Im gonna talk today about a case study that we did. We have carved out this niche for testing, and as weve gone farther along as a company working with voice and weve seen these use cases emerge, weve just seen really great applications of testing, and I would even expand it.</p>
<p>We think of it now as just managing the overall conversational AI life cycle. We see that as our responsibility, and we really try to help our customers do that. And so Im gonna walk you through one of the scenarios where we did do that. But just to start off, I am gonna briefly opine on where AI is today. You know, there are folks who think that this is the current state of it. This is probably it looks a little bit more menacing than it actually is. But people who think that the AI is really self-learning, autonomous, incredibly intelligent, etcetera, etcetera we do talk to a lot of customers, and when we talk about how theyre gonna train their models, where theyre gonna get data from, theyre just kinda like, I thought it just trained itself. Thats not an uncommon answer. Ive even heard people who work at Nuance say that kinda confusingly. You know? And so even though its quite advanced, I I would say its a its a long way off from Skynet. Instead, the mental model I like people to work with is if you just our new customer of Dialogflow?</p>
<p>Think of it as Google dropped off a beautiful child that looks like this for you. This is what youve received. Yes. Its extremely intelligent. Its very capable. It someday will be very skillful and will respond to customer queries in a useful way, but that day is not today. Its not the day that you start using it. Its not the day that you launch it. You need to constantly train and mature this intelligent being so that it does something meaningful for your customers. And its not gonna take eighteen years, thankfully, but its probably gonna take you know, think eighteen months. Think about that longer time frame. Alright. So onto the case study. Im not gonna use the customers name for this. We dont yet have permission for that. Hopefully, we will have that shortly. But its its a pretty basic one, and I think the simplicity of it is something I really love about it. It illustrates even for sort of basic cases how tricky it can be to do this stuff.</p>
<p>So this the part that we helped the customer with initially was just doing an initial lookup of a user based on a member ID, and so thats an alphanumeric sequence. Its three letters, six numbers, and a dash. Really simple. Right? And then once thats been input, theyre doing a lookup into their database, and then they know the first name and last name and ZIP code, and theyre gonna verify the user as who they purport to be by matching that information. So they ask them whats your name, whats your date of birth, all this other verifying information, and they have to give the right response. I mean, that sounds easy. That sounds like something in twenty twenty one. This should not be hard. And they used what I would say is a pretty stay state-of-the-art tech stack. Theyre using Genesys. We do a lot of work with Genesys. We really love those guys. Theyre using the Genesys flow architect.</p>
<p>I think Voiceflow should really take a lot of business away from that partly cause its not quite as good. And then they have Lex integrated for doing the speech recognition in the NLU, and then theyve got a back end thats based on AWS Lambda. So this is this is a a pretty standard tech stack. If you were like, Im gonna build a call center today, its very likely that you would see this pop up at the top of your Google searches. You would find a lot of things that suggest this is a great way to go, but its not. It does absolutely terrible on the alphanumeric recognition. Were not terrible. It does it does poorly on the alphanumeric recognition. On the first name and last name, it does horrendously. Even though you know what the first and last name is, it still cant get it right because Lex is missing features, like recognition hints.</p>
<p>It has an inability to measure or the customer had inability to measure performance accurately and repeatedly. They didnt have the tools in place for those processes, and they didnt have an ability to improve Lex over time. So one sort of theme for this presentation is, you know, you need to have good models, but then you need to have the engines around it that have good capabilities because, you know, you could have like, lets pick on Google. Obviously, their models are incredible, but they dont have as much tooling for actually tuning those models. And so that can be a limitation if youre using, like, Googles ASR. So what did we see with the member ID? We saw a thirteen greater than thirteen percent error rate on matching the the correct member ID that the user said. Like I said, really not very good.</p>
<p>A lot of users are not being understood with that. And then on the first name and last name, it was a forty one point five percent and fifty six point eight percent, respectively, error rates. So thats the part thats really, you know, horrendous and just really not workable. The type of project and we hear this a lot from people using Lex and Dialogflow where people are like, gee. I dont know that we can go forward with this. We saw this as a big strategic initiative for our company, but these numbers, this is not gonna work for our customers. Theyre just gonna get frustrated. Were not gonna be delivering a satisfying customer experience, whatsoever. So then we do some training on it. And in this case, it was just actually the training is in the form because theres not actually much that we can do with Lex there, were actually just applying a fuzzy search algorithm, a really simple algorithm after we get the results from the ASR slash NLU. And and that does actually really help a lot, putting that in place. On the first name, last name, though I mean, we tried a bunch of stuff with Lex.</p>
<p>There really wasnt much that actually improves it. So theres some in a way, these improvements are substantial, but they dont actually get it to a place where it really becomes usable. Thirty five point one percent error rate, forty eight point eight percent error rate are really not in an acceptable range. So then we said, well, lets try Azure speech. And, you know, in general, thats a product that we like. We think it works really well. By switching to that, what we found was that on that first name and last name, initially, it was seventy five percent correct. And then by just adding in recognition hints and this almost feels like cheating, but when you say, ok. This is the name were expecting.</p>
<p>If you feed that to Azure first, it then understands it correctly ninety eight point four percent of the time. Same thing on the last name. So vastly reducing the number of errors, really improving that experience, you know, getting it to a range that is gonna feel almost perfect for users. And this is not I mean, I think most modern ASRs would deliver a performance that are similar to this with their models, so its not really a model limitation that were looking in here. Instead, its about the tooling thats provided by these different vendors. And some of its a limitation of Lex. Some of its a limitation of Genesys. But a lot of these tech stacks are really early. They dont have, you know, features like recognition hints. I mean, on one hand, thats been something thats been around for twenty plus years for IVRs.</p>
<p>At the same time, though, some of the more newfangled ones you know, the Dialogflow team has not had a chance to enter to introduce this yet. Same thing with Lex. So theyre sort of behind in a certain way while, at the same time, they have pushed the state of innovation forward. On the member IDs using Azure, here, we did not see a very good performance measure, though. It really struggled with the member IDs. Even with doing an extensive amount of training, it still did not do very well. I personally found this kinda surprising. I mean, we do like, in general, the Azure tool kit. It does give you a lot of power and flexibility, someone whos doing modeling, but this was a limitation that we did run into.</p>
<p>I will say at the same time, I think this is probably overcomeable. Were reaching out to some people at Microsoft. Theres likely a way to resolve this and get to a better performance level with Azure. That said, its still its interesting to see that Lex here, out of the box, has really done better than Azure. And so that sorta leads us to, well, a couple overall larger points. You know, one, Azure provides more leverage for optimization than Lex. It performs very well for transcription. It does not perform as well on a tightly prescribed domain. And so that leads us to these sort of larger observations. If youre building a conversational application, the things that you should be looking for, the qualities of that system that you build are that it should be trainable, each component with it within it should be able to be tuned and trained quite easily. It should be testable. It should be easy to measure. You should know what those measurements are. You should know what the accuracy level is. It should be modular. Each component should be able to be easily swapped out for another when and if necessary.</p>
<p>I mean, the state of the art in this space is just changing all the time. And theres companies like Deepgram. Im very interested in their presentation. Theyre doing really innovative stuff. Theres a lot of other companies doing innovative stuff, and so you wanna be able to take the best of whats out there and easily bring it into your system. And then finally, it should be contextual. One thing I like about this case study a lot is youre just talking about its really just one intent, and were really focused on two slots within that single intent. But even within them, you really need two different models.</p>
<p>The actual solution that weve ended up using going forward with this specific case is a a grammar thats built with IBM Watson for that member ID. Right? And so that thats a specific way to solve that slot-filling challenge. But then you get to the first name, last name, and theres a different approach that you wanna take. Thats not something people should be afraid of. In fact, thats thats just kind of the nature of the beast. You even within using a single vendor, you may end up with using different models between those different slots between the different intents.</p>
<p>So you want it to be contextual. You wanna embrace making very granular decisions and doing really granular processing on these transactions to get results that are really gonna wow customers. And so we see this overall as part of a a larger ecosystem.</p>
<p>We are, as a company, expanding the footprint of our software where we we really see ourselves as a platform that is supporting this whole life cycle. We really see that as essential. And the analytics and the management that are coming out of this, we really see as essential. Were helping customers with this orchestrator piece that sits in the center thatll actually act as a runtime to interact with the different ASRs and NLUs that are out there, and then at the same time, capture all this data.</p>
<p>Now, ideally, it would not be necessary for us to supply that orchestration piece. But, again, its its not easy. You know, if you use Twilio or Genesys, they dont really supply that. So we see this as a whole platform that then is gonna make it easy to maintain and manage your AI system over time. And so what are some the benefits of this? Youre removing the risk of any single vendor. Youre creating a system that can be easily improved and optimized over time, really, nearly endlessly. Thats thats not necessarily such a great thing. I mean but it is optimization, so theres always room for improvement. Youre never really gonna get to a hundred percent, and it allows you to hire the best tech for each job. You know? So you might use Lex for one thing, Watson for another, Azure for a third, Deepgram for another.</p>
<p>And so just to kinda zoom in on that platform a little bit, this is what we see as that overall cycle where, you know, theres a gathering of data, testing, training, monitoring on an ongoing basis, and then that is basically repeated ad infinitum. Im not saying that right. But, basically, its its repeated indefinitely as your system evolves and until it becomes truly a sort of mature, intelligent adult. And I did set a timer just to remind myself when its fifteen minutes. So Im on my last slide here. You wanna go you wanna take your little intelligent baby, get it walking, running, flying, and this does come back to an ROI. I put in here we have a nice ROI calculator on our website now. We think its pretty cool. If you plug in the numbers, the ROI, if you make significant improvements to your accuracy, really huge. Its really huge. So theres an immense benefit from that. You know, for even fairly moderate usage systems, you can really see substantial savings. So it really does matter. This is not just an academic exercise that were talking about this for. Its gonna make customers happier, and its gonna really deliver to your bottom line. So thats my talk. Thanks a lot for having me, and Im John Kelvie. I dont have a slide with my name, but its j-p-k b-s-t on Twitter, and, you know, look forward to talking with and meeting all you.</p>`;
});

export { compiledContent, $$Index as default, frontmatter, metadata, rawContent };
